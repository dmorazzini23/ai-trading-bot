--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/main.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/main.py@@ -81,9 +81,9 @@ def validate_environment() -> None:
     """Ensure required environment variables are present and dependencies are available."""
     # Check critical environment variables
-    if not config.WEBHOOK_SECRET:
+    if not S.webhook_secret:
         raise RuntimeError("WEBHOOK_SECRET is required")
-    if not config.ALPACA_API_KEY or not config.ALPACA_SECRET_KEY:
+    if not S.alpaca_api_key or not S.alpaca_secret_key:
         raise RuntimeError("ALPACA_API_KEY and ALPACA_SECRET_KEY are required")
 
     # Check optional but important dependencies
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/meta_learning.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/meta_learning.py@@ -1,3 +1,4 @@+S = get_settings()
 """Utility helpers for meta-learning weight management."""
 
 import csv
@@ -579,7 +580,7 @@ 
     # Set default trade log path
     if trade_log_path is None:
-        trade_log_path = config.TRADE_LOG_FILE if config else "trades.csv"
+        trade_log_path = S.trade_log_file if config else "trades.csv"
 
     logger.info(
         "META_RETRAIN_START",
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/rebalancer.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/rebalancer.py@@ -593,7 +593,7 @@             )
 
             # Dynamic threshold based on market volatility if available
-            drift_threshold = config.PORTFOLIO_DRIFT_THRESHOLD
+            drift_threshold = S.portfolio_drift_threshold
 
             # Use portfolio-first rebalancing if available
             if PORTFOLIO_FIRST_AVAILABLE:
@@ -927,7 +927,7 @@                 if current
                 else 0.0
             )
-            if drift > config.PORTFOLIO_DRIFT_THRESHOLD:
+            if drift > S.portfolio_drift_threshold:
                 rebalance_portfolio(ctx)
                 _last_rebalance = now
 
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/config/settings.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/config/settings.py@@ -10,6 +10,20 @@ 
 
 class Settings(BaseSettings):
+    config: str | None = Field(default=None, env='CONFIG')
+    force_trades: str | None = Field(default=None, env='FORCE_TRADES')
+    max_drawdown_threshold: str | None = Field(default=None, env='MAX_DRAWDOWN_THRESHOLD')
+    min_health_rows: str | None = Field(default=None, env='MIN_HEALTH_ROWS')
+    ml_confidence_threshold: str | None = Field(default=None, env='ML_CONFIDENCE_THRESHOLD')
+    portfolio_drift_threshold: str | None = Field(default=None, env='PORTFOLIO_DRIFT_THRESHOLD')
+    pyramid_levels: str | None = Field(default=None, env='PYRAMID_LEVELS')
+    rl_model_path: str | None = Field(default=None, env='RL_MODEL_PATH')
+    sgd_params: str | None = Field(default=None, env='SGD_PARAMS')
+    trade_audit_dir: str | None = Field(default=None, env='TRADE_AUDIT_DIR')
+    use_rl_agent: str | None = Field(default=None, env='USE_RL_AGENT')
+    verbose: str | None = Field(default=None, env='VERBOSE')
+    verbose_logging: str | None = Field(default=None, env='VERBOSE_LOGGING')
+    volume_spike_threshold: str | None = Field(default=None, env='VOLUME_SPIKE_THRESHOLD')
     # Runtime toggles
     trading_mode: str = Field("balanced", env="TRADING_MODE")
     shadow_mode: bool = Field(False, env="SHADOW_MODE")
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/core/bot_engine.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/core/bot_engine.py@@ -1,12 +1,6 @@ from __future__ import annotations
-
-# (any existing comments or module docstring go below the future import)
-__all__ = ["pre_trade_health_check", "run_all_trades_worker", "BotState"]
-
-
-# AI-AGENT-REF: Track regime warnings to avoid spamming logs during market closed
-# Using a mutable dict to avoid fragile `global` declarations inside functions.
-_REGIME_INSUFFICIENT_DATA_WARNED = {"done": False}
+__all__ = ['pre_trade_health_check', 'run_all_trades_worker', 'BotState']
+_REGIME_INSUFFICIENT_DATA_WARNED = {'done': False}
 import asyncio
 import atexit
 import io
@@ -22,248 +16,128 @@ from datetime import UTC, date, datetime, timedelta
 from pathlib import Path
 
-# AI-AGENT-REF: Memory optimization as optional feature
-# (settings will be imported below with other config imports)
 def _get_memory_optimization():
     """Initialize memory optimization based on settings."""
     from ai_trading.config import get_settings
     S = get_settings()
-    
     if S.enable_memory_optimization:
-        from memory_optimizer import (
-            emergency_memory_cleanup,
-            memory_profile,
-            optimize_memory,
-        )
-        return True, memory_profile, optimize_memory, emergency_memory_cleanup
+        from memory_optimizer import emergency_memory_cleanup, memory_profile, optimize_memory
+        return (True, memory_profile, optimize_memory, emergency_memory_cleanup)
     else:
-        # Fallback no-op decorators when memory optimization is disabled
+
         def memory_profile(func):
             return func
-        
+
         def optimize_memory():
             return {}
-        
+
         def emergency_memory_cleanup():
             return {}
-            
-        return False, memory_profile, optimize_memory, emergency_memory_cleanup
-
+        return (False, memory_profile, optimize_memory, emergency_memory_cleanup)
 MEMORY_OPTIMIZATION_AVAILABLE, memory_profile, optimize_memory, emergency_memory_cleanup = _get_memory_optimization()
-
-
-# AI-AGENT-REF: replace utcnow with timezone-aware now
-old_generate = datetime.now(UTC)  # replaced utcnow for tz-aware
+old_generate = datetime.now(UTC)
 new_generate = datetime.now(UTC)
-
-# AI-AGENT-REF: suppress noisy external library warnings
-warnings.filterwarnings(
-    "ignore", category=SyntaxWarning, message="invalid escape sequence"
-)
-warnings.filterwarnings("ignore", message=".*_register_pytree_node.*")
-
-# Avoid failing under older Python versions during tests
-
+warnings.filterwarnings('ignore', category=SyntaxWarning, message='invalid escape sequence')
+warnings.filterwarnings('ignore', message='.*_register_pytree_node.*')
 from concurrent.futures import ThreadPoolExecutor, as_completed
-
-from ai_trading import (
-    paths,  # AI-AGENT-REF: Runtime paths for proper directory separation
-)
+from ai_trading import paths
 from ai_trading.config import get_settings
 from ai_trading.config import management as config
-
-# Initialize settings once for global use
 S = get_settings()
-from ai_trading.data_fetcher import (
-    get_bars,
-    get_bars_batch,
-    get_minute_bars,
-    get_minute_bars_batch,
-    warmup_cache,
-)
+from ai_trading.data_fetcher import get_bars, get_bars_batch, get_minute_bars, get_minute_bars_batch, warmup_cache
 from ai_trading.market.calendars import ensure_final_bar
-from ai_trading.utils.timefmt import (
-    utc_now_iso,  # AI-AGENT-REF: Import UTC timestamp utilities
-)
-
-# AI-AGENT-REF: Import drawdown circuit breaker for real-time portfolio protection
+from ai_trading.utils.timefmt import utc_now_iso
 from ai_trading.risk.circuit_breakers import DrawdownCircuitBreaker
-# AI-AGENT-REF: lazy import expensive modules to speed up import for tests
-if not os.getenv("PYTEST_RUNNING"):
-    from ai_trading.model_loader import ML_MODELS  # AI-AGENT-REF: preloaded models
+if not os.getenv('PYTEST_RUNNING'):
+    from ai_trading.model_loader import ML_MODELS
 else:
-    # AI-AGENT-REF: mock ML_MODELS for test environments to avoid slow imports
     ML_MODELS = {}
-
-
-# AI-AGENT-REF: lazy numpy loader for improved import performance
-# AI-AGENT-REF: numpy is a hard dependency - import directly
 import numpy as np
-
-LOG_PATH = os.getenv("BOT_LOG_FILE", "logs/scheduler.log")
-# Set up logging only once
-logger = logging.getLogger(__name__)  # AI-AGENT-REF: define logger before use
-# AI-AGENT-REF: lazy logger setup to avoid expensive imports during test
-if not logging.getLogger().handlers and not os.getenv("PYTEST_RUNNING"):
-    from ai_trading.logging import setup_logging  # AI-AGENT-REF: lazy logger import
-
+LOG_PATH = os.getenv('BOT_LOG_FILE', 'logs/scheduler.log')
+logger = logging.getLogger(__name__)
+if not logging.getLogger().handlers and (not os.getenv('PYTEST_RUNNING')):
+    from ai_trading.logging import setup_logging
     setup_logging(log_file=LOG_PATH)
 
-
-# Handling missing portfolio weights function
 def ensure_portfolio_weights(ctx, symbols):
     """Ensure portfolio weights are computed with fallback handling."""
     try:
         from ai_trading import portfolio
-
-        if hasattr(portfolio, "compute_portfolio_weights"):
+        if hasattr(portfolio, 'compute_portfolio_weights'):
             return portfolio.compute_portfolio_weights(ctx, symbols)
         else:
-            logger.warning(
-                "compute_portfolio_weights not found, using fallback method."
-            )
-            # Placeholder fallback: Evenly distribute portfolio weights
+            logger.warning('compute_portfolio_weights not found, using fallback method.')
             return {symbol: 1.0 / len(symbols) for symbol in symbols}
     except Exception as e:
-        logger.error(f"Error computing portfolio weights: {e}, using fallback")
+        logger.error(f'Error computing portfolio weights: {e}, using fallback')
         return {symbol: 1.0 / len(symbols) for symbol in symbols if symbols}
-
-
-# Log Alpaca availability on startup
-logger.info("Alpaca SDK is available")
-# Mirror config to maintain historical constant name
+logger.info('Alpaca SDK is available')
 MIN_CYCLE = S.scheduler_sleep_seconds
-# AI-AGENT-REF: guard environment validation with explicit error logging
-# AI-AGENT-REF: Move config validation to runtime to prevent import crashes
-# Config validation moved to init_runtime_config()
-# This ensures imports don't fail due to missing environment variables
-
 try:
-    # Only import config module, don't validate at import time
     from ai_trading.config.settings import get_settings
-
-    logger.info("Config settings loaded, validation deferred to runtime")
+    logger.info('Config settings loaded, validation deferred to runtime')
 except Exception as e:
-    logger.warning("Config settings import failed: %s", e)
-
-# Provide a no-op ``profile`` decorator when line_profiler is not active.
+    logger.warning('Config settings import failed: %s', e)
 try:
-    profile  # type: ignore[name-defined]
-except NameError:  # pragma: no cover - used only when kernprof is absent
-
-    def profile(func):  # type: ignore[return-type]
+    profile
+except NameError:
+
+    def profile(func):
         return func
-
 
 def handle_exception(exc_type, exc_value, exc_traceback):
     if issubclass(exc_type, KeyboardInterrupt):
         sys.__excepthook__(exc_type, exc_value, exc_traceback)
         return
-    "".join(
-        traceback.format_exception(exc_type, exc_value, exc_traceback)
-    )
-    logging.critical(
-        "Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback)
-    )
-    # AI-AGENT-REF: flush and close log handlers to preserve logs on crash
+    ''.join(traceback.format_exception(exc_type, exc_value, exc_traceback))
+    logging.critical('Uncaught exception', exc_info=(exc_type, exc_value, exc_traceback))
     for h in logging.getLogger().handlers:
         try:
             h.flush()
             h.close()
         except (AttributeError, OSError) as e:
-            # Log handler cleanup issues but continue shutdown process
-            logger.warning("Failed to close logging handler: %s", e)
+            logger.warning('Failed to close logging handler: %s', e)
     logging.shutdown()
-
-
 sys.excepthook = handle_exception
-
 import warnings
-
-warnings.filterwarnings(
-    "ignore",
-    message=".*invalid escape sequence.*",
-    category=SyntaxWarning,
-    module="pandas_ta.*",
-)
-
-
-
-
-
-# Import pandas directly as it's a hard dependency
+warnings.filterwarnings('ignore', message='.*invalid escape sequence.*', category=SyntaxWarning, module='pandas_ta.*')
 import pandas as pd
-
-
-
-
-
-
-
-
 from ai_trading import utils
-
-# AI-AGENT-REF: lazy import heavy feature computation modules to speed up import for tests
-if not os.getenv("PYTEST_RUNNING"):
-    from ai_trading.features.indicators import (
-        compute_atr,
-        compute_macd,
-        compute_macds,
-        compute_vwap,
-        ensure_columns,
-    )
+if not os.getenv('PYTEST_RUNNING'):
+    from ai_trading.features.indicators import compute_atr, compute_macd, compute_macds, compute_vwap, ensure_columns
 else:
-    # AI-AGENT-REF: mock feature functions for test environments to avoid slow imports
+
     def compute_macd(*args, **kwargs):
-        return [0.0] * 20  # Mock MACD values
+        return [0.0] * 20
 
     def compute_atr(*args, **kwargs):
-        return [1.0] * 20  # Mock ATR values
+        return [1.0] * 20
 
     def compute_vwap(*args, **kwargs):
-        return [100.0] * 20  # Mock VWAP values
+        return [100.0] * 20
 
     def compute_macds(*args, **kwargs):
-        return [0.0] * 20  # Mock MACD signal values
+        return [0.0] * 20
 
     def ensure_columns(*args, **kwargs):
-        return args[0] if args else {}  # Mock column ensurer
-
-
+        return args[0] if args else {}
 try:
     from sklearn.exceptions import InconsistentVersionWarning
-except Exception:  # pragma: no cover - sklearn optional
+except Exception:
 
     class InconsistentVersionWarning(UserWarning):
         pass
-
-
-warnings.filterwarnings(
-    "ignore",
-    message="pkg_resources is deprecated as an API.*",
-    category=UserWarning,
-)
-warnings.filterwarnings("ignore", category=InconsistentVersionWarning)
-warnings.filterwarnings(
-    "ignore",
-    message="Converting to PeriodArray/Index representation will drop timezone information.*",
-    category=UserWarning,
-)
-
+warnings.filterwarnings('ignore', message='pkg_resources is deprecated as an API.*', category=UserWarning)
+warnings.filterwarnings('ignore', category=InconsistentVersionWarning)
+warnings.filterwarnings('ignore', message='Converting to PeriodArray/Index representation will drop timezone information.*', category=UserWarning)
 import os
-
-if "ALPACA_API_KEY" in os.environ:
-    os.environ.setdefault("APCA_API_KEY_ID", os.environ["ALPACA_API_KEY"])
-if "ALPACA_SECRET_KEY" in os.environ:
-    os.environ.setdefault("APCA_API_SECRET_KEY", os.environ["ALPACA_SECRET_KEY"])
-
-
-# Refresh environment variables on startup for reliability
+if 'ALPACA_API_KEY' in os.environ:
+    os.environ.setdefault('APCA_API_KEY_ID', os.environ['ALPACA_API_KEY'])
+if 'ALPACA_SECRET_KEY' in os.environ:
+    os.environ.setdefault('APCA_API_SECRET_KEY', os.environ['ALPACA_SECRET_KEY'])
 config.reload_env()
-
-# BOT_MODE must be defined before any classes that reference it
 BOT_MODE = S.bot_mode
-assert BOT_MODE is not None, "BOT_MODE must be set before using BotState"
+assert BOT_MODE is not None, 'BOT_MODE must be set before using BotState'
 import csv
 import json
 import logging
@@ -284,34 +158,22 @@ from threading import Lock, Semaphore, Thread
 from typing import Any
 from zoneinfo import ZoneInfo
-
-# Set deterministic random seeds for reproducibility
 SEED = S.seed
 random.seed(SEED)
-# AI-AGENT-REF: guard numpy random seed for test environments
-if hasattr(np, "random"):
+if hasattr(np, 'random'):
     np.random.seed(SEED)
-
-# AI-AGENT-REF: throttle SKIP_COOLDOWN logs
 _LAST_SKIP_CD_TIME = 0.0
 _LAST_SKIP_SYMBOLS: frozenset[str] = frozenset()
 import torch
-
 torch.manual_seed(SEED)
-
-_DEFAULT_FEED = get_settings().alpaca_data_feed or "iex"
-
-# Ensure numpy.NaN exists for pandas_ta compatibility
-# AI-AGENT-REF: guard numpy.NaN assignment for test environments
-if hasattr(np, "nan"):
+_DEFAULT_FEED = get_settings().alpaca_data_feed or 'iex'
+if hasattr(np, 'nan'):
     np.NaN = np.nan
-
 import importlib
 from functools import cache
 
-
-# AI-AGENT-REF: lazy load heavy modules when first accessed
 class _LazyModule(types.ModuleType):
+
     def __init__(self, name: str) -> None:
         super().__init__(name)
         self._module = None
@@ -319,193 +181,120 @@         self._failed = False
 
     def _load(self):
-        if self._module is None and not self._failed:
+        if self._module is None and (not self._failed):
             try:
                 self._module = importlib.import_module(self.__name__)
             except ImportError:
-                logger.warning(f"Module {self.__name__} not available - using fallback")
+                logger.warning(f'Module {self.__name__} not available - using fallback')
                 self._failed = True
-                # Create a fallback module
                 self._module = self._create_fallback()
 
     def _create_fallback(self):
         """Create a fallback module object with common methods."""
 
         class FallbackModule:
-            def __getattr__(self, name):
-                # Return a dummy function that returns sensible defaults
-                def dummy_func(*args, **kwargs):
-                    if "dataframe" in name.lower() or "df" in name.lower():
-                        return pd.DataFrame()  # Return empty DataFrame
-                    return None
-
-                return dummy_func
 
             def ichimoku(self, *args, **kwargs):
-                return pd.DataFrame(), {}
-
+                return (pd.DataFrame(), {})
         return FallbackModule()
-
-    def __getattr__(self, item):
-        self._load()
-        return getattr(self._module, item)
-
-
-# AI-AGENT-REF: use our improved lazy loading instead of _LazyModule for pandas
-# pd = _LazyModule("pandas")  # Commented out to use our LazyPandas implementation
-mcal = _LazyModule("pandas_market_calendars")
-ta = _LazyModule("pandas_ta")
-
+mcal = _LazyModule('pandas_market_calendars')
+ta = _LazyModule('pandas_ta')
 
 def limits(*args, **kwargs):
+
     def decorator(func):
+
         def wrapped(*a, **k):
             from ratelimit import limits as _limits
-
             return _limits(*args, **kwargs)(func)(*a, **k)
-
         return wrapped
-
     return decorator
 
-
 def sleep_and_retry(func):
+
     def wrapped(*a, **k):
         from ratelimit import sleep_and_retry as _sr
-
         return _sr(func)(*a, **k)
-
     return wrapped
 
-
 def retry(*dargs, **dkwargs):
+
     def decorator(func):
+
         def wrapped(*a, **k):
             from tenacity import retry as _retry
-
             return _retry(*dargs, **dkwargs)(func)(*a, **k)
-
         return wrapped
-
     return decorator
-
 
 def stop_after_attempt(*args, **kwargs):
     from tenacity import stop_after_attempt as _saa
-
     return _saa(*args, **kwargs)
-
 
 def wait_exponential(*args, **kwargs):
     from tenacity import wait_exponential as _we
-
     return _we(*args, **kwargs)
-
 
 def wait_random(*args, **kwargs):
     from tenacity import wait_random as _wr
-
     return _wr(*args, **kwargs)
-
 
 def retry_if_exception_type(*args, **kwargs):
     from tenacity import retry_if_exception_type as _riet
-
     return _riet(*args, **kwargs)
-
-
-# Tenacity retry error import with validation
 try:
-    # Import RetryError from tenacity.  In some test environments Tenacity may be
-    # monkeypatched so that RetryError is not actually an exception class.  To
-    # avoid ``TypeError: catching classes that do not inherit from BaseException``
-    # when using ``except RetryError``, verify that the imported symbol is a
-    # proper exception type.  Fall back to a simple Exception subclass when
-    # Tenacity is unavailable or invalid.
-    from tenacity import RetryError as _TenacityRetryError  # type: ignore[assignment]
-
-    if not isinstance(_TenacityRetryError, type) or not issubclass(
-        _TenacityRetryError, BaseException
-    ):
-        raise TypeError("Invalid RetryError type")
-    RetryError = _TenacityRetryError  # type: ignore[assignment]
+    from tenacity import RetryError as _TenacityRetryError
+    if not isinstance(_TenacityRetryError, type) or not issubclass(_TenacityRetryError, BaseException):
+        raise TypeError('Invalid RetryError type')
+    RetryError = _TenacityRetryError
 except Exception:
 
-    class RetryError(
-        Exception
-    ):  # pragma: no cover - fallback when tenacity.RetryError is invalid
+    class RetryError(Exception):
         """Fallback RetryError used when Tenacity's RetryError is unavailable or not an exception."""
-
-
-# AI-AGENT-REF: lazy ichimoku setup to avoid pandas_ta import in tests
-if not os.getenv("PYTEST_RUNNING"):
-    ta.ichimoku = (
-        ta.ichimoku if hasattr(ta, "ichimoku") else lambda *a, **k: (pd.DataFrame(), {})
-    )
+if not os.getenv('PYTEST_RUNNING'):
+    ta.ichimoku = ta.ichimoku if hasattr(ta, 'ichimoku') else lambda *a, **k: (pd.DataFrame(), {})
 else:
-    # AI-AGENT-REF: mock ichimoku for test environments
+
     def mock_ichimoku(*a, **k):
         return (pd.DataFrame(), {})
-
     ta.ichimoku = mock_ichimoku
-
 _MARKET_SCHEDULE = None
-
 
 def get_market_schedule():
     global _MARKET_SCHEDULE
     if _MARKET_SCHEDULE is None:
-        # AI-AGENT-REF: Handle testing environment where NY is SimpleNamespace without schedule()
-        if hasattr(NY, "schedule"):
-            _MARKET_SCHEDULE = NY.schedule(
-                start_date="2020-01-01", end_date="2030-12-31"
-            )
+        if hasattr(NY, 'schedule'):
+            _MARKET_SCHEDULE = NY.schedule(start_date='2020-01-01', end_date='2030-12-31')
         else:
-            # Return empty DataFrame for testing environments
             _MARKET_SCHEDULE = pd.DataFrame()
     return _MARKET_SCHEDULE
-
-
 _MARKET_CALENDAR = None
-
 
 def get_market_calendar():
     """Lazy-load the NYSE calendar itself (but not its full schedule)."""
     global _MARKET_CALENDAR
     if _MARKET_CALENDAR is None:
         import pandas_market_calendars as mcal
-        _MARKET_CALENDAR = mcal.get_calendar("NYSE")
+        _MARKET_CALENDAR = mcal.get_calendar('NYSE')
     return _MARKET_CALENDAR
-
-
-# AI-AGENT-REF: Only initialize market calendar in non-test environments to avoid import issues
 import os
-
-if not os.getenv("TESTING"):
-    # back-compat for existing code references
+if not os.getenv('TESTING'):
     NY = get_market_calendar()
 else:
-    # Provide a test-friendly stub
     import types
-
     NY = types.SimpleNamespace()
     NY.is_session_open = lambda dt: True
     NY.sessions_in_range = lambda start, end: []
-
-
 _FULL_DATETIME_RANGE = None
-
 
 def get_full_datetime_range():
     global _FULL_DATETIME_RANGE
     if _FULL_DATETIME_RANGE is None:
-        _FULL_DATETIME_RANGE = pd.date_range(start="09:30", end="16:00", freq="1T")
+        _FULL_DATETIME_RANGE = pd.date_range(start='09:30', end='16:00', freq='1T')
     return _FULL_DATETIME_RANGE
 
-
-# AI-AGENT-REF: add simple timeout helper for API calls
 @contextmanager
-def timeout_protection(seconds: int = 30):
+def timeout_protection(seconds: int=30):
     """
     Context manager to enforce timeouts on operations.
 
@@ -517,212 +306,116 @@     installing an alarm, preventing crashes in threaded environments.
     """
     import threading
-
-    # Only install SIGALRM in the main thread if available
-    if threading.current_thread() is threading.main_thread() and hasattr(
-        signal, "SIGALRM"
-    ):
+    if threading.current_thread() is threading.main_thread() and hasattr(signal, 'SIGALRM'):
 
         def timeout_handler(signum, frame):
-            raise TimeoutError(f"Operation timed out after {seconds} seconds")
-
+            raise TimeoutError(f'Operation timed out after {seconds} seconds')
         old_handler = signal.signal(signal.SIGALRM, timeout_handler)
         signal.alarm(seconds)
         try:
             yield
         finally:
-            # Disable alarm and restore handler
             signal.alarm(0)
             signal.signal(signal.SIGALRM, old_handler)
     else:
-        # Non-main thread or no SIGALRM support: no-op
         try:
             yield
         finally:
             pass
 
-
 @cache
 def is_holiday(ts: pd.Timestamp) -> bool:
-    # Compare only dates, not full timestamps, to handle schedule timezones correctly
     dt = pd.Timestamp(ts).date()
-    # Precompute set of valid trading dates (as dates) once
     trading_dates = {d.date() for d in get_market_schedule().index}
     return dt not in trading_dates
-
-
-# AI-AGENT-REF: lazy import heavy signal calculation module to speed up import for tests
-if not os.getenv("PYTEST_RUNNING"):
-    from ai_trading.signals import (
-        calculate_macd as signals_calculate_macd,  # type: ignore
-    )
+if not os.getenv('PYTEST_RUNNING'):
+    from ai_trading.signals import calculate_macd as signals_calculate_macd
 else:
-    # AI-AGENT-REF: mock signals_calculate_macd for test environments
+
     def signals_calculate_macd(*args, **kwargs):
-        return [0.0] * 20  # Mock MACD signal values
-
-
-# FutureWarning now filtered globally in pytest.ini
-
-# AI-AGENT-REF: portalocker is a hard dependency in pyproject.toml
+        return [0.0] * 20
 import portalocker
-# The `requests` library and its exceptions may be monkeypatched or absent in some
-# test environments.  Attempt to import them normally but fall back to simple
-# stand-ins when unavailable.  Without this guard an ImportError here would
-# prevent the module from importing, which in turn would cause unrelated code
-# (e.g. FinBERT sentiment loading) to fail at import time.
 try:
-    import requests  # type: ignore[assignment]
-    from requests import Session  # type: ignore[assignment]
-    from requests.exceptions import HTTPError  # type: ignore[assignment]
-except (
-    Exception
-):  # pragma: no cover - fallback when requests is missing or partially mocked
+    import requests
+    from requests import Session
+    from requests.exceptions import HTTPError
+except Exception:
     import types
-
-    requests = types.SimpleNamespace(
-        Session=lambda *a, **k: types.SimpleNamespace(get=lambda *a, **k: None),
-        get=lambda *a, **k: None,
-        exceptions=types.SimpleNamespace(
-            RequestException=Exception,
-            HTTPError=Exception,
-        ),
-    )
-    Session = requests.Session  # type: ignore[assignment]
-    HTTPError = Exception  # type: ignore[assignment]
-
-# AI-AGENT-REF: schedule is a hard dependency in pyproject.toml
+    requests = types.SimpleNamespace(Session=lambda *a, **k: types.SimpleNamespace(get=lambda *a, **k: None), get=lambda *a, **k: None, exceptions=types.SimpleNamespace(RequestException=Exception, HTTPError=Exception))
+    Session = requests.Session
+    HTTPError = Exception
 import schedule
-
-# AI-AGENT-REF: yfinance is a hard dependency in pyproject.toml
 import yfinance as yf
-
 YFINANCE_AVAILABLE = True
-
-# Production imports - real Alpaca SDK
 from alpaca.data.historical import StockHistoricalDataClient
 from alpaca.data.models import Quote
 from alpaca.data.requests import StockBarsRequest, StockLatestQuoteRequest
 from alpaca.data.timeframe import TimeFrame
 from alpaca.trading.client import TradingClient
-from alpaca.trading.enums import (
-    OrderSide,
-    OrderStatus,
-    QueryOrderStatus,
-    TimeInForce,
-)
+from alpaca.trading.enums import OrderSide, OrderStatus, QueryOrderStatus, TimeInForce
 from alpaca.trading.models import Order
-from alpaca.trading.requests import (
-    GetOrdersRequest,
-    MarketOrderRequest,
-)
-from alpaca_trade_api.rest import (
-    APIError,  # kept for legacy exception compatibility
-)
-
-logger.info("Real Alpaca Trading SDK imported successfully")
-logger.debug("Production trading ready with Python %s", sys.version)
-
-# AI-AGENT-REF: beautifulsoup4 is a hard dependency in pyproject.toml
+from alpaca.trading.requests import GetOrdersRequest, MarketOrderRequest
+from alpaca_trade_api.rest import APIError
+logger.info('Real Alpaca Trading SDK imported successfully')
+logger.debug('Production trading ready with Python %s', sys.version)
 from bs4 import BeautifulSoup
-
-# AI-AGENT-REF: flask is a hard dependency in pyproject.toml  
 from flask import Flask
-
 from ai_trading.alpaca_api import alpaca_get, start_trade_updates_stream
-
-from ai_trading.rebalancer import (
-    maybe_rebalance as original_rebalance,  # type: ignore
-)
-
-
-# Use base URL from configuration
+from ai_trading.rebalancer import maybe_rebalance as original_rebalance
 ALPACA_BASE_URL = get_settings().alpaca_base_url
 import pickle
-
-# AI-AGENT-REF: Optional meta-learning — do not crash if unavailable
-if not os.getenv("PYTEST_RUNNING"):
-    try:
-        from ai_trading.meta_learning import optimize_signals  # type: ignore
+if not os.getenv('PYTEST_RUNNING'):
+    try:
+        from ai_trading.meta_learning import optimize_signals
     except Exception as _e:
-        logger.warning(
-            "Meta-learning unavailable (%s); proceeding without signal optimization", _e
-        )
-
-        def optimize_signals(signals, *a, **k):  # type: ignore[no-redef]
+        logger.warning('Meta-learning unavailable (%s); proceeding without signal optimization', _e)
+
+        def optimize_signals(signals, *a, **k):
             return signals
-
 else:
-    # AI-AGENT-REF: mock optimize_signals for test environments
+
     def optimize_signals(*args, **kwargs):
-        return args[0] if args else []  # Return signals as-is
+        return args[0] if args else []
 from ai_trading.telemetry.metrics_logger import log_metrics
-
-from ai_trading.pipeline import model_pipeline  # type: ignore
-
-# ML dependencies - sklearn is a hard dependency
+from ai_trading.pipeline import model_pipeline
 from sklearn.decomposition import PCA
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.linear_model import BayesianRidge, Ridge
-
 from ai_trading.utils import log_warning, model_lock, safe_to_datetime, validate_ohlcv
-
-# ai_trading/core/bot_engine.py:670 - Move retrain_meta_learner import to lazy location
-
 ALPACA_API_KEY = get_settings().alpaca_api_key
 ALPACA_SECRET_KEY = get_settings().alpaca_secret_key
-ALPACA_PAPER = getattr(config, "ALPACA_PAPER", None)
-validate_alpaca_credentials = getattr(config, "validate_alpaca_credentials", None)
+ALPACA_PAPER = getattr(config, 'ALPACA_PAPER', None)
+validate_alpaca_credentials = getattr(config, 'validate_alpaca_credentials', None)
 CONFIG_NEWS_API_KEY = get_settings().news_api_key
-# Support new sentiment API configuration with backwards compatibility
-CONFIG_SENTIMENT_API_KEY = (
-    get_settings().sentiment_api_key or CONFIG_NEWS_API_KEY
-)
+CONFIG_SENTIMENT_API_KEY = get_settings().sentiment_api_key or CONFIG_NEWS_API_KEY
 CONFIG_SENTIMENT_API_URL = get_settings().sentiment_api_url
 FINNHUB_API_KEY = get_settings().finnhub_api_key
-BOT_MODE_ENV = getattr(config, "BOT_MODE", BOT_MODE)
-RUN_HEALTHCHECK = getattr(config, "RUN_HEALTHCHECK", None)
-
+BOT_MODE_ENV = getattr(config, 'BOT_MODE', BOT_MODE)
+RUN_HEALTHCHECK = getattr(config, 'RUN_HEALTHCHECK', None)
 
 def _require_cfg(value: str | None, name: str) -> str:
     """Return value or load from config, retrying in production."""
     if value:
         return value
-
-    # In testing mode, return a dummy value
     if S.testing:
-        dummy_values = {
-            "ALPACA_API_KEY": "test_api_key",
-            "ALPACA_SECRET_KEY": "test_secret_key",
-            "BOT_MODE": "test",
-        }
-        return dummy_values.get(name, f"test_{name.lower()}")
-
-    if BOT_MODE_ENV == "production":
+        dummy_values = {'ALPACA_API_KEY': 'test_api_key', 'ALPACA_SECRET_KEY': 'test_secret_key', 'BOT_MODE': 'test'}
+        return dummy_values.get(name, f'test_{name.lower()}')
+    if BOT_MODE_ENV == 'production':
         while not value:
-            logger.critical("Missing %s; retrying in 60s", name)
+            logger.critical('Missing %s; retrying in 60s', name)
             time.sleep(60)
             config.reload_env()
             import importlib
-
             importlib.reload(config)
             value = getattr(config, name, None)
         return str(value)
-    raise RuntimeError(f"{name} must be defined in the configuration or environment")
-
-
-# Defer credential checks to runtime (avoid import-time crashes before .env loads)
+    raise RuntimeError(f'{name} must be defined in the configuration or environment')
+
 def _resolve_alpaca_env():
-    key = os.getenv("ALPACA_API_KEY") or os.getenv("APCA_API_KEY_ID")
-    secret = os.getenv("ALPACA_SECRET_KEY") or os.getenv("APCA_API_SECRET_KEY")
-    base_url = (
-        os.getenv("ALPACA_BASE_URL")
-        or os.getenv("APCA_API_BASE_URL")
-        or getattr(config, "ALPACA_BASE_URL", None)
-        or "https://paper-api.alpaca.markets"
-    )
-    return key, secret, base_url
-
+    key = os.getenv('ALPACA_API_KEY') or os.getenv('APCA_API_KEY_ID')
+    secret = os.getenv('ALPACA_SECRET_KEY') or os.getenv('APCA_API_SECRET_KEY')
+    base_url = os.getenv('ALPACA_BASE_URL') or os.getenv('APCA_API_BASE_URL') or getattr(config, 'ALPACA_BASE_URL', None) or 'https://paper-api.alpaca.markets'
+    return (key, secret, base_url)
 
 def _ensure_alpaca_env_or_raise():
     """
@@ -731,73 +424,39 @@     - Outside SHADOW_MODE, if missing key/secret, raise with a clear message.
     """
     k, s, b = _resolve_alpaca_env()
-    # Check both config and environment for SHADOW_MODE
-    shadow_mode = getattr(config, "SHADOW_MODE", False) or os.getenv(
-        "SHADOW_MODE", ""
-    ).lower() in ("true", "1")
+    shadow_mode = getattr(config, 'SHADOW_MODE', False) or os.getenv('SHADOW_MODE', '').lower() in ('true', '1')
     if shadow_mode:
-        return k, s, b
+        return (k, s, b)
     if not (k and s):
-        logger.critical("Alpaca credentials missing – aborting client initialization")
-        raise RuntimeError("Missing Alpaca API credentials")
-    return k, s, b
-
+        logger.critical('Alpaca credentials missing – aborting client initialization')
+        raise RuntimeError('Missing Alpaca API credentials')
+    return (k, s, b)
 
 def init_runtime_config():
     """Initialize runtime configuration and validate critical keys."""
     from ai_trading.config import Settings
-
     cfg = Settings()
-
-    # Validate critical keys at runtime, not import time
     global ALPACA_API_KEY, ALPACA_SECRET_KEY, BOT_MODE_ENV
-
-    # Use the new credential resolution functions
     try:
         ALPACA_API_KEY, ALPACA_SECRET_KEY, _ = _ensure_alpaca_env_or_raise()
     except RuntimeError as e:
-        if not S.testing:  # Allow missing credentials in test mode
+        if not S.testing:
             raise e
-        # AI-AGENT-REF: Use environment variables even in test mode to avoid hardcoded secrets
-        ALPACA_API_KEY = os.getenv("TEST_ALPACA_API_KEY", "")
-        ALPACA_SECRET_KEY = os.getenv("TEST_ALPACA_SECRET_KEY", "")
-
-    BOT_MODE_ENV = _require_cfg(getattr(cfg, "BOT_MODE", None), "BOT_MODE")
-
+        ALPACA_API_KEY = os.getenv('TEST_ALPACA_API_KEY', '')
+        ALPACA_SECRET_KEY = os.getenv('TEST_ALPACA_SECRET_KEY', '')
+    BOT_MODE_ENV = _require_cfg(getattr(cfg, 'BOT_MODE', None), 'BOT_MODE')
     if not callable(validate_alpaca_credentials):
-        raise RuntimeError("validate_alpaca_credentials not found in config")
-
-    logger.info(
-        "Runtime config initialized",
-        extra={
-            "alpaca_key_set": bool(ALPACA_API_KEY and len(ALPACA_API_KEY) > 8),
-            "bot_mode": BOT_MODE_ENV,
-        },
-    )
+        raise RuntimeError('validate_alpaca_credentials not found in config')
+    logger.info('Runtime config initialized', extra={'alpaca_key_set': bool(ALPACA_API_KEY and len(ALPACA_API_KEY) > 8), 'bot_mode': BOT_MODE_ENV})
     return cfg
-
-
-# Set module-level defaults that won't crash on import
 ALPACA_API_KEY = None
 ALPACA_SECRET_KEY = None
-BOT_MODE_ENV = "development"
-
+BOT_MODE_ENV = 'development'
 BASE_DIR = os.path.dirname(os.path.abspath(__file__))
-
-# AI-AGENT-REF: pybreaker is a hard dependency in pyproject.toml
 import pybreaker
-
-# AI-AGENT-REF: finnhub is a hard dependency in pyproject.toml
 from finnhub import FinnhubAPIException
-
-
-# AI-AGENT-REF: prometheus-client is a hard dependency in pyproject.toml
 from prometheus_client import REGISTRY, Counter, Gauge, Histogram, start_http_server
-
-
-# Prometheus metrics - lazy initialization to prevent duplicates
 _METRICS_READY = False
-
 
 def _init_metrics() -> None:
     """Create/register metrics once; tolerate partial imports & re-imports."""
@@ -808,63 +467,43 @@     if _METRICS_READY:
         return
     try:
-        orders_total = Counter("bot_orders_total", "Total orders sent")
-        order_failures = Counter("bot_order_failures", "Order submission failures")
-        daily_drawdown = Gauge("bot_daily_drawdown", "Current daily drawdown fraction")
-        signals_evaluated = Counter(
-            "bot_signals_evaluated_total", "Total signals evaluated"
-        )
-        run_all_trades_duration = Histogram(
-            "run_all_trades_duration_seconds", "Time spent in run_all_trades"
-        )
-        minute_cache_hit = Counter("bot_minute_cache_hits", "Minute bar cache hits")
-        minute_cache_miss = Counter(
-            "bot_minute_cache_misses", "Minute bar cache misses"
-        )
-        daily_cache_hit = Counter("bot_daily_cache_hits", "Daily bar cache hits")
-        daily_cache_miss = Counter("bot_daily_cache_misses", "Daily bar cache misses")
-        event_cooldown_hits = Counter("bot_event_cooldown_hits", "Event cooldown hits")
-        slippage_total = Counter("bot_slippage_total", "Cumulative slippage in cents")
-        slippage_count = Counter(
-            "bot_slippage_count", "Number of orders with slippage logged"
-        )
-        weekly_drawdown = Gauge(
-            "bot_weekly_drawdown", "Current weekly drawdown fraction"
-        )
-        skipped_duplicates = Counter(
-            "bot_skipped_duplicates",
-            "Trades skipped due to open position",
-        )
-        skipped_cooldown = Counter(
-            "bot_skipped_cooldown",
-            "Trades skipped due to recent execution",
-        )
+        orders_total = Counter('bot_orders_total', 'Total orders sent')
+        order_failures = Counter('bot_order_failures', 'Order submission failures')
+        daily_drawdown = Gauge('bot_daily_drawdown', 'Current daily drawdown fraction')
+        signals_evaluated = Counter('bot_signals_evaluated_total', 'Total signals evaluated')
+        run_all_trades_duration = Histogram('run_all_trades_duration_seconds', 'Time spent in run_all_trades')
+        minute_cache_hit = Counter('bot_minute_cache_hits', 'Minute bar cache hits')
+        minute_cache_miss = Counter('bot_minute_cache_misses', 'Minute bar cache misses')
+        daily_cache_hit = Counter('bot_daily_cache_hits', 'Daily bar cache hits')
+        daily_cache_miss = Counter('bot_daily_cache_misses', 'Daily bar cache misses')
+        event_cooldown_hits = Counter('bot_event_cooldown_hits', 'Event cooldown hits')
+        slippage_total = Counter('bot_slippage_total', 'Cumulative slippage in cents')
+        slippage_count = Counter('bot_slippage_count', 'Number of orders with slippage logged')
+        weekly_drawdown = Gauge('bot_weekly_drawdown', 'Current weekly drawdown fraction')
+        skipped_duplicates = Counter('bot_skipped_duplicates', 'Trades skipped due to open position')
+        skipped_cooldown = Counter('bot_skipped_cooldown', 'Trades skipped due to recent execution')
     except ValueError:
-        # Already registered (e.g., prior partial import). Reuse existing.
-        # Accessing REGISTRY internals is stable in prometheus-client; safe fallback.
         if REGISTRY is not None:
-            existing = getattr(REGISTRY, "_names_to_collectors", {})
-            orders_total = existing.get("bot_orders_total")
-            order_failures = existing.get("bot_order_failures")
-            daily_drawdown = existing.get("bot_daily_drawdown")
-            signals_evaluated = existing.get("bot_signals_evaluated_total")
-            run_all_trades_duration = existing.get("run_all_trades_duration_seconds")
-            minute_cache_hit = existing.get("bot_minute_cache_hits")
-            minute_cache_miss = existing.get("bot_minute_cache_misses")
-            daily_cache_hit = existing.get("bot_daily_cache_hits")
-            daily_cache_miss = existing.get("bot_daily_cache_misses")
-            event_cooldown_hits = existing.get("bot_event_cooldown_hits")
-            slippage_total = existing.get("bot_slippage_total")
-            slippage_count = existing.get("bot_slippage_count")
-            weekly_drawdown = existing.get("bot_weekly_drawdown")
-            skipped_duplicates = existing.get("bot_skipped_duplicates")
-            skipped_cooldown = existing.get("bot_skipped_cooldown")
+            existing = getattr(REGISTRY, '_names_to_collectors', {})
+            orders_total = existing.get('bot_orders_total')
+            order_failures = existing.get('bot_order_failures')
+            daily_drawdown = existing.get('bot_daily_drawdown')
+            signals_evaluated = existing.get('bot_signals_evaluated_total')
+            run_all_trades_duration = existing.get('run_all_trades_duration_seconds')
+            minute_cache_hit = existing.get('bot_minute_cache_hits')
+            minute_cache_miss = existing.get('bot_minute_cache_misses')
+            daily_cache_hit = existing.get('bot_daily_cache_hits')
+            daily_cache_miss = existing.get('bot_daily_cache_misses')
+            event_cooldown_hits = existing.get('bot_event_cooldown_hits')
+            slippage_total = existing.get('bot_slippage_total')
+            slippage_count = existing.get('bot_slippage_count')
+            weekly_drawdown = existing.get('bot_weekly_drawdown')
+            skipped_duplicates = existing.get('bot_skipped_duplicates')
+            skipped_cooldown = existing.get('bot_skipped_cooldown')
     _METRICS_READY = True
-
-
 try:
-    from ai_trading.trade_execution import ExecutionEngine  # type: ignore
-except Exception:  # pragma: no cover - allow tests with stubbed module
+    from ai_trading.trade_execution import ExecutionEngine
+except Exception:
 
     class ExecutionEngine:
         """
@@ -877,76 +516,54 @@         """
 
         def __init__(self, *args, **kwargs) -> None:
-            # Provide a logger specific to the stub
-            self._logger = logging.getLogger(__name__ + ".StubExecutionEngine")
+            self._logger = logging.getLogger(__name__ + '.StubExecutionEngine')
 
         def _log(self, method: str, *args, **kwargs) -> None:
-            self._logger.debug(
-                "StubExecutionEngine.%s called with args=%s kwargs=%s",
-                method,
-                args,
-                kwargs,
-            )
+            self._logger.debug('StubExecutionEngine.%s called with args=%s kwargs=%s', method, args, kwargs)
 
         def execute_order(self, symbol: str, qty: int, side: str):
             """Simulate an order execution and return a dummy order object."""
-            self._log("execute_order", symbol, qty, side)
-            # Return a simple namespace with an id attribute to mimic a real order
+            self._log('execute_order', symbol, qty, side)
             return types.SimpleNamespace(id=None)
 
-        # Provide empty hooks for cycle management used elsewhere in the code
         def start_cycle(self) -> None:
-            self._log("start_cycle")
+            self._log('start_cycle')
 
         def end_cycle(self) -> None:
-            self._log("end_cycle")
+            self._log('end_cycle')
 
         def check_trailing_stops(self) -> None:
             """Stub method for trailing stops check - used when real execution engine unavailable."""
-            self._log("check_trailing_stops")
-
-
+            self._log('check_trailing_stops')
 try:
     from ai_trading.capital_scaling import CapitalScalingEngine
-except Exception:  # pragma: no cover - allow tests with stubbed module
+except Exception:
 
     class CapitalScalingEngine:
+
         def __init__(self, *args, **kwargs):
             pass
 
         def scale_position(self, position):
             """Return ``position`` unchanged for smoke tests."""
-            # AI-AGENT-REF: stub passthrough for unit tests
             return position
 
-        def update(self, *args, **kwargs):  # AI-AGENT-REF: add missing update method
+        def update(self, *args, **kwargs):
             """Update method for test compatibility."""
 
-
 class StrategyAllocator:
+
     def __init__(self, *args, **kwargs):
-        # AI-AGENT-REF: delegate to underlying allocator for tests
         from strategy_allocator import StrategyAllocator as _Alloc
-
         self._alloc = _Alloc(*args, **kwargs)
 
     def allocate_signals(self, *args, **kwargs):
         return self._alloc.allocate(*args, **kwargs)
-
-    # tests do alloc.allocate(...), so alias that to the real method
     allocate = allocate_signals
-
-
-# AI-AGENT-REF: lazy import heavy data_fetcher module to speed up import for tests
-if not os.getenv("PYTEST_RUNNING"):
-    from ai_trading.data_fetcher import (  # type: ignore
-        _MINUTE_CACHE,
-        DataFetchError,
-        DataFetchException,
-        get_minute_df,
-    )
+if not os.getenv('PYTEST_RUNNING'):
+    from ai_trading.data_fetcher import _MINUTE_CACHE, DataFetchError, DataFetchException, get_minute_df
 else:
-    # AI-AGENT-REF: mock data_fetcher functions for test environments
+
     class DataFetchError(Exception):
         pass
 
@@ -954,346 +571,229 @@         pass
 
     def get_minute_df(*args, **kwargs):
-        return pd.DataFrame()  # Mock empty DataFrame
-
-    _MINUTE_CACHE = {}  # Mock cache
-
+        return pd.DataFrame()
+    _MINUTE_CACHE = {}
 try:
-    if not os.getenv("PYTEST_RUNNING"):
-        from ai_trading.data_fetcher import finnhub_client  # noqa: F401
+    if not os.getenv('PYTEST_RUNNING'):
+        from ai_trading.data_fetcher import finnhub_client
     else:
-        finnhub_client = None  # Mock client for tests
+        finnhub_client = None
 except Exception:
-    finnhub_client = None  # type: ignore
-
-# AI-AGENT-REF: Add cache size management to prevent memory leaks
+    finnhub_client = None
 _ML_MODEL_CACHE: dict[str, Any] = {}
-_ML_MODEL_CACHE_MAX_SIZE = 100  # Limit cache size to prevent memory issues
-
+_ML_MODEL_CACHE_MAX_SIZE = 100
 
 def _cleanup_ml_model_cache():
     """Clean up ML model cache if it gets too large."""
     global _ML_MODEL_CACHE
     if len(_ML_MODEL_CACHE) > _ML_MODEL_CACHE_MAX_SIZE:
-        # Keep only the most recently used items (simple LRU-like behavior)
-        # For now, just clear half the cache when it gets too large
         items_to_remove = len(_ML_MODEL_CACHE) // 2
         keys_to_remove = list(_ML_MODEL_CACHE.keys())[:items_to_remove]
         for key in keys_to_remove:
             _ML_MODEL_CACHE.pop(key, None)
-        logger.info("Cleaned up ML model cache, removed %d items", items_to_remove)
-
-
+        logger.info('Cleaned up ML model cache, removed %d items', items_to_remove)
 logger = logging.getLogger(__name__)
 
-
-# AI-AGENT-REF: helper for throttled SKIP_COOLDOWN logging
-def log_skip_cooldown(
-    symbols: Sequence[str] | str, state: BotState | None = None
-) -> None:
+def log_skip_cooldown(symbols: Sequence[str] | str, state: BotState | None=None) -> None:
     """Log SKIP_COOLDOWN once per unique set within 15 seconds."""
     global _LAST_SKIP_CD_TIME, _LAST_SKIP_SYMBOLS
     now = time.monotonic()
     sym_set = frozenset([symbols]) if isinstance(symbols, str) else frozenset(symbols)
     if sym_set != _LAST_SKIP_SYMBOLS or now - _LAST_SKIP_CD_TIME >= 15:
-        logger.info("SKIP_COOLDOWN | %s", ", ".join(sorted(sym_set)))
+        logger.info('SKIP_COOLDOWN | %s', ', '.join(sorted(sym_set)))
         _LAST_SKIP_CD_TIME = now
         _LAST_SKIP_SYMBOLS = sym_set
 
-
-def market_is_open(now: datetime | None = None) -> bool:
+def market_is_open(now: datetime | None=None) -> bool:
     from ai_trading.utils import is_market_open as utils_market_open
-
-    """Return True if the market is currently open."""
+    'Return True if the market is currently open.'
     try:
         with timeout_protection(10):
-            if os.getenv("FORCE_MARKET_OPEN", "false").lower() == "true":
-                logger.info(
-                    "FORCE_MARKET_OPEN is enabled; overriding market hours checks."
-                )
+            if os.getenv('FORCE_MARKET_OPEN', 'false').lower() == 'true':
+                logger.info('FORCE_MARKET_OPEN is enabled; overriding market hours checks.')
                 return True
             return utils_market_open(now)
     except TimeoutError:
-        logger.error("Market status check timed out, assuming market closed")
+        logger.error('Market status check timed out, assuming market closed')
         return False
     except Exception as e:
-        logger.error("Market status check failed: %s", e)
+        logger.error('Market status check failed: %s', e)
         return False
-
-
-# backward compatibility
 is_market_open = market_is_open
-
-
-# AI-AGENT-REF: snapshot live positions for debugging
-PORTFOLIO_FILE = "portfolio_snapshot.json"
-
+PORTFOLIO_FILE = 'portfolio_snapshot.json'
 
 def save_portfolio_snapshot(portfolio: dict[str, int]) -> None:
-    data = {
-        "timestamp": utc_now_iso(),  # AI-AGENT-REF: Use UTC timestamp utility
-        "positions": portfolio,
-    }
-    with open(PORTFOLIO_FILE, "w") as f:
+    data = {'timestamp': utc_now_iso(), 'positions': portfolio}
+    with open(PORTFOLIO_FILE, 'w') as f:
         json.dump(data, f, indent=2)
-
 
 def load_portfolio_snapshot() -> dict[str, int]:
     if not os.path.exists(PORTFOLIO_FILE):
         return {}
     with open(PORTFOLIO_FILE) as f:
         data = json.load(f)
-    return data.get("positions", {})
-
+    return data.get('positions', {})
 
 def compute_current_positions(ctx: BotContext) -> dict[str, int]:
     try:
         positions = ctx.api.get_all_positions()
-        logger.debug("Raw Alpaca positions: %s", positions)
+        logger.debug('Raw Alpaca positions: %s', positions)
         return {p.symbol: int(p.qty) for p in positions}
     except (AttributeError, ValueError, ConnectionError, TimeoutError) as e:
-        logger.warning("compute_current_positions failed: %s", e, exc_info=True)
+        logger.warning('compute_current_positions failed: %s', e, exc_info=True)
         return {}
-
 
 def maybe_rebalance(ctx):
     portfolio = compute_current_positions(ctx)
     save_portfolio_snapshot(portfolio)
     return original_rebalance(ctx)
 
-
 def get_latest_close(df: pd.DataFrame) -> float:
     """Return the last closing price or ``0.0`` if unavailable."""
-    # AI-AGENT-REF: debug output to understand test failure
-    logger.debug("get_latest_close called with df: %s", type(df).__name__)
-
-    # AI-AGENT-REF: More robust check that works with different pandas instances
+    logger.debug('get_latest_close called with df: %s', type(df).__name__)
     if df is None:
-        logger.debug("get_latest_close early return: df is None")
+        logger.debug('get_latest_close early return: df is None')
         return 0.0
-
-    # Check if df has empty attribute and columns attribute (duck typing)
     try:
         is_empty = df.empty
-        has_close = "close" in df.columns
+        has_close = 'close' in df.columns
     except (AttributeError, TypeError) as e:
-        logger.debug("get_latest_close: DataFrame methods failed: %s", e)
+        logger.debug('get_latest_close: DataFrame methods failed: %s', e)
         return 0.0
-
     if is_empty or not has_close:
-        logger.debug(
-            "get_latest_close early return: empty: %s, close in columns: %s",
-            is_empty,
-            has_close,
-        )
+        logger.debug('get_latest_close early return: empty: %s, close in columns: %s', is_empty, has_close)
         return 0.0
-
-    try:
-        last_valid_close = df["close"].dropna()
-        logger.debug(
-            "get_latest_close last_valid_close length: %d", len(last_valid_close)
-        )
-
+    try:
+        last_valid_close = df['close'].dropna()
+        logger.debug('get_latest_close last_valid_close length: %d', len(last_valid_close))
         if not last_valid_close.empty:
             price = last_valid_close.iloc[-1]
-            logger.debug(
-                "get_latest_close price from iloc[-1]: %s (type: %s)",
-                price,
-                type(price).__name__,
-            )
+            logger.debug('get_latest_close price from iloc[-1]: %s (type: %s)', price, type(price).__name__)
         else:
-            logger.critical("All NaNs in close column for get_latest_close")
+            logger.critical('All NaNs in close column for get_latest_close')
             price = 0.0
-
-        # More robust NaN check that works with different pandas instances
-        if price is None or (hasattr(price, "__ne__") and price != price) or price <= 0:
-            logger.debug("get_latest_close price is NaN or <= 0: price=%s", price)
+        if price is None or (hasattr(price, '__ne__') and price != price) or price <= 0:
+            logger.debug('get_latest_close price is NaN or <= 0: price=%s', price)
             return 0.0
-
         result = float(price)
-        logger.debug("get_latest_close returning: %s", result)
+        logger.debug('get_latest_close returning: %s', result)
         return result
-
     except Exception as e:
-        logger.warning("get_latest_close exception: %s", e)
+        logger.warning('get_latest_close exception: %s', e)
         return 0.0
-
 
 def compute_time_range(minutes: int) -> tuple[datetime, datetime]:
     """Return a UTC datetime range spanning the past ``minutes`` minutes."""
-    # AI-AGENT-REF: provide timezone-aware datetimes
     now = datetime.now(UTC)
     start = now - timedelta(minutes=minutes)
-    return start, now
-
+    return (start, now)
 
 def safe_price(price: float) -> float:
     """Defensively clamp ``price`` to a minimal positive value."""
-    # AI-AGENT-REF: prevent invalid zero/negative prices
-    return max(price, 1e-3)
-
-
-# AI-AGENT-REF: utility to detect row drops during feature engineering
-def assert_row_integrity(
-    before_len: int, after_len: int, func_name: str, symbol: str
-) -> None:
+    return max(price, 0.001)
+
+def assert_row_integrity(before_len: int, after_len: int, func_name: str, symbol: str) -> None:
     if after_len < before_len:
-        logger.warning(
-            f"Row count dropped in {func_name} for {symbol}: {before_len} -> {after_len}"
-        )
-
+        logger.warning(f'Row count dropped in {func_name} for {symbol}: {before_len} -> {after_len}')
 
 def _load_ml_model(symbol: str):
     """Return preloaded ML model from ``ML_MODELS`` cache."""
-
-    # AI-AGENT-REF: Check cache size and cleanup if needed
     _cleanup_ml_model_cache()
-
     cached = _ML_MODEL_CACHE.get(symbol)
     if cached is not None:
         return cached
-
     model = ML_MODELS.get(symbol)
     if model is not None:
         _ML_MODEL_CACHE[symbol] = model
     return model
 
-
 def fetch_minute_df_safe(symbol: str) -> pd.DataFrame:
     """Fetch the last day of minute bars and raise on empty."""
-    # AI-AGENT-REF: raise on empty DataFrame
     now_utc = datetime.now(UTC)
     start_dt = now_utc - timedelta(days=1)
     df = get_minute_df(symbol, start_dt, now_utc)
     if df.empty:
-        logger.error(f"Fetch failed: empty DataFrame for {symbol}")
-        raise DataFetchError(f"No data for {symbol}")
-
-    # Check data freshness before proceeding with trading logic
-    try:
-        # Allow data up to 10 minutes old during market hours (600 seconds)
+        logger.error(f'Fetch failed: empty DataFrame for {symbol}')
+        raise DataFetchError(f'No data for {symbol}')
+    try:
         _ensure_data_fresh(symbols=[symbol], max_age_seconds=600)
     except RuntimeError as e:
-        logger.warning(f"Data staleness check failed for {symbol}: {e}")
-        # Still return the data but log the staleness issue
-
+        logger.warning(f'Data staleness check failed for {symbol}: {e}')
     return df
-
 
 def cancel_all_open_orders(ctx: BotContext) -> None:
     """
     On startup or each run, cancel every Alpaca order whose status is 'open'.
     """
     if ctx.api is None:
-        logger.warning("ctx.api is None - cannot cancel orders")
+        logger.warning('ctx.api is None - cannot cancel orders')
         return
-
     try:
         req = GetOrdersRequest(status=QueryOrderStatus.OPEN)
         open_orders = ctx.api.get_orders(req)
         if not open_orders:
             return
         for od in open_orders:
-            if getattr(od, "status", "").lower() == "open":
+            if getattr(od, 'status', '').lower() == 'open':
                 try:
                     ctx.api.cancel_order_by_id(od.id)
                 except Exception as exc:
-                    logger.exception(
-                        "Failed to cancel order %s",
-                        getattr(od, "id", "unknown"),
-                        exc_info=exc,
-                    )
+                    logger.exception('Failed to cancel order %s', getattr(od, 'id', 'unknown'), exc_info=exc)
     except Exception as exc:
-        logger.warning("Failed to cancel open orders: %s", exc, exc_info=True)
-
+        logger.warning('Failed to cancel open orders: %s', exc, exc_info=True)
 
 def reconcile_positions(ctx: BotContext) -> None:
     """On startup, fetch all live positions and clear any in-memory stop/take targets for assets no longer held."""
     try:
-        live_positions = {
-            pos.symbol: int(pos.qty) for pos in ctx.api.get_all_positions()
-        }
+        live_positions = {pos.symbol: int(pos.qty) for pos in ctx.api.get_all_positions()}
         with targets_lock:
-            symbols_with_targets = list(ctx.stop_targets.keys()) + list(
-                ctx.take_profit_targets.keys()
-            )
+            symbols_with_targets = list(ctx.stop_targets.keys()) + list(ctx.take_profit_targets.keys())
             for symbol in symbols_with_targets:
                 if symbol not in live_positions or live_positions[symbol] == 0:
                     ctx.stop_targets.pop(symbol, None)
                     ctx.take_profit_targets.pop(symbol, None)
     except Exception as exc:
-        logger.exception("reconcile_positions failed", exc_info=exc)
-
-
+        logger.exception('reconcile_positions failed', exc_info=exc)
 import warnings
-
-# ─── A. CONFIGURATION CONSTANTS ─────────────────────────────────────────────────
-RUN_HEALTH = RUN_HEALTHCHECK == "1"
-
-# Logging: set root logger to INFO, send to both stderr and a log file
-logging.getLogger("alpaca_trade_api").setLevel(logging.WARNING)
-logging.getLogger("urllib3").setLevel(logging.WARNING)
-logging.getLogger("requests").setLevel(logging.WARNING)
-
-# Suppress specific pandas_ta warnings
-warnings.filterwarnings(
-    "ignore", message=".*valid feature names.*", category=UserWarning
-)
-
-# ─── FINBERT SENTIMENT MODEL IMPORTS & FALLBACK ─────────────────────────────────
-# Load FinBERT unless explicitly disabled
+RUN_HEALTH = RUN_HEALTHCHECK == '1'
+logging.getLogger('alpaca_trade_api').setLevel(logging.WARNING)
+logging.getLogger('urllib3').setLevel(logging.WARNING)
+logging.getLogger('requests').setLevel(logging.WARNING)
+warnings.filterwarnings('ignore', message='.*valid feature names.*', category=UserWarning)
 try:
     import torch
-
     with warnings.catch_warnings():
-        warnings.filterwarnings(
-            "ignore",
-            message=".*_register_pytree_node.*",
-            module="transformers.*",
-        )
+        warnings.filterwarnings('ignore', message='.*_register_pytree_node.*', module='transformers.*')
         from transformers import AutoModelForSequenceClassification, AutoTokenizer
-
-    _FINBERT_TOKENIZER = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
-    _FINBERT_MODEL = AutoModelForSequenceClassification.from_pretrained(
-        "yiyanghkust/finbert-tone"
-    )
+    _FINBERT_TOKENIZER = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')
+    _FINBERT_MODEL = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')
     _FINBERT_MODEL.eval()
     _HUGGINGFACE_AVAILABLE = True
-    logger.info("FinBERT loaded successfully")
+    logger.info('FinBERT loaded successfully')
 except Exception as e:
     _HUGGINGFACE_AVAILABLE = False
     _FINBERT_TOKENIZER = None
     _FINBERT_MODEL = None
-    logger.warning(f"FinBERT load failed ({e}); falling back to neutral sentiment")
-
-
+    logger.warning(f'FinBERT load failed ({e}); falling back to neutral sentiment')
 DISASTER_DD_LIMIT = S.disaster_dd_limit
-
-# Paths
 BASE_DIR = os.path.dirname(os.path.abspath(__file__))
-# PROJECT_ROOT: repo root (…/ai_trading/core/ -> up two levels)
 PROJECT_ROOT = Path(__file__).resolve().parents[2]
-
 
 def abspath(fname: str) -> str:
     """Path within core/ directory."""
     return os.path.join(BASE_DIR, fname)
 
-
 def abspath_repo_root(fname: str) -> str:
     """Path relative to repository root."""
     return str(PROJECT_ROOT.joinpath(fname))
 
-
 def atomic_joblib_dump(obj, path: str) -> None:
     """Safely write joblib file using atomic replace."""
     import tempfile
-
     import joblib
-
     dir_name = os.path.dirname(path)
     os.makedirs(dir_name, exist_ok=True)
-    fd, tmp_path = tempfile.mkstemp(dir=dir_name, suffix=".tmp")
+    fd, tmp_path = tempfile.mkstemp(dir=dir_name, suffix='.tmp')
     os.close(fd)
     try:
         joblib.dump(obj, tmp_path)
@@ -1302,308 +802,195 @@         if os.path.exists(tmp_path):
             os.remove(tmp_path)
 
-
 def atomic_pickle_dump(obj, path: str) -> None:
     """Safely pickle object to path with atomic replace."""
     import tempfile
-
     dir_name = os.path.dirname(path)
     os.makedirs(dir_name, exist_ok=True)
-    fd, tmp_path = tempfile.mkstemp(dir=dir_name, suffix=".tmp")
+    fd, tmp_path = tempfile.mkstemp(dir=dir_name, suffix='.tmp')
     os.close(fd)
     try:
-        with open(tmp_path, "wb") as f:
+        with open(tmp_path, 'wb') as f:
             pickle.dump(obj, f)
         os.replace(tmp_path, path)
     finally:
         if os.path.exists(tmp_path):
             os.remove(tmp_path)
 
-
 def get_git_hash() -> str:
     """Return current git commit short hash if available."""
     try:
         import subprocess
-
-        return subprocess.run(
-            ["git", "rev-parse", "--short", "HEAD"],
-            check=True,
-            timeout=10,
-            capture_output=True,
-            text=True,
-        ).stdout.strip()
+        return subprocess.run(['git', 'rev-parse', '--short', 'HEAD'], check=True, timeout=10, capture_output=True, text=True).stdout.strip()
     except Exception:
-        return "unknown"
-
-
-# Tickers file resides at repo root by convention
-TICKERS_FILE = abspath_repo_root("tickers.csv")
-# AI-AGENT-REF: use centralized trade log path
+        return 'unknown'
+TICKERS_FILE = abspath_repo_root('tickers.csv')
 TRADE_LOG_FILE = S.trade_log_file
-SIGNAL_WEIGHTS_FILE = str(paths.DATA_DIR / "signal_weights.csv")
-EQUITY_FILE = str(paths.DATA_DIR / "last_equity.txt")
-PEAK_EQUITY_FILE = str(paths.DATA_DIR / "peak_equity.txt")
-HALT_FLAG_PATH = str(paths.DATA_DIR / "halt.flag")
-SLIPPAGE_LOG_FILE = str(paths.LOG_DIR / "slippage.csv")
-REWARD_LOG_FILE = str(paths.LOG_DIR / "reward_log.csv")
-FEATURE_PERF_FILE = abspath("feature_perf.csv")
-INACTIVE_FEATURES_FILE = abspath("inactive_features.json")
-
-# Hyperparameter files (repo root, not core/)
-HYPERPARAMS_FILE = abspath_repo_root("hyperparams.json")
-BEST_HYPERPARAMS_FILE = abspath_repo_root("best_hyperparams.json")
-
+SIGNAL_WEIGHTS_FILE = str(paths.DATA_DIR / 'signal_weights.csv')
+EQUITY_FILE = str(paths.DATA_DIR / 'last_equity.txt')
+PEAK_EQUITY_FILE = str(paths.DATA_DIR / 'peak_equity.txt')
+HALT_FLAG_PATH = str(paths.DATA_DIR / 'halt.flag')
+SLIPPAGE_LOG_FILE = str(paths.LOG_DIR / 'slippage.csv')
+REWARD_LOG_FILE = str(paths.LOG_DIR / 'reward_log.csv')
+FEATURE_PERF_FILE = abspath('feature_perf.csv')
+INACTIVE_FEATURES_FILE = abspath('inactive_features.json')
+HYPERPARAMS_FILE = abspath_repo_root('hyperparams.json')
+BEST_HYPERPARAMS_FILE = abspath_repo_root('best_hyperparams.json')
 
 def load_hyperparams() -> dict:
     """Load hyperparameters from best_hyperparams.json if present, else default."""
-    path = (
-        BEST_HYPERPARAMS_FILE
-        if os.path.exists(BEST_HYPERPARAMS_FILE)
-        else HYPERPARAMS_FILE
-    )
+    path = BEST_HYPERPARAMS_FILE if os.path.exists(BEST_HYPERPARAMS_FILE) else HYPERPARAMS_FILE
     if not os.path.exists(path):
-        logger.warning(f"Hyperparameter file {path} not found; using defaults")
+        logger.warning(f'Hyperparameter file {path} not found; using defaults')
         return {}
     try:
-        with open(path, encoding="utf-8") as f:
+        with open(path, encoding='utf-8') as f:
             return json.load(f)
     except (OSError, json.JSONDecodeError) as exc:
-        logger.warning("Failed to load hyperparameters from %s: %s", path, exc)
+        logger.warning('Failed to load hyperparameters from %s: %s', path, exc)
         return {}
-
 
 def _maybe_warm_cache(ctx: BotContext) -> None:
     """
     Warm up cache for the main universe symbols (daily + optional intraday).
     """
     settings = get_settings()
-    if not getattr(settings, "data_cache_enable", False):
+    if not getattr(settings, 'data_cache_enable', False):
         return
     try:
-        # Daily warm-up
         warmup_cache(ctx.symbols, lookback_days=settings.data_warmup_lookback_days)
-        # Optional intraday warm-up
-        if getattr(settings, "intraday_batch_enable", True):
+        if getattr(settings, 'intraday_batch_enable', True):
             end_dt = datetime.now(UTC)
-            start_dt = end_dt - timedelta(
-                minutes=int(settings.intraday_lookback_minutes)
-            )
-            _fetch_intraday_bars_chunked(
-                ctx,
-                ctx.symbols,
-                start=start_dt,
-                end=end_dt,
-                feed=getattr(ctx, "data_feed", None),
-            )
+            start_dt = end_dt - timedelta(minutes=int(settings.intraday_lookback_minutes))
+            _fetch_intraday_bars_chunked(ctx, ctx.symbols, start=start_dt, end=end_dt, feed=getattr(ctx, 'data_feed', None))
     except Exception as exc:
-        logger.warning("Cache warm-up failed: %s", exc)
-
-
-def _fetch_universe_bars(
-    ctx: BotContext,
-    symbols: list[str],
-    timeframe: str,
-    start: datetime | str,
-    end: datetime | str,
-    feed: str | None = None,
-) -> dict[str, pd.DataFrame]:
+        logger.warning('Cache warm-up failed: %s', exc)
+
+def _fetch_universe_bars(ctx: BotContext, symbols: list[str], timeframe: str, start: datetime | str, end: datetime | str, feed: str | None=None) -> dict[str, pd.DataFrame]:
     """
     Fetch universe bars for symbols with parallel fallback.
     """
     if not symbols:
         return {}
-
     try:
         batch = get_bars_batch(symbols, timeframe, start, end, feed=feed)
     except Exception as exc:
-        logger.warning("Universe batch failed: %s", exc)
+        logger.warning('Universe batch failed: %s', exc)
         batch = {}
-
-    remaining = [
-        s
-        for s in symbols
-        if s not in batch
-        or batch.get(s) is None
-        or getattr(batch.get(s), "empty", False)
-    ]
+    remaining = [s for s in symbols if s not in batch or batch.get(s) is None or getattr(batch.get(s), 'empty', False)]
     if remaining:
         settings = get_settings()
-        max_workers = max(1, int(getattr(settings, "batch_fallback_workers", 4)))
+        max_workers = max(1, int(getattr(settings, 'batch_fallback_workers', 4)))
 
         def _pull(sym: str):
             try:
-                return sym, get_bars(sym, timeframe, start, end)
+                return (sym, get_bars(sym, timeframe, start, end))
             except Exception as one_exc:
-                logger.warning("Per-symbol fetch failed for %s: %s", sym, one_exc)
-                return sym, None
-
-        with ThreadPoolExecutor(
-            max_workers=max_workers, thread_name_prefix="fallback-daily"
-        ) as ex:
+                logger.warning('Per-symbol fetch failed for %s: %s', sym, one_exc)
+                return (sym, None)
+        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix='fallback-daily') as ex:
             for fut in as_completed([ex.submit(_pull, s) for s in remaining]):
                 sym, df = fut.result()
-                if df is not None and not getattr(df, "empty", False):
+                if df is not None and (not getattr(df, 'empty', False)):
                     batch[sym] = df
-
-    return {
-        k: v
-        for k, v in batch.items()
-        if v is not None and not getattr(v, "empty", False)
-    }
-
-
-def _fetch_universe_bars_chunked(
-    ctx: BotContext,
-    symbols: list[str],
-    timeframe: str,
-    start: datetime | str,
-    end: datetime | str,
-    feed: str | None = None,
-) -> dict[str, pd.DataFrame]:
+    return {k: v for k, v in batch.items() if v is not None and (not getattr(v, 'empty', False))}
+
+def _fetch_universe_bars_chunked(ctx: BotContext, symbols: list[str], timeframe: str, start: datetime | str, end: datetime | str, feed: str | None=None) -> dict[str, pd.DataFrame]:
     """
     Chunked batched fetch for universe bars with safe fallback.
     """
     if not symbols:
         return {}
     settings = get_settings()
-    batch_size = max(1, int(getattr(settings, "pretrade_batch_size", 50)))
+    batch_size = max(1, int(getattr(settings, 'pretrade_batch_size', 50)))
     out: dict[str, pd.DataFrame] = {}
     for i in range(0, len(symbols), batch_size):
-        chunk = symbols[i : i + batch_size]
+        chunk = symbols[i:i + batch_size]
         out.update(_fetch_universe_bars(ctx, chunk, timeframe, start, end, feed))
     return out
 
-
-def _fetch_intraday_bars_chunked(
-    ctx: BotContext,
-    symbols: list[str],
-    start: datetime | str,
-    end: datetime | str,
-    feed: str | None = None,
-) -> dict[str, pd.DataFrame]:
+def _fetch_intraday_bars_chunked(ctx: BotContext, symbols: list[str], start: datetime | str, end: datetime | str, feed: str | None=None) -> dict[str, pd.DataFrame]:
     """
     Chunked batched fetch for 1-minute bars with safe fallback.
     """
     if not symbols:
         return {}
     settings = get_settings()
-    if not getattr(settings, "intraday_batch_enable", True):
+    if not getattr(settings, 'intraday_batch_enable', True):
         return {s: get_minute_bars(s, start, end, feed=feed) for s in symbols}
-    batch_size = max(1, int(getattr(settings, "intraday_batch_size", 40)))
+    batch_size = max(1, int(getattr(settings, 'intraday_batch_size', 40)))
     out: dict[str, pd.DataFrame] = {}
     for i in range(0, len(symbols), batch_size):
-        chunk = symbols[i : i + batch_size]
+        chunk = symbols[i:i + batch_size]
         try:
             got = get_minute_bars_batch(chunk, start, end, feed=feed)
         except Exception as exc:
-            logger.warning(
-                "Intraday batch failed for chunk size %d: %s; falling back",
-                len(chunk),
-                exc,
-            )
+            logger.warning('Intraday batch failed for chunk size %d: %s; falling back', len(chunk), exc)
             got = {}
-        # fill any missing with bounded concurrency
-        missing = [
-            s
-            for s in chunk
-            if s not in got or got.get(s) is None or getattr(got.get(s), "empty", False)
-        ]
+        missing = [s for s in chunk if s not in got or got.get(s) is None or getattr(got.get(s), 'empty', False)]
         if missing:
             settings = get_settings()
-            max_workers = max(1, int(getattr(settings, "batch_fallback_workers", 4)))
+            max_workers = max(1, int(getattr(settings, 'batch_fallback_workers', 4)))
 
             def _pull(sym: str):
                 try:
-                    return sym, get_minute_bars(sym, start, end, feed=feed)
+                    return (sym, get_minute_bars(sym, start, end, feed=feed))
                 except Exception as one_exc:
-                    logger.warning(
-                        "Intraday per-symbol fallback failed for %s: %s", sym, one_exc
-                    )
-                    return sym, None
-
-            with ThreadPoolExecutor(
-                max_workers=max_workers, thread_name_prefix="fallback-intraday"
-            ) as ex:
+                    logger.warning('Intraday per-symbol fallback failed for %s: %s', sym, one_exc)
+                    return (sym, None)
+            with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix='fallback-intraday') as ex:
                 for fut in as_completed([ex.submit(_pull, s) for s in missing]):
                     sym, df = fut.result()
-                    if df is not None and not getattr(df, "empty", False):
+                    if df is not None and (not getattr(df, 'empty', False)):
                         got[sym] = df
-        out.update(
-            {
-                k: v
-                for k, v in got.items()
-                if v is not None and not getattr(v, "empty", False)
-            }
-        )
+        out.update({k: v for k, v in got.items() if v is not None and (not getattr(v, 'empty', False))})
     return out
 
-
-def _fetch_regime_bars(
-    ctx: BotContext, start, end, timeframe="1D"
-) -> dict[str, pd.DataFrame]:
+def _fetch_regime_bars(ctx: BotContext, start, end, timeframe='1D') -> dict[str, pd.DataFrame]:
     settings = get_settings()
-    syms_csv = (getattr(settings, "regime_symbols_csv", None) or "SPY").strip()
-    symbols = [s.strip() for s in syms_csv.split(",") if s.strip()]
-    return _fetch_universe_bars_chunked(
-        ctx, symbols, timeframe, start, end, getattr(ctx, "data_feed", None)
-    )
-
+    syms_csv = (getattr(settings, 'regime_symbols_csv', None) or 'SPY').strip()
+    symbols = [s.strip() for s in syms_csv.split(',') if s.strip()]
+    return _fetch_universe_bars_chunked(ctx, symbols, timeframe, start, end, getattr(ctx, 'data_feed', None))
 
 def _build_regime_dataset(ctx: BotContext) -> pd.DataFrame:
     """
     Build regime dataset using a configurable basket via batched fetch.
     Returns a *wide* DataFrame: columns are symbols, rows are aligned by timestamp (index reset).
     """
-    logger.info("Building regime dataset (batched)")
+    logger.info('Building regime dataset (batched)')
     try:
         end_dt = datetime.now(UTC)
-        start_dt = end_dt - timedelta(
-            days=max(30, int(getattr(ctx, "regime_lookback_days", 100)))
-        )
-        bundle = _fetch_regime_bars(ctx, start=start_dt, end=end_dt, timeframe="1D")
+        start_dt = end_dt - timedelta(days=max(30, int(getattr(ctx, 'regime_lookback_days', 100))))
+        bundle = _fetch_regime_bars(ctx, start=start_dt, end=end_dt, timeframe='1D')
         if not bundle:
             return pd.DataFrame()
         cols = []
         for sym, df in bundle.items():
-            if df is None or getattr(df, "empty", False):
+            if df is None or getattr(df, 'empty', False):
                 continue
-            s = (
-                df[["timestamp", "close"]]
-                .rename(columns={"close": sym})
-                .set_index("timestamp")
-            )
+            s = df[['timestamp', 'close']].rename(columns={'close': sym}).set_index('timestamp')
             cols.append(s)
         if not cols:
-            logger.warning(
-                "Regime dataset empty after normalization; attempting SPY-only fallback"
-            )
+            logger.warning('Regime dataset empty after normalization; attempting SPY-only fallback')
             try:
-                spy_df = ctx.data_fetcher.fetch_bars("SPY", timeframe="1D", limit=180)
-                if spy_df is not None and not getattr(spy_df, "empty", False):
-                    s = (
-                        spy_df[["timestamp", "close"]]
-                        .rename(columns={"close": "SPY"})
-                        .set_index("timestamp")
-                    )
+                spy_df = ctx.data_fetcher.fetch_bars('SPY', timeframe='1D', limit=180)
+                if spy_df is not None and (not getattr(spy_df, 'empty', False)):
+                    s = spy_df[['timestamp', 'close']].rename(columns={'close': 'SPY'}).set_index('timestamp')
                     cols.append(s)
                 else:
-                    raise Exception("SPY data not available")
+                    raise Exception('SPY data not available')
             except Exception as e:
-                logger.error("SPY fallback failed: %s", e)
-                logger.error(
-                    "Not enough valid rows (0) to train regime model; using dummy fallback"
-                )
+                logger.error('SPY fallback failed: %s', e)
+                logger.error('Not enough valid rows (0) to train regime model; using dummy fallback')
                 return pd.DataFrame()
-
-        if not cols:  # Final check after SPY fallback attempt
+        if not cols:
             return pd.DataFrame()
         out = pd.concat(cols, axis=1).sort_index().reset_index()
         out.columns.name = None
         return out
     except Exception as exc:
-        logger.warning("REGIME bootstrap failed: %s", exc)
+        logger.warning('REGIME bootstrap failed: %s', exc)
         return pd.DataFrame()
-
 
 def _regime_basket_to_proxy_bars(wide: pd.DataFrame) -> pd.DataFrame:
     """
@@ -1611,41 +998,31 @@     DataFrame with at least ['timestamp','close'] for downstream model training.
     The proxy is an equal-weighted index of normalized closes.
     """
-    if wide is None or getattr(wide, "empty", False):
+    if wide is None or getattr(wide, 'empty', False):
         return pd.DataFrame()
-    if "timestamp" not in wide.columns:
+    if 'timestamp' not in wide.columns:
         return pd.DataFrame()
-    close_cols = [c for c in wide.columns if c != "timestamp"]
+    close_cols = [c for c in wide.columns if c != 'timestamp']
     if not close_cols:
         return pd.DataFrame()
     df = wide.copy()
-    # Normalize each series to 1.0 at first valid point to avoid scale bias
     base = df[close_cols].iloc[0]
     norm = df[close_cols] / base.replace(0, pd.NA)
     proxy_close = norm.mean(axis=1).astype(float)
-    out = pd.DataFrame({"timestamp": df["timestamp"], "close": proxy_close})
+    out = pd.DataFrame({'timestamp': df['timestamp'], 'close': proxy_close})
     return out
-
-
-# <-- NEW: marker file for daily retraining -->
-RETRAIN_MARKER_FILE = abspath("last_retrain.txt")
-
-# Main meta‐learner path: this is where retrain.py will dump the new sklearn model each day.
+RETRAIN_MARKER_FILE = abspath('last_retrain.txt')
 MODEL_PATH = abspath(S.model_path)
 MODEL_RF_PATH = abspath(S.model_rf_path)
 MODEL_XGB_PATH = abspath(S.model_xgb_path)
 MODEL_LGB_PATH = abspath(S.model_lgb_path)
-
-REGIME_MODEL_PATH = abspath("regime_model.pkl")
-# (We keep a separate meta‐model for signal‐weight learning, if you use Bayesian/Ridge, etc.)
-META_MODEL_PATH = abspath("meta_model.pkl")
-
-
-# Strategy mode
+REGIME_MODEL_PATH = abspath('regime_model.pkl')
+META_MODEL_PATH = abspath('meta_model.pkl')
+
 class BotMode:
-    def __init__(self, mode: str = "balanced") -> None:
+
+    def __init__(self, mode: str='balanced') -> None:
         self.mode = mode.lower()
-        # Use centralized configuration instead of hardcoded parameters
         self.config = config.TradingConfig.from_env(mode=self.mode)
         self.params = self.config.get_legacy_params()
 
@@ -1659,7 +1036,6 @@ 
     def get_config(self) -> dict[str, float]:
         return self.params
-
 
 @dataclass
 class BotState:
@@ -1706,167 +1082,93 @@         This class uses dataclass fields with default factories to ensure proper
         initialization of mutable default values across instances.
     """
-
-    # Risk Management State
     loss_streak: int = 0
     streak_halt_until: datetime | None = None
     day_start_equity: tuple[date, float] | None = None
     week_start_equity: tuple[date, float] | None = None
     last_drawdown: float = 0.0
-
-    # Operational State
     updates_halted: bool = False
     running: bool = False
-    current_regime: str = "sideways"
+    current_regime: str = 'sideways'
     rolling_losses: list[float] = field(default_factory=list)
     mode_obj: BotMode = field(default_factory=lambda: BotMode(BOT_MODE))
-
-    # Signal & Indicator State
     no_signal_events: int = 0
     indicator_failures: int = 0
     pdt_blocked: bool = False
-
-    # Position Management
     position_cache: dict[str, int] = field(default_factory=dict)
     long_positions: set[str] = field(default_factory=set)
     short_positions: set[str] = field(default_factory=set)
-
-    # Execution Timing
     last_run_at: datetime | None = None
     last_loop_duration: float = 0.0
-
-    # Trade Management
     trade_cooldowns: dict[str, datetime] = field(default_factory=dict)
     last_trade_direction: dict[str, str] = field(default_factory=dict)
     skipped_cycles: int = 0
-
-    # AI-AGENT-REF: Trade frequency tracking for overtrading prevention
-    trade_history: list[tuple[str, datetime]] = field(
-        default_factory=list
-    )  # (symbol, timestamp)
-
-
+    trade_history: list[tuple[str, datetime]] = field(default_factory=list)
 state = BotState()
 logger.info(f"Trading mode is set to '{state.mode_obj.mode}'")
 params = state.mode_obj.get_config()
 params.update(load_hyperparams())
-
-# Other constants
-NEWS_API_KEY = CONFIG_NEWS_API_KEY  # Keep for backwards compatibility
-SENTIMENT_API_KEY = CONFIG_SENTIMENT_API_KEY  # New preferred API key
-SENTIMENT_API_URL = CONFIG_SENTIMENT_API_URL  # Configurable API URL
-TRAILING_FACTOR = params.get("TRAILING_FACTOR", state.mode_obj.config.trailing_factor)
+NEWS_API_KEY = CONFIG_NEWS_API_KEY
+SENTIMENT_API_KEY = CONFIG_SENTIMENT_API_KEY
+SENTIMENT_API_URL = CONFIG_SENTIMENT_API_URL
+TRAILING_FACTOR = params.get('TRAILING_FACTOR', state.mode_obj.config.trailing_factor)
 SECONDARY_TRAIL_FACTOR = 1.0
-TAKE_PROFIT_FACTOR = params.get(
-    "TAKE_PROFIT_FACTOR", state.mode_obj.config.take_profit_factor
-)
-SCALING_FACTOR = params.get("SCALING_FACTOR", state.mode_obj.config.scaling_factor)
-ORDER_TYPE = "market"
-LIMIT_ORDER_SLIPPAGE = params.get(
-    "LIMIT_ORDER_SLIPPAGE", state.mode_obj.config.limit_order_slippage
-)
+TAKE_PROFIT_FACTOR = params.get('TAKE_PROFIT_FACTOR', state.mode_obj.config.take_profit_factor)
+SCALING_FACTOR = params.get('SCALING_FACTOR', state.mode_obj.config.scaling_factor)
+ORDER_TYPE = 'market'
+LIMIT_ORDER_SLIPPAGE = params.get('LIMIT_ORDER_SLIPPAGE', state.mode_obj.config.limit_order_slippage)
 MAX_POSITION_SIZE = state.mode_obj.config.max_position_size
 SLICE_THRESHOLD = 50
-POV_SLICE_PCT = params.get("POV_SLICE_PCT", state.mode_obj.config.pov_slice_pct)
-DAILY_LOSS_LIMIT = params.get(
-    "DAILY_LOSS_LIMIT", state.mode_obj.config.daily_loss_limit
-)
-# AI-AGENT-REF: Increase default position limit from 10 to 20 for better portfolio utilization
+POV_SLICE_PCT = params.get('POV_SLICE_PCT', state.mode_obj.config.pov_slice_pct)
+DAILY_LOSS_LIMIT = params.get('DAILY_LOSS_LIMIT', state.mode_obj.config.daily_loss_limit)
 MAX_PORTFOLIO_POSITIONS = S.max_portfolio_positions
-CORRELATION_THRESHOLD = 0.60
+CORRELATION_THRESHOLD = 0.6
 SECTOR_EXPOSURE_CAP = S.sector_exposure_cap
 MAX_OPEN_POSITIONS = S.max_open_positions
 WEEKLY_DRAWDOWN_LIMIT = S.weekly_drawdown_limit
 MARKET_OPEN = dt_time(6, 30)
 MARKET_CLOSE = dt_time(13, 0)
 VOLUME_THRESHOLD = S.volume_threshold
-ENTRY_START_OFFSET = timedelta(
-    minutes=params.get(
-        "ENTRY_START_OFFSET_MIN", state.mode_obj.config.entry_start_offset_min
-    )
-)
-ENTRY_END_OFFSET = timedelta(
-    minutes=params.get(
-        "ENTRY_END_OFFSET_MIN", state.mode_obj.config.entry_end_offset_min
-    )
-)
+ENTRY_START_OFFSET = timedelta(minutes=params.get('ENTRY_START_OFFSET_MIN', state.mode_obj.config.entry_start_offset_min))
+ENTRY_END_OFFSET = timedelta(minutes=params.get('ENTRY_END_OFFSET_MIN', state.mode_obj.config.entry_end_offset_min))
 REGIME_LOOKBACK = 14
 REGIME_ATR_THRESHOLD = 20.0
 RF_ESTIMATORS = 300
-
-# AI-AGENT-REF: Initialize trading parameters from centralized configuration
 RF_MAX_DEPTH = 3
 RF_MIN_SAMPLES_LEAF = 5
 ATR_LENGTH = 10
-CONF_THRESHOLD = params.get("CONF_THRESHOLD", state.mode_obj.config.conf_threshold)
-CONFIRMATION_COUNT = params.get(
-    "CONFIRMATION_COUNT", state.mode_obj.config.confirmation_count
-)
-CAPITAL_CAP = params.get("CAPITAL_CAP", state.mode_obj.config.capital_cap)
+CONF_THRESHOLD = params.get('CONF_THRESHOLD', state.mode_obj.config.conf_threshold)
+CONFIRMATION_COUNT = params.get('CONFIRMATION_COUNT', state.mode_obj.config.confirmation_count)
+CAPITAL_CAP = params.get('CAPITAL_CAP', state.mode_obj.config.capital_cap)
 DOLLAR_RISK_LIMIT = S.dollar_risk_limit
-BUY_THRESHOLD = params.get("BUY_THRESHOLD", state.mode_obj.config.buy_threshold)
-
-
-# AI-AGENT-REF: Add comprehensive validation for critical trading parameters
+BUY_THRESHOLD = params.get('BUY_THRESHOLD', state.mode_obj.config.buy_threshold)
+
 def validate_trading_parameters():
     """Validate critical trading parameters and log warnings for invalid values."""
     global CAPITAL_CAP, DOLLAR_RISK_LIMIT, MAX_POSITION_SIZE, CONF_THRESHOLD, BUY_THRESHOLD
-
-    # Validate CAPITAL_CAP (should be between 0.01 and 0.5)
-    if not isinstance(CAPITAL_CAP, int | float) or not (0.01 <= CAPITAL_CAP <= 0.5):
-        logger.error("Invalid CAPITAL_CAP %s, using default 0.25", CAPITAL_CAP)
+    if not isinstance(CAPITAL_CAP, int | float) or not 0.01 <= CAPITAL_CAP <= 0.5:
+        logger.error('Invalid CAPITAL_CAP %s, using default 0.25', CAPITAL_CAP)
         CAPITAL_CAP = 0.25
-
-    # Validate DOLLAR_RISK_LIMIT (should be between 0.005 and 0.1)
-    if not isinstance(DOLLAR_RISK_LIMIT, int | float) or not (
-        0.005 <= DOLLAR_RISK_LIMIT <= 0.1
-    ):
-        logger.error(
-            "Invalid DOLLAR_RISK_LIMIT %s, using default 0.05", DOLLAR_RISK_LIMIT
-        )
+    if not isinstance(DOLLAR_RISK_LIMIT, int | float) or not 0.005 <= DOLLAR_RISK_LIMIT <= 0.1:
+        logger.error('Invalid DOLLAR_RISK_LIMIT %s, using default 0.05', DOLLAR_RISK_LIMIT)
         DOLLAR_RISK_LIMIT = 0.05
-
-    # Validate MAX_POSITION_SIZE (should be between 1 and 10000)
-    if not isinstance(MAX_POSITION_SIZE, int) or not (1 <= MAX_POSITION_SIZE <= 10000):
-        logger.error(
-            "Invalid MAX_POSITION_SIZE %s, using default 8000", MAX_POSITION_SIZE
-        )
+    if not isinstance(MAX_POSITION_SIZE, int) or not 1 <= MAX_POSITION_SIZE <= 10000:
+        logger.error('Invalid MAX_POSITION_SIZE %s, using default 8000', MAX_POSITION_SIZE)
         MAX_POSITION_SIZE = 8000
-
-    # Validate CONF_THRESHOLD (should be between 0.5 and 0.95)
-    if not isinstance(CONF_THRESHOLD, int | float) or not (
-        0.5 <= CONF_THRESHOLD <= 0.95
-    ):
-        logger.error("Invalid CONF_THRESHOLD %s, using default 0.75", CONF_THRESHOLD)
+    if not isinstance(CONF_THRESHOLD, int | float) or not 0.5 <= CONF_THRESHOLD <= 0.95:
+        logger.error('Invalid CONF_THRESHOLD %s, using default 0.75', CONF_THRESHOLD)
         CONF_THRESHOLD = 0.75
-
-    # Validate BUY_THRESHOLD (should be between 0.1 and 0.9)
-    if not isinstance(BUY_THRESHOLD, int | float) or not (0.1 <= BUY_THRESHOLD <= 0.9):
-        logger.error("Invalid BUY_THRESHOLD %s, using default 0.2", BUY_THRESHOLD)
+    if not isinstance(BUY_THRESHOLD, int | float) or not 0.1 <= BUY_THRESHOLD <= 0.9:
+        logger.error('Invalid BUY_THRESHOLD %s, using default 0.2', BUY_THRESHOLD)
         BUY_THRESHOLD = 0.2
-
-    logger.info(
-        "Trading parameters validated: CAPITAL_CAP=%.3f, DOLLAR_RISK_LIMIT=%.3f, MAX_POSITION_SIZE=%d",
-        CAPITAL_CAP,
-        DOLLAR_RISK_LIMIT,
-        MAX_POSITION_SIZE,
-    )
-
-
-# AI-AGENT-REF: Defer parameter validation in testing environments to prevent import blocking
-# Validate parameters after loading
-if not os.getenv("TESTING"):
+    logger.info('Trading parameters validated: CAPITAL_CAP=%.3f, DOLLAR_RISK_LIMIT=%.3f, MAX_POSITION_SIZE=%d', CAPITAL_CAP, DOLLAR_RISK_LIMIT, MAX_POSITION_SIZE)
+if not os.getenv('TESTING'):
     validate_trading_parameters()
-
-PACIFIC = ZoneInfo("America/Los_Angeles")
-PDT_DAY_TRADE_LIMIT = params.get("PDT_DAY_TRADE_LIMIT", 3)
-PDT_EQUITY_THRESHOLD = params.get("PDT_EQUITY_THRESHOLD", 25_000.0)
+PACIFIC = ZoneInfo('America/Los_Angeles')
+PDT_DAY_TRADE_LIMIT = params.get('PDT_DAY_TRADE_LIMIT', 3)
+PDT_EQUITY_THRESHOLD = params.get('PDT_EQUITY_THRESHOLD', 25000.0)
 FINNHUB_RPM = S.finnhub_rpm
-
-# Regime symbols (makes SPY configurable)
-REGIME_SYMBOLS = ["SPY"]
-
-# ─── THREAD-SAFETY LOCKS & CIRCUIT BREAKER ─────────────────────────────────────
+REGIME_SYMBOLS = ['SPY']
 cache_lock = Lock()
 targets_lock = Lock()
 vol_lock = Lock()
@@ -1874,95 +1176,49 @@ slippage_lock = Lock()
 meta_lock = Lock()
 run_lock = Lock()
-# AI-AGENT-REF: Add thread-safe locking for trade cooldown state
 trade_cooldowns_lock = Lock()
-
-# AI-AGENT-REF: Enhanced circuit breaker configuration for external services
 breaker = pybreaker.CircuitBreaker(fail_max=5, reset_timeout=60)
-
-# AI-AGENT-REF: Specific circuit breakers for different external services
-alpaca_breaker = pybreaker.CircuitBreaker(
-    fail_max=3,  # Alpaca should be more reliable, fail after 3 attempts
-    reset_timeout=30,  # Shorter reset timeout for trading API
-    name="alpaca_api",
-)
-
-data_breaker = pybreaker.CircuitBreaker(
-    fail_max=5,  # Data services can be less reliable
-    reset_timeout=120,  # Longer timeout for data recovery
-    name="data_services",
-)
-
-finnhub_breaker = pybreaker.CircuitBreaker(
-    fail_max=3,  # External data API
-    reset_timeout=300,  # 5 minutes for external services
-    name="finnhub_api",
-)
-# Bounded, CPU-aware executors sized via Settings
+alpaca_breaker = pybreaker.CircuitBreaker(fail_max=3, reset_timeout=30, name='alpaca_api')
+data_breaker = pybreaker.CircuitBreaker(fail_max=5, reset_timeout=120, name='data_services')
+finnhub_breaker = pybreaker.CircuitBreaker(fail_max=3, reset_timeout=300, name='finnhub_api')
 _cpu = os.cpu_count() or 2
 _S = get_settings()
 executor = ThreadPoolExecutor(max_workers=_S.effective_executor_workers(_cpu))
-prediction_executor = ThreadPoolExecutor(
-    max_workers=_S.effective_prediction_workers(_cpu)
-)
-
-
-# AI-AGENT-REF: Add proper cleanup with atexit handlers for ThreadPoolExecutor resource leak
+prediction_executor = ThreadPoolExecutor(max_workers=_S.effective_prediction_workers(_cpu))
+
 def cleanup_executors():
     """Cleanup ThreadPoolExecutor resources to prevent resource leaks."""
     try:
         if executor:
             executor.shutdown(wait=True, cancel_futures=True)
-            logger.debug("Main executor shutdown successfully")
+            logger.debug('Main executor shutdown successfully')
     except Exception as e:
-        logger.warning("Error shutting down main executor: %s", e)
-
+        logger.warning('Error shutting down main executor: %s', e)
     try:
         if prediction_executor:
             prediction_executor.shutdown(wait=True, cancel_futures=True)
-            logger.debug("Prediction executor shutdown successfully")
+            logger.debug('Prediction executor shutdown successfully')
     except Exception as e:
-        logger.warning("Error shutting down prediction executor: %s", e)
-
-
+        logger.warning('Error shutting down prediction executor: %s', e)
 atexit.register(cleanup_executors)
-
-# EVENT cooldown
 _LAST_EVENT_TS = {}
-EVENT_COOLDOWN = 15.0  # seconds
-# AI-AGENT-REF: hold time now configurable; default to 0 for pure signal holding
-REBALANCE_HOLD_SECONDS = int(os.getenv("REBALANCE_HOLD_SECONDS", "0"))
-RUN_INTERVAL_SECONDS = 60  # don't run trading loop more often than this
-TRADE_COOLDOWN_MIN = S.trade_cooldown_min  # minutes
-
-# AI-AGENT-REF: Enhanced overtrading prevention with frequency limits
-MAX_TRADES_PER_HOUR = S.max_trades_per_hour  # limit high-frequency trading
-MAX_TRADES_PER_DAY = S.max_trades_per_day  # daily limit to prevent excessive trading
-TRADE_FREQUENCY_WINDOW_HOURS = 1  # rolling window for hourly limits
-
-# Loss streak kill-switch (managed via BotState)
-
-# Volatility stats (for SPY ATR mean/std)
-_VOL_STATS = {"mean": None, "std": None, "last_update": None, "last": None}
-
-# Slippage logs (in-memory for quick access)
-_slippage_log: list[tuple[str, float, float, datetime]] = (
-    []
-)  # (symbol, expected, actual, timestamp)
-# Ensure persistent slippage log file exists
+EVENT_COOLDOWN = 15.0
+REBALANCE_HOLD_SECONDS = int(os.getenv('REBALANCE_HOLD_SECONDS', '0'))
+RUN_INTERVAL_SECONDS = 60
+TRADE_COOLDOWN_MIN = S.trade_cooldown_min
+MAX_TRADES_PER_HOUR = S.max_trades_per_hour
+MAX_TRADES_PER_DAY = S.max_trades_per_day
+TRADE_FREQUENCY_WINDOW_HOURS = 1
+_VOL_STATS = {'mean': None, 'std': None, 'last_update': None, 'last': None}
+_slippage_log: list[tuple[str, float, float, datetime]] = []
 if not os.path.exists(SLIPPAGE_LOG_FILE):
     try:
-        os.makedirs(os.path.dirname(SLIPPAGE_LOG_FILE) or ".", exist_ok=True)
-        with open(SLIPPAGE_LOG_FILE, "w", newline="") as f:
-            csv.writer(f).writerow(
-                ["timestamp", "symbol", "expected", "actual", "slippage_cents"]
-            )
+        os.makedirs(os.path.dirname(SLIPPAGE_LOG_FILE) or '.', exist_ok=True)
+        with open(SLIPPAGE_LOG_FILE, 'w', newline='') as f:
+            csv.writer(f).writerow(['timestamp', 'symbol', 'expected', 'actual', 'slippage_cents'])
     except Exception as e:
-        logger.warning(f"Could not create slippage log {SLIPPAGE_LOG_FILE}: {e}")
-
-# Sector cache for portfolio exposure calculations
+        logger.warning(f'Could not create slippage log {SLIPPAGE_LOG_FILE}: {e}')
 _SECTOR_CACHE: dict[str, str] = {}
-
 
 def _log_health_diagnostics(ctx: BotContext, reason: str) -> None:
     """Log detailed diagnostics used for halt decisions."""
@@ -1973,188 +1229,116 @@         cash = -1.0
         positions = -1
     try:
-        df = ctx.data_fetcher.get_minute_df(
-            ctx, REGIME_SYMBOLS[0], lookback_minutes=config.MIN_HEALTH_ROWS
-        )
+        df = ctx.data_fetcher.get_minute_df(ctx, REGIME_SYMBOLS[0], lookback_minutes=S.min_health_rows)
         rows = len(df)
-        last_time = df.index[-1].isoformat() if not df.empty else "n/a"
+        last_time = df.index[-1].isoformat() if not df.empty else 'n/a'
     except Exception:
         rows = 0
-        last_time = "n/a"
-    vol = _VOL_STATS.get("last")
-    sentiment = getattr(ctx, "last_sentiment", 0.0)
-    logger.debug(
-        "Health diagnostics: rows=%s, last_time=%s, vol=%s, sent=%s, cash=%s, positions=%s, reason=%s",
-        rows,
-        last_time,
-        vol,
-        sentiment,
-        cash,
-        positions,
-        reason,
-    )
-
-
-# ─── TYPED EXCEPTION ─────────────────────────────────────────────────────────
+        last_time = 'n/a'
+    vol = _VOL_STATS.get('last')
+    sentiment = getattr(ctx, 'last_sentiment', 0.0)
+    logger.debug('Health diagnostics: rows=%s, last_time=%s, vol=%s, sent=%s, cash=%s, positions=%s, reason=%s', rows, last_time, vol, sentiment, cash, positions, reason)
+
 class DataFetchErrorLegacy(Exception):
     pass
 
-
 class OrderExecutionError(Exception):
     """Raised when an Alpaca order fails after submission."""
-
-
-# ─── B. CLIENTS & SINGLETONS ─────────────────────────────────────────────────
-
 
 def ensure_alpaca_credentials() -> None:
     """Verify Alpaca credentials are present before starting."""
     validate_alpaca_credentials()
 
-
 def log_circuit_breaker_status():
     """Log the status of all circuit breakers for monitoring."""
     try:
-        breakers = {
-            "main": breaker,
-            "alpaca": alpaca_breaker,
-            "data": data_breaker,
-            "finnhub": finnhub_breaker,
-        }
-
+        breakers = {'main': breaker, 'alpaca': alpaca_breaker, 'data': data_breaker, 'finnhub': finnhub_breaker}
         for name, cb in breakers.items():
-            if hasattr(cb, "state") and hasattr(cb, "fail_counter"):
-                logger.info(
-                    "CIRCUIT_BREAKER_STATUS",
-                    extra={
-                        "breaker": name,
-                        "state": cb.state,
-                        "failures": cb.fail_counter,
-                        "last_failure": getattr(cb, "last_failure", None),
-                    },
-                )
+            if hasattr(cb, 'state') and hasattr(cb, 'fail_counter'):
+                logger.info('CIRCUIT_BREAKER_STATUS', extra={'breaker': name, 'state': cb.state, 'failures': cb.fail_counter, 'last_failure': getattr(cb, 'last_failure', None)})
     except Exception as e:
-        logger.debug(f"Circuit breaker status logging failed: {e}")
-
+        logger.debug(f'Circuit breaker status logging failed: {e}')
 
 def get_circuit_breaker_health() -> dict:
     """Get health status of all circuit breakers."""
     try:
-        breakers = {
-            "main": breaker,
-            "alpaca": alpaca_breaker,
-            "data": data_breaker,
-            "finnhub": finnhub_breaker,
-        }
-
+        breakers = {'main': breaker, 'alpaca': alpaca_breaker, 'data': data_breaker, 'finnhub': finnhub_breaker}
         health = {}
         for name, cb in breakers.items():
-            if hasattr(cb, "state"):
-                health[name] = {
-                    "state": str(cb.state),
-                    "healthy": cb.state != "open",
-                    "failures": getattr(cb, "fail_counter", 0),
-                }
+            if hasattr(cb, 'state'):
+                health[name] = {'state': str(cb.state), 'healthy': cb.state != 'open', 'failures': getattr(cb, 'fail_counter', 0)}
             else:
-                health[name] = {"state": "unknown", "healthy": True, "failures": 0}
-
+                health[name] = {'state': 'unknown', 'healthy': True, 'failures': 0}
         return health
     except Exception as e:
-        logger.error(f"Failed to get circuit breaker health: {e}")
+        logger.error(f'Failed to get circuit breaker health: {e}')
         return {}
 
-
-# IMPORTANT: Alpaca credentials will be validated at runtime when needed.
-# Do not validate at import time to prevent crashes during module loading.
-
-
-# Prometheus-safe account fetch with circuit breaker protection
 @alpaca_breaker
 def safe_alpaca_get_account(ctx: BotContext):
     """Safely get account information."""
     if ctx.api is None:
-        logger.warning("ctx.api is None - Alpaca trading client unavailable")
+        logger.warning('ctx.api is None - Alpaca trading client unavailable')
         return None
     return ctx.api.get_account()
 
-
-# ─── C. HELPERS ────────────────────────────────────────────────────────────────
 def chunked(iterable: Sequence, n: int):
     """Yield successive n-sized chunks from iterable."""
     for i in range(0, len(iterable), n):
-        yield iterable[i : i + n]
-
+        yield iterable[i:i + n]
 
 def ttl_seconds() -> int:
     """Configurable TTL for minute-bar cache (default 60s)."""
     return S.minute_cache_ttl
 
-
 def asset_class_for(symbol: str) -> str:
     """Very small heuristic to map tickers to asset classes."""
     sym = symbol.upper()
-    if sym.endswith("USD") and len(sym) == 6:
-        return "forex"
-    if sym.startswith(("BTC", "ETH")):
-        return "crypto"
-    return "equity"
-
+    if sym.endswith('USD') and len(sym) == 6:
+        return 'forex'
+    if sym.startswith(('BTC', 'ETH')):
+        return 'crypto'
+    return 'equity'
 
 def compute_spy_vol_stats(ctx: BotContext) -> None:
     """Compute daily ATR mean/std on SPY for the past 1 year."""
     today = date.today()
     with vol_lock:
-        if _VOL_STATS["last_update"] == today:
+        if _VOL_STATS['last_update'] == today:
             return
-
     df = ctx.data_fetcher.get_daily_df(ctx, REGIME_SYMBOLS[0])
     if df is None or len(df) < 252 + ATR_LENGTH:
         return True
-
-    # Compute ATR series for last 252 trading days
-    atr_series = ta.atr(df["high"], df["low"], df["close"], length=ATR_LENGTH).dropna()
+    atr_series = ta.atr(df['high'], df['low'], df['close'], length=ATR_LENGTH).dropna()
     if len(atr_series) < 252:
         return True
-
     recent = atr_series.iloc[-252:]
     mean_val = float(recent.mean())
     std_val = float(recent.std())
     last_val = float(atr_series.iloc[-1]) if not atr_series.empty else 0.0
-
     with vol_lock:
-        _VOL_STATS["mean"] = mean_val
-        _VOL_STATS["std"] = std_val
-        _VOL_STATS["last_update"] = today
-        _VOL_STATS["last"] = last_val
-
-    logger.info(
-        "SPY_VOL_STATS_UPDATED",
-        extra={"mean": mean_val, "std": std_val, "atr": last_val},
-    )
-
+        _VOL_STATS['mean'] = mean_val
+        _VOL_STATS['std'] = std_val
+        _VOL_STATS['last_update'] = today
+        _VOL_STATS['last'] = last_val
+    logger.info('SPY_VOL_STATS_UPDATED', extra={'mean': mean_val, 'std': std_val, 'atr': last_val})
 
 def is_high_vol_thr_spy() -> bool:
     """Return True if SPY ATR > mean + 2*std."""
     with vol_lock:
-        mean = _VOL_STATS["mean"]
-        std = _VOL_STATS["std"]
+        mean = _VOL_STATS['mean']
+        std = _VOL_STATS['std']
     if mean is None or std is None:
         return False
-
     with cache_lock:
         spy_df = data_fetcher._daily_cache.get(REGIME_SYMBOLS[0])
     if spy_df is None or len(spy_df) < ATR_LENGTH:
         return False
-
-    atr_series = ta.atr(
-        spy_df["high"], spy_df["low"], spy_df["close"], length=ATR_LENGTH
-    )
+    atr_series = ta.atr(spy_df['high'], spy_df['low'], spy_df['close'], length=ATR_LENGTH)
     if atr_series.empty:
         return False
-
     current_atr = float(atr_series.iloc[-1])
     return (current_atr - mean) / std >= 2
-
 
 def is_high_vol_regime() -> bool:
     """
@@ -2163,10 +1347,9 @@     """
     return is_high_vol_thr_spy()
 
-
-# ─── D. DATA FETCHERS ─────────────────────────────────────────────────────────
 class FinnhubFetcherLegacy:
-    def __init__(self, calls_per_minute: int = FINNHUB_RPM):
+
+    def __init__(self, calls_per_minute: int=FINNHUB_RPM):
         self.max_calls = calls_per_minute
         self._timestamps = deque()
         self.client = finnhub_client
@@ -2174,96 +1357,71 @@     def _throttle(self):
         while True:
             now_ts = pytime.time()
-            # drop timestamps older than 60 seconds
             while self._timestamps and now_ts - self._timestamps[0] > 60:
                 self._timestamps.popleft()
             if len(self._timestamps) < self.max_calls:
                 self._timestamps.append(now_ts)
                 return
             wait_secs = 60 - (now_ts - self._timestamps[0]) + random.uniform(0.1, 0.5)
-            logger.debug(f"[FH] rate-limit reached; sleeping {wait_secs:.2f}s")
+            logger.debug(f'[FH] rate-limit reached; sleeping {wait_secs:.2f}s')
             pytime.sleep(wait_secs)
 
     def _parse_period(self, period: str) -> int:
-        if period.endswith("mo"):
+        if period.endswith('mo'):
             num = int(period[:-2])
             return num * 30 * 86400
         num = int(period[:-1])
         unit = period[-1]
-        if unit == "d":
+        if unit == 'd':
             return num * 86400
-        raise ValueError(f"Unsupported period: {period}")
-
-    @retry(
-        stop=stop_after_attempt(3),
-        wait=wait_exponential(multiplier=1, min=1, max=10) + wait_random(0.1, 1),
-        retry=retry_if_exception_type(Exception),
-    )
-    def fetch(self, symbols, period="1mo", interval="1d") -> pd.DataFrame:
+        raise ValueError(f'Unsupported period: {period}')
+
+    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10) + wait_random(0.1, 1), retry=retry_if_exception_type(Exception))
+    def fetch(self, symbols, period='1mo', interval='1d') -> pd.DataFrame:
         syms = symbols if isinstance(symbols, list | tuple) else [symbols]
         now_ts = int(pytime.time())
         span = self._parse_period(period)
         start_ts = now_ts - span
-
-        resolution = "D" if interval == "1d" else "1"
+        resolution = 'D' if interval == '1d' else '1'
         frames = []
         for sym in syms:
             self._throttle()
             resp = self.client.stock_candles(sym, resolution, _from=start_ts, to=now_ts)
-            if resp.get("s") != "ok":
+            if resp.get('s') != 'ok':
                 logger.warning(f"[FH] no data for {sym}: status={resp.get('s')}")
                 frames.append(pd.DataFrame())
                 continue
-            idx = safe_to_datetime(resp["t"], context=f"Finnhub {sym}")
-            df = pd.DataFrame(
-                {
-                    "open": resp["o"],
-                    "high": resp["h"],
-                    "low": resp["l"],
-                    "close": resp["c"],
-                    "volume": resp["v"],
-                },
-                index=idx,
-            )
+            idx = safe_to_datetime(resp['t'], context=f'Finnhub {sym}')
+            df = pd.DataFrame({'open': resp['o'], 'high': resp['h'], 'low': resp['l'], 'close': resp['c'], 'volume': resp['v']}, index=idx)
             frames.append(df)
-
         if not frames:
             return pd.DataFrame()
         if len(frames) == 1:
             return frames[0]
-        return pd.concat(frames, axis=1, keys=syms, names=["Symbol", "Field"])
-
-
+        return pd.concat(frames, axis=1, keys=syms, names=['Symbol', 'Field'])
 _last_fh_prefetch_date: date | None = None
 
-
-def safe_get_stock_bars(client, request, symbol: str, context: str = ""):
+def safe_get_stock_bars(client, request, symbol: str, context: str=''):
     """Safely get stock bars with proper null checking and error handling."""
     try:
         response = client.get_stock_bars(request)
         if response is None:
-            logger.error(
-                f"ALPACA {context} FETCH ERROR for {symbol}: get_stock_bars returned None"
-            )
+            logger.error(f'ALPACA {context} FETCH ERROR for {symbol}: get_stock_bars returned None')
             return None
-        if not hasattr(response, "df"):
-            logger.error(
-                f"ALPACA {context} FETCH ERROR for {symbol}: response missing 'df' attribute"
-            )
+        if not hasattr(response, 'df'):
+            logger.error(f"ALPACA {context} FETCH ERROR for {symbol}: response missing 'df' attribute")
             return None
         return response.df
     except AttributeError as e:
-        logger.error(f"ALPACA {context} FETCH ERROR for {symbol}: AttributeError: {e}")
+        logger.error(f'ALPACA {context} FETCH ERROR for {symbol}: AttributeError: {e}')
         return None
     except Exception as e:
-        logger.error(
-            f"ALPACA {context} FETCH ERROR for {symbol}: {type(e).__name__}: {e}"
-        )
+        logger.error(f'ALPACA {context} FETCH ERROR for {symbol}: {type(e).__name__}: {e}')
         return None
-
 
 @dataclass
 class DataFetcher:
+
     def __post_init__(self):
         self._daily_cache: dict[str, pd.DataFrame | None] = {}
         self._minute_cache: dict[str, pd.DataFrame | None] = {}
@@ -2273,142 +1431,99 @@         symbol = symbol.upper()
         now_utc = datetime.now(UTC)
         end_ts = now_utc.replace(hour=0, minute=0, second=0, microsecond=0)
-        # fetch ~6 months of daily bars for health checks and indicators
         start_ts = end_ts - timedelta(days=150)
-
         with cache_lock:
             if symbol in self._daily_cache:
                 if daily_cache_hit:
                     try:
                         daily_cache_hit.inc()
                     except Exception as exc:
-                        logger.exception("bot.py unexpected", exc_info=exc)
+                        logger.exception('bot.py unexpected', exc_info=exc)
                         raise
                 return self._daily_cache[symbol]
-
         api_key = get_settings().alpaca_api_key
         api_secret = get_settings().alpaca_secret_key
         if not api_key or not api_secret:
-            logger.error(f"Missing Alpaca credentials for {symbol}")
+            logger.error(f'Missing Alpaca credentials for {symbol}')
             return None
-
         client = StockHistoricalDataClient(api_key, api_secret)
-
         try:
-            req = StockBarsRequest(
-                symbol_or_symbols=[symbol],
-                timeframe=TimeFrame.Day,
-                start=start_ts,
-                end=end_ts,
-                feed=_DEFAULT_FEED,
-            )
-            bars = safe_get_stock_bars(client, req, symbol, "DAILY")
+            req = StockBarsRequest(symbol_or_symbols=[symbol], timeframe=TimeFrame.Day, start=start_ts, end=end_ts, feed=_DEFAULT_FEED)
+            bars = safe_get_stock_bars(client, req, symbol, 'DAILY')
             if bars is None:
                 return None
             if isinstance(bars.columns, _RealMultiIndex):
                 bars = bars.xs(symbol, level=0, axis=1)
             else:
-                bars = bars.drop(columns=["symbol"], errors="ignore")
+                bars = bars.drop(columns=['symbol'], errors='ignore')
             if bars.empty:
-                logger.warning(
-                    f"No daily bars returned for {symbol}. Possible market holiday or API outage"
-                )
+                logger.warning(f'No daily bars returned for {symbol}. Possible market holiday or API outage')
                 return None
             if len(bars.index) and isinstance(bars.index[0], tuple):
                 idx_vals = [t[1] for t in bars.index]
             else:
                 idx_vals = bars.index
             try:
-                idx = safe_to_datetime(idx_vals, context=f"daily {symbol}")
+                idx = safe_to_datetime(idx_vals, context=f'daily {symbol}')
             except ValueError as e:
-                reason = "empty data" if bars.empty else "unparseable timestamps"
-                logger.warning(
-                    f"Invalid daily index for {symbol}; skipping. {reason} | {e}"
-                )
+                reason = 'empty data' if bars.empty else 'unparseable timestamps'
+                logger.warning(f'Invalid daily index for {symbol}; skipping. {reason} | {e}')
                 return None
             bars.index = idx
-            df = bars.rename(columns=lambda c: c.lower()).drop(
-                columns=["symbol"], errors="ignore"
-            )
+            df = bars.rename(columns=lambda c: c.lower()).drop(columns=['symbol'], errors='ignore')
         except APIError as e:
             err_msg = str(e).lower()
-            if "subscription does not permit querying recent sip data" in err_msg:
-                logger.warning(f"ALPACA SUBSCRIPTION ERROR for {symbol}: {repr(e)}")
-                logger.info(f"ATTEMPTING IEX-DELAYERED DATA FOR {symbol}")
+            if 'subscription does not permit querying recent sip data' in err_msg:
+                logger.warning(f'ALPACA SUBSCRIPTION ERROR for {symbol}: {repr(e)}')
+                logger.info(f'ATTEMPTING IEX-DELAYERED DATA FOR {symbol}')
                 try:
-                    req.feed = "iex"
-                    df_iex = safe_get_stock_bars(client, req, symbol, "IEX DAILY")
+                    req.feed = 'iex'
+                    df_iex = safe_get_stock_bars(client, req, symbol, 'IEX DAILY')
                     if df_iex is None:
                         return None
                     if isinstance(df_iex.columns, _RealMultiIndex):
                         df_iex = df_iex.xs(symbol, level=0, axis=1)
                     else:
-                        df_iex = df_iex.drop(columns=["symbol"], errors="ignore")
+                        df_iex = df_iex.drop(columns=['symbol'], errors='ignore')
                     if len(df_iex.index) and isinstance(df_iex.index[0], tuple):
                         idx_vals = [t[1] for t in df_iex.index]
                     else:
                         idx_vals = df_iex.index
                     try:
-                        idx = safe_to_datetime(idx_vals, context=f"IEX daily {symbol}")
+                        idx = safe_to_datetime(idx_vals, context=f'IEX daily {symbol}')
                     except ValueError as e:
-                        reason = (
-                            "empty data" if df_iex.empty else "unparseable timestamps"
-                        )
-                        logger.warning(
-                            f"Invalid IEX daily index for {symbol}; skipping. {reason} | {e}"
-                        )
+                        reason = 'empty data' if df_iex.empty else 'unparseable timestamps'
+                        logger.warning(f'Invalid IEX daily index for {symbol}; skipping. {reason} | {e}')
                         return None
                     df_iex.index = idx
                     df = df_iex.rename(columns=lambda c: c.lower())
                 except Exception as iex_err:
-                    logger.warning(f"ALPACA IEX ERROR for {symbol}: {repr(iex_err)}")
-                    logger.info(
-                        f"INSERTING DUMMY DAILY FOR {symbol} ON {end_ts.date().isoformat()}"
-                    )
-                    ts = pd.to_datetime(end_ts, utc=True, errors="coerce")
+                    logger.warning(f'ALPACA IEX ERROR for {symbol}: {repr(iex_err)}')
+                    logger.info(f'INSERTING DUMMY DAILY FOR {symbol} ON {end_ts.date().isoformat()}')
+                    ts = pd.to_datetime(end_ts, utc=True, errors='coerce')
                     if ts is None:
-                        ts = pd.Timestamp.now(tz="UTC")
+                        ts = pd.Timestamp.now(tz='UTC')
                     dummy_date = ts
-                    df = pd.DataFrame(
-                        [
-                            {
-                                "open": 0.0,
-                                "high": 0.0,
-                                "low": 0.0,
-                                "close": 0.0,
-                                "volume": 0,
-                            }
-                        ],
-                        index=[dummy_date],
-                    )
+                    df = pd.DataFrame([{'open': 0.0, 'high': 0.0, 'low': 0.0, 'close': 0.0, 'volume': 0}], index=[dummy_date])
             else:
-                logger.warning(f"ALPACA DAILY FETCH ERROR for {symbol}: {repr(e)}")
-                ts2 = pd.to_datetime(end_ts, utc=True, errors="coerce")
+                logger.warning(f'ALPACA DAILY FETCH ERROR for {symbol}: {repr(e)}')
+                ts2 = pd.to_datetime(end_ts, utc=True, errors='coerce')
                 if ts2 is None:
-                    ts2 = pd.Timestamp.now(tz="UTC")
+                    ts2 = pd.Timestamp.now(tz='UTC')
                 dummy_date = ts2
-                df = pd.DataFrame(
-                    [{"open": 0.0, "high": 0.0, "low": 0.0, "close": 0.0, "volume": 0}],
-                    index=[dummy_date],
-                )
+                df = pd.DataFrame([{'open': 0.0, 'high': 0.0, 'low': 0.0, 'close': 0.0, 'volume': 0}], index=[dummy_date])
         except Exception as e:
-            logger.error(f"Failed to fetch daily data for {symbol}: {repr(e)}")
+            logger.error(f'Failed to fetch daily data for {symbol}: {repr(e)}')
             return None
-
         with cache_lock:
             self._daily_cache[symbol] = df
         return df
 
-    def get_minute_df(
-        self, ctx: BotContext, symbol: str, lookback_minutes: int = 30
-    ) -> pd.DataFrame | None:
+    def get_minute_df(self, ctx: BotContext, symbol: str, lookback_minutes: int=30) -> pd.DataFrame | None:
         symbol = symbol.upper()
         now_utc = datetime.now(UTC)
-        last_closed_minute = now_utc.replace(second=0, microsecond=0) - timedelta(
-            minutes=1
-        )
+        last_closed_minute = now_utc.replace(second=0, microsecond=0) - timedelta(minutes=1)
         start_minute = last_closed_minute - timedelta(minutes=lookback_minutes)
-
         with cache_lock:
             last_ts = self._minute_timestamps.get(symbol)
             if last_ts and last_ts > now_utc - timedelta(seconds=ttl_seconds()):
@@ -2416,119 +1531,87 @@                     try:
                         minute_cache_hit.inc()
                     except Exception as exc:
-                        logger.exception("bot.py unexpected", exc_info=exc)
+                        logger.exception('bot.py unexpected', exc_info=exc)
                         raise
                 return self._minute_cache[symbol]
-
         if minute_cache_miss:
             try:
                 minute_cache_miss.inc()
             except Exception as exc:
-                logger.exception("bot.py unexpected", exc_info=exc)
+                logger.exception('bot.py unexpected', exc_info=exc)
                 raise
         api_key = get_settings().alpaca_api_key
         api_secret = get_settings().alpaca_secret_key
         if not api_key or not api_secret:
-            raise RuntimeError(
-                "ALPACA_API_KEY and ALPACA_SECRET_KEY must be set for data fetching"
-            )
+            raise RuntimeError('ALPACA_API_KEY and ALPACA_SECRET_KEY must be set for data fetching')
         client = StockHistoricalDataClient(api_key, api_secret)
-
         try:
-            req = StockBarsRequest(
-                symbol_or_symbols=[symbol],
-                timeframe=TimeFrame.Minute,
-                start=start_minute,
-                end=last_closed_minute,
-                feed=_DEFAULT_FEED,
-            )
-            bars = safe_get_stock_bars(client, req, symbol, "MINUTE")
+            req = StockBarsRequest(symbol_or_symbols=[symbol], timeframe=TimeFrame.Minute, start=start_minute, end=last_closed_minute, feed=_DEFAULT_FEED)
+            bars = safe_get_stock_bars(client, req, symbol, 'MINUTE')
             if bars is None:
                 return None
             if isinstance(bars.columns, _RealMultiIndex):
                 bars = bars.xs(symbol, level=0, axis=1)
             else:
-                bars = bars.drop(columns=["symbol"], errors="ignore")
+                bars = bars.drop(columns=['symbol'], errors='ignore')
             if bars.empty:
-                logger.warning(
-                    f"No minute bars returned for {symbol}. Possible market holiday or API outage"
-                )
+                logger.warning(f'No minute bars returned for {symbol}. Possible market holiday or API outage')
                 return None
             if len(bars.index) and isinstance(bars.index[0], tuple):
                 idx_vals = [t[1] for t in bars.index]
             else:
                 idx_vals = bars.index
             try:
-                idx = safe_to_datetime(idx_vals, context=f"minute {symbol}")
+                idx = safe_to_datetime(idx_vals, context=f'minute {symbol}')
             except ValueError as e:
-                reason = "empty data" if bars.empty else "unparseable timestamps"
-                logger.warning(
-                    f"Invalid minute index for {symbol}; skipping. {reason} | {e}"
-                )
+                reason = 'empty data' if bars.empty else 'unparseable timestamps'
+                logger.warning(f'Invalid minute index for {symbol}; skipping. {reason} | {e}')
                 return None
             bars.index = idx
-            df = bars.rename(columns=lambda c: c.lower()).drop(
-                columns=["symbol"], errors="ignore"
-            )[["open", "high", "low", "close", "volume"]]
+            df = bars.rename(columns=lambda c: c.lower()).drop(columns=['symbol'], errors='ignore')[['open', 'high', 'low', 'close', 'volume']]
         except APIError as e:
             err_msg = str(e)
-            if (
-                "subscription does not permit querying recent sip data"
-                in err_msg.lower()
-            ):
-                logger.warning(f"ALPACA SUBSCRIPTION ERROR for {symbol}: {repr(e)}")
-                logger.info(f"ATTEMPTING IEX-DELAYERED DATA FOR {symbol}")
+            if 'subscription does not permit querying recent sip data' in err_msg.lower():
+                logger.warning(f'ALPACA SUBSCRIPTION ERROR for {symbol}: {repr(e)}')
+                logger.info(f'ATTEMPTING IEX-DELAYERED DATA FOR {symbol}')
                 try:
-                    req.feed = "iex"
-                    df_iex = safe_get_stock_bars(client, req, symbol, "IEX MINUTE")
+                    req.feed = 'iex'
+                    df_iex = safe_get_stock_bars(client, req, symbol, 'IEX MINUTE')
                     if df_iex is None:
                         return None
                     if isinstance(df_iex.columns, _RealMultiIndex):
                         df_iex = df_iex.xs(symbol, level=0, axis=1)
                     else:
-                        df_iex = df_iex.drop(columns=["symbol"], errors="ignore")
+                        df_iex = df_iex.drop(columns=['symbol'], errors='ignore')
                     if len(df_iex.index) and isinstance(df_iex.index[0], tuple):
                         idx_vals = [t[1] for t in df_iex.index]
                     else:
                         idx_vals = df_iex.index
                     try:
-                        idx = safe_to_datetime(idx_vals, context=f"IEX minute {symbol}")
+                        idx = safe_to_datetime(idx_vals, context=f'IEX minute {symbol}')
                     except ValueError as _e:
-                        reason = (
-                            "empty data" if df_iex.empty else "unparseable timestamps"
-                        )
-                        logger.warning(
-                            f"Invalid IEX minute index for {symbol}; skipping. {reason} | {_e}"
-                        )
+                        reason = 'empty data' if df_iex.empty else 'unparseable timestamps'
+                        logger.warning(f'Invalid IEX minute index for {symbol}; skipping. {reason} | {_e}')
                         df = pd.DataFrame()
                     else:
                         df_iex.index = idx
-                        df = df_iex.rename(columns=lambda c: c.lower())[
-                            "open", "high", "low", "close", "volume"
-                        ]
+                        df = df_iex.rename(columns=lambda c: c.lower())['open', 'high', 'low', 'close', 'volume']
                 except Exception as iex_err:
-                    logger.warning(f"ALPACA IEX ERROR for {symbol}: {repr(iex_err)}")
-                    logger.info(f"NO ALTERNATIVE MINUTE DATA FOR {symbol}")
+                    logger.warning(f'ALPACA IEX ERROR for {symbol}: {repr(iex_err)}')
+                    logger.info(f'NO ALTERNATIVE MINUTE DATA FOR {symbol}')
                     df = pd.DataFrame()
             else:
-                logger.warning(f"ALPACA MINUTE FETCH ERROR for {symbol}: {repr(e)}")
+                logger.warning(f'ALPACA MINUTE FETCH ERROR for {symbol}: {repr(e)}')
                 df = pd.DataFrame()
         except Exception as e:
-            logger.warning(f"ALPACA MINUTE FETCH ERROR for {symbol}: {repr(e)}")
+            logger.warning(f'ALPACA MINUTE FETCH ERROR for {symbol}: {repr(e)}')
             df = pd.DataFrame()
-
         with cache_lock:
             self._minute_cache[symbol] = df
             self._minute_timestamps[symbol] = now_utc
         return df
 
-    def get_historical_minute(
-        self,
-        ctx: BotContext,  # ← still needs ctx here, per retrain.py
-        symbol: str,
-        start_date: date,
-        end_date: date,
-    ) -> pd.DataFrame | None:
+    def get_historical_minute(self, ctx: BotContext, symbol: str, start_date: date, end_date: date) -> pd.DataFrame | None:
         """
         Fetch all minute bars for `symbol` between start_date and end_date (inclusive),
         by calling Alpaca’s get_bars for each calendar day. Returns a DataFrame
@@ -2536,7 +1619,6 @@         """
         all_days: list[pd.DataFrame] = []
         current_day = start_date
-
         while current_day <= end_date:
             day_start = datetime.combine(current_day, dt_time.min, UTC)
             day_end = datetime.combine(current_day, dt_time.max, UTC)
@@ -2544,41 +1626,17 @@                 day_start, _tmp = day_start
             if isinstance(day_end, tuple):
                 _, day_end = day_end
-
             try:
-                bars_req = StockBarsRequest(
-                    symbol_or_symbols=[symbol],
-                    timeframe=TimeFrame.Minute,
-                    start=day_start,
-                    end=day_end,
-                    limit=10000,
-                    feed=_DEFAULT_FEED,
-                )
+                bars_req = StockBarsRequest(symbol_or_symbols=[symbol], timeframe=TimeFrame.Minute, start=day_start, end=day_end, limit=10000, feed=_DEFAULT_FEED)
                 try:
-                    bars_day = safe_get_stock_bars(
-                        ctx.data_client, bars_req, symbol, "INTRADAY"
-                    )
+                    bars_day = safe_get_stock_bars(ctx.data_client, bars_req, symbol, 'INTRADAY')
                     if bars_day is None:
                         return []
                 except APIError as e:
-                    if (
-                        "subscription does not permit" in str(e).lower()
-                        and _DEFAULT_FEED != "iex"
-                    ):
-                        logger.warning(
-                            (
-                                "[historic_minute] subscription error for %s %s-%s: %s; "
-                                "retrying with IEX"
-                            ),
-                            symbol,
-                            day_start,
-                            day_end,
-                            e,
-                        )
-                        bars_req.feed = "iex"
-                        bars_day = safe_get_stock_bars(
-                            ctx.data_client, bars_req, symbol, "IEX INTRADAY"
-                        )
+                    if 'subscription does not permit' in str(e).lower() and _DEFAULT_FEED != 'iex':
+                        logger.warning('[historic_minute] subscription error for %s %s-%s: %s; retrying with IEX', symbol, day_start, day_end, e)
+                        bars_req.feed = 'iex'
+                        bars_day = safe_get_stock_bars(ctx.data_client, bars_req, symbol, 'IEX INTRADAY')
                         if bars_day is None:
                             return []
                     else:
@@ -2586,86 +1644,53 @@                 if isinstance(bars_day.columns, _RealMultiIndex):
                     bars_day = bars_day.xs(symbol, level=0, axis=1)
                 else:
-                    bars_day = bars_day.drop(columns=["symbol"], errors="ignore")
+                    bars_day = bars_day.drop(columns=['symbol'], errors='ignore')
             except Exception as e:
-                logger.warning(
-                    f"[historic_minute] failed for {symbol} {day_start}-{day_end}: {e}"
-                )
+                logger.warning(f'[historic_minute] failed for {symbol} {day_start}-{day_end}: {e}')
                 bars_day = None
-
-            if bars_day is not None and not bars_day.empty:
-                if "symbol" in bars_day.columns:
-                    bars_day = bars_day.drop(columns=["symbol"], errors="ignore")
-
+            if bars_day is not None and (not bars_day.empty):
+                if 'symbol' in bars_day.columns:
+                    bars_day = bars_day.drop(columns=['symbol'], errors='ignore')
                 try:
-                    idx = safe_to_datetime(
-                        bars_day.index, context=f"historic minute {symbol}"
-                    )
+                    idx = safe_to_datetime(bars_day.index, context=f'historic minute {symbol}')
                 except ValueError as e:
-                    reason = (
-                        "empty data" if bars_day.empty else "unparseable timestamps"
-                    )
-                    logger.warning(
-                        f"Invalid minute index for {symbol}; skipping day {day_start}. {reason} | {e}"
-                    )
+                    reason = 'empty data' if bars_day.empty else 'unparseable timestamps'
+                    logger.warning(f'Invalid minute index for {symbol}; skipping day {day_start}. {reason} | {e}')
                     bars_day = None
                 else:
                     bars_day.index = idx
-                    bars_day = bars_day.rename(columns=lambda c: c.lower())[
-                        ["open", "high", "low", "close", "volume"]
-                    ]
+                    bars_day = bars_day.rename(columns=lambda c: c.lower())[['open', 'high', 'low', 'close', 'volume']]
                     all_days.append(bars_day)
-
             current_day += timedelta(days=1)
-
         if not all_days:
             return None
-
         combined = pd.concat(all_days, axis=0)
-        combined = combined[~combined.index.duplicated(keep="first")]
+        combined = combined[~combined.index.duplicated(keep='first')]
         combined = combined.sort_index()
         return combined
 
-
-# Helper to prefetch daily data in bulk with Alpaca, handling SIP subscription
-# issues and falling back to IEX delayed feed per symbol if needed.
-def prefetch_daily_data(
-    symbols: list[str], start_date: date, end_date: date
-) -> dict[str, pd.DataFrame]:
+def prefetch_daily_data(symbols: list[str], start_date: date, end_date: date) -> dict[str, pd.DataFrame]:
     alpaca_key = get_settings().alpaca_api_key
     alpaca_secret = get_settings().alpaca_secret_key
     if not alpaca_key or not alpaca_secret:
-        raise RuntimeError(
-            "ALPACA_API_KEY and ALPACA_SECRET_KEY must be set for data fetching"
-        )
+        raise RuntimeError('ALPACA_API_KEY and ALPACA_SECRET_KEY must be set for data fetching')
     client = StockHistoricalDataClient(alpaca_key, alpaca_secret)
-
-    try:
-        req = StockBarsRequest(
-            symbol_or_symbols=symbols,
-            timeframe=TimeFrame.Day,
-            start=start_date,
-            end=end_date,
-            feed=_DEFAULT_FEED,
-        )
-        bars = safe_get_stock_bars(client, req, str(symbols), "BULK DAILY")
+    try:
+        req = StockBarsRequest(symbol_or_symbols=symbols, timeframe=TimeFrame.Day, start=start_date, end=end_date, feed=_DEFAULT_FEED)
+        bars = safe_get_stock_bars(client, req, str(symbols), 'BULK DAILY')
         if bars is None:
             return {}
         if isinstance(bars.columns, _RealMultiIndex):
-            grouped_raw = {
-                sym: bars.xs(sym, level=0, axis=1)
-                for sym in symbols
-                if sym in bars.columns.get_level_values(0)
-            }
+            grouped_raw = {sym: bars.xs(sym, level=0, axis=1) for sym in symbols if sym in bars.columns.get_level_values(0)}
         else:
-            grouped_raw = dict(bars.groupby("symbol"))
+            grouped_raw = dict(bars.groupby('symbol'))
         grouped = {}
         for sym, df in grouped_raw.items():
-            df = df.drop(columns=["symbol"], errors="ignore")
+            df = df.drop(columns=['symbol'], errors='ignore')
             try:
-                idx = safe_to_datetime(df.index, context=f"bulk {sym}")
+                idx = safe_to_datetime(df.index, context=f'bulk {sym}')
             except ValueError as e:
-                logger.warning(f"Invalid bulk index for {sym}; skipping | {e}")
+                logger.warning(f'Invalid bulk index for {sym}; skipping | {e}')
                 continue
             df.index = idx
             df = df.rename(columns=lambda c: c.lower())
@@ -2673,233 +1698,132 @@         return grouped
     except APIError as e:
         err_msg = str(e).lower()
-        if "subscription does not permit querying recent sip data" in err_msg:
-            logger.warning(
-                f"ALPACA SUBSCRIPTION ERROR in bulk for {symbols}: {repr(e)}"
-            )
-            logger.info(f"ATTEMPTING IEX-DELAYERED BULK FETCH FOR {symbols}")
+        if 'subscription does not permit querying recent sip data' in err_msg:
+            logger.warning(f'ALPACA SUBSCRIPTION ERROR in bulk for {symbols}: {repr(e)}')
+            logger.info(f'ATTEMPTING IEX-DELAYERED BULK FETCH FOR {symbols}')
             try:
-                req.feed = "iex"
-                bars_iex = safe_get_stock_bars(
-                    client, req, str(symbols), "IEX BULK DAILY"
-                )
+                req.feed = 'iex'
+                bars_iex = safe_get_stock_bars(client, req, str(symbols), 'IEX BULK DAILY')
                 if bars_iex is None:
                     return {}
                 if isinstance(bars_iex.columns, _RealMultiIndex):
-                    grouped_raw = {
-                        sym: bars_iex.xs(sym, level=0, axis=1)
-                        for sym in symbols
-                        if sym in bars_iex.columns.get_level_values(0)
-                    }
+                    grouped_raw = {sym: bars_iex.xs(sym, level=0, axis=1) for sym in symbols if sym in bars_iex.columns.get_level_values(0)}
                 else:
-                    grouped_raw = dict(bars_iex.groupby("symbol"))
+                    grouped_raw = dict(bars_iex.groupby('symbol'))
                 grouped = {}
                 for sym, df in grouped_raw.items():
-                    df = df.drop(columns=["symbol"], errors="ignore")
+                    df = df.drop(columns=['symbol'], errors='ignore')
                     try:
-                        idx = safe_to_datetime(df.index, context=f"IEX bulk {sym}")
+                        idx = safe_to_datetime(df.index, context=f'IEX bulk {sym}')
                     except ValueError as e:
-                        logger.warning(
-                            f"Invalid IEX bulk index for {sym}; skipping | {e}"
-                        )
+                        logger.warning(f'Invalid IEX bulk index for {sym}; skipping | {e}')
                         continue
                     df.index = idx
                     df = df.rename(columns=lambda c: c.lower())
                     grouped[sym] = df
                 return grouped
             except Exception as iex_err:
-                logger.warning(f"ALPACA IEX BULK ERROR for {symbols}: {repr(iex_err)}")
+                logger.warning(f'ALPACA IEX BULK ERROR for {symbols}: {repr(iex_err)}')
                 daily_dict = {}
                 for sym in symbols:
                     try:
-                        req_sym = StockBarsRequest(
-                            symbol_or_symbols=[sym],
-                            timeframe=TimeFrame.Day,
-                            start=start_date,
-                            end=end_date,
-                            feed=_DEFAULT_FEED,
-                        )
-                        df_sym = safe_get_stock_bars(
-                            client, req_sym, sym, "FALLBACK DAILY"
-                        )
+                        req_sym = StockBarsRequest(symbol_or_symbols=[sym], timeframe=TimeFrame.Day, start=start_date, end=end_date, feed=_DEFAULT_FEED)
+                        df_sym = safe_get_stock_bars(client, req_sym, sym, 'FALLBACK DAILY')
                         if df_sym is None:
                             continue
-                        df_sym = df_sym.drop(columns=["symbol"], errors="ignore")
+                        df_sym = df_sym.drop(columns=['symbol'], errors='ignore')
                         try:
-                            idx = safe_to_datetime(
-                                df_sym.index, context=f"fallback bulk {sym}"
-                            )
+                            idx = safe_to_datetime(df_sym.index, context=f'fallback bulk {sym}')
                         except ValueError as _e:
-                            logger.warning(
-                                f"Invalid fallback bulk index for {sym}; skipping | {_e}"
-                            )
+                            logger.warning(f'Invalid fallback bulk index for {sym}; skipping | {_e}')
                             continue
                         df_sym.index = idx
                         df_sym = df_sym.rename(columns=lambda c: c.lower())
                         daily_dict[sym] = df_sym
                     except Exception as indiv_err:
-                        logger.warning(f"ALPACA IEX ERROR for {sym}: {repr(indiv_err)}")
-                        logger.info(
-                            f"INSERTING DUMMY DAILY FOR {sym} ON {end_date.isoformat()}"
-                        )
-                        tsd = pd.to_datetime(end_date, utc=True, errors="coerce")
+                        logger.warning(f'ALPACA IEX ERROR for {sym}: {repr(indiv_err)}')
+                        logger.info(f'INSERTING DUMMY DAILY FOR {sym} ON {end_date.isoformat()}')
+                        tsd = pd.to_datetime(end_date, utc=True, errors='coerce')
                         if tsd is None:
-                            tsd = pd.Timestamp.now(tz="UTC")
+                            tsd = pd.Timestamp.now(tz='UTC')
                         dummy_date = tsd
-                        dummy_df = pd.DataFrame(
-                            [
-                                {
-                                    "open": 0.0,
-                                    "high": 0.0,
-                                    "low": 0.0,
-                                    "close": 0.0,
-                                    "volume": 0,
-                                }
-                            ],
-                            index=[dummy_date],
-                        )
+                        dummy_df = pd.DataFrame([{'open': 0.0, 'high': 0.0, 'low': 0.0, 'close': 0.0, 'volume': 0}], index=[dummy_date])
                         daily_dict[sym] = dummy_df
                 return daily_dict
         else:
-            logger.warning(f"ALPACA BULK FETCH UNKNOWN ERROR for {symbols}: {repr(e)}")
+            logger.warning(f'ALPACA BULK FETCH UNKNOWN ERROR for {symbols}: {repr(e)}')
             daily_dict = {}
             for sym in symbols:
-                t2 = pd.to_datetime(end_date, utc=True, errors="coerce")
+                t2 = pd.to_datetime(end_date, utc=True, errors='coerce')
                 if t2 is None:
-                    t2 = pd.Timestamp.now(tz="UTC")
+                    t2 = pd.Timestamp.now(tz='UTC')
                 dummy_date = t2
-                dummy_df = pd.DataFrame(
-                    [{"open": 0.0, "high": 0.0, "low": 0.0, "close": 0.0, "volume": 0}],
-                    index=[dummy_date],
-                )
+                dummy_df = pd.DataFrame([{'open': 0.0, 'high': 0.0, 'low': 0.0, 'close': 0.0, 'volume': 0}], index=[dummy_date])
                 daily_dict[sym] = dummy_df
             return daily_dict
     except Exception as e:
-        logger.warning(f"ALPACA BULK FETCH EXCEPTION for {symbols}: {repr(e)}")
+        logger.warning(f'ALPACA BULK FETCH EXCEPTION for {symbols}: {repr(e)}')
         daily_dict = {}
         for sym in symbols:
-            t3 = pd.to_datetime(end_date, utc=True, errors="coerce")
+            t3 = pd.to_datetime(end_date, utc=True, errors='coerce')
             if t3 is None:
-                t3 = pd.Timestamp.now(tz="UTC")
+                t3 = pd.Timestamp.now(tz='UTC')
             dummy_date = t3
-            dummy_df = pd.DataFrame(
-                [{"open": 0.0, "high": 0.0, "low": 0.0, "close": 0.0, "volume": 0}],
-                index=[dummy_date],
-            )
+            dummy_df = pd.DataFrame([{'open': 0.0, 'high': 0.0, 'low': 0.0, 'close': 0.0, 'volume': 0}], index=[dummy_date])
             daily_dict[sym] = dummy_df
         return daily_dict
 
-
-# ─── E. TRADE LOGGER ───────────────────────────────────────────────────────────
 class TradeLogger:
-    def __init__(self, path: str = TRADE_LOG_FILE) -> None:
+
+    def __init__(self, path: str=TRADE_LOG_FILE) -> None:
         self.path = path
         if not os.path.exists(path):
             try:
-                with open(path, "w") as f:
+                with open(path, 'w') as f:
                     portalocker.lock(f, portalocker.LOCK_EX)
                     try:
-                        csv.writer(f).writerow(
-                            [
-                                "symbol",
-                                "entry_time",
-                                "entry_price",
-                                "exit_time",
-                                "exit_price",
-                                "qty",
-                                "side",
-                                "strategy",
-                                "classification",
-                                "signal_tags",
-                                "confidence",
-                                "reward",
-                            ]
-                        )
+                        csv.writer(f).writerow(['symbol', 'entry_time', 'entry_price', 'exit_time', 'exit_price', 'qty', 'side', 'strategy', 'classification', 'signal_tags', 'confidence', 'reward'])
                     finally:
                         portalocker.unlock(f)
             except PermissionError:
-                logger.debug("TradeLogger init path not writable: %s", path)
+                logger.debug('TradeLogger init path not writable: %s', path)
         if not os.path.exists(REWARD_LOG_FILE):
             try:
-                os.makedirs(os.path.dirname(REWARD_LOG_FILE) or ".", exist_ok=True)
-                with open(REWARD_LOG_FILE, "w", newline="") as rf:
-                    csv.writer(rf).writerow(
-                        [
-                            "timestamp",
-                            "symbol",
-                            "reward",
-                            "pnl",
-                            "confidence",
-                            "band",
-                        ]
-                    )
+                os.makedirs(os.path.dirname(REWARD_LOG_FILE) or '.', exist_ok=True)
+                with open(REWARD_LOG_FILE, 'w', newline='') as rf:
+                    csv.writer(rf).writerow(['timestamp', 'symbol', 'reward', 'pnl', 'confidence', 'band'])
             except Exception as e:
-                logger.warning(f"Failed to create reward log: {e}")
-
-    def log_entry(
-        self,
-        symbol: str,
-        price: float,
-        qty: int,
-        side: str,
-        strategy: str,
-        signal_tags: str = "",
-        confidence: float = 0.0,
-    ) -> None:
+                logger.warning(f'Failed to create reward log: {e}')
+
+    def log_entry(self, symbol: str, price: float, qty: int, side: str, strategy: str, signal_tags: str='', confidence: float=0.0) -> None:
         now_iso = utc_now_iso()
         try:
-            with open(self.path, "a") as f:
+            with open(self.path, 'a') as f:
                 portalocker.lock(f, portalocker.LOCK_EX)
                 try:
-                    csv.writer(f).writerow(
-                        [
-                            symbol,
-                            now_iso,
-                            price,
-                            "",
-                            "",
-                            qty,
-                            side,
-                            strategy,
-                            "",
-                            signal_tags,
-                            confidence,
-                            "",
-                        ]
-                    )
+                    csv.writer(f).writerow([symbol, now_iso, price, '', '', qty, side, strategy, '', signal_tags, confidence, ''])
                 finally:
                     portalocker.unlock(f)
         except PermissionError:
-            logger.debug("TradeLogger entry log skipped; path not writable")
+            logger.debug('TradeLogger entry log skipped; path not writable')
 
     def log_exit(self, state: BotState, symbol: str, exit_price: float) -> None:
         try:
-            with open(self.path, "r+") as f:
+            with open(self.path, 'r+') as f:
                 portalocker.lock(f, portalocker.LOCK_EX)
                 try:
                     rows = list(csv.reader(f))
-                    header, data = rows[0], rows[1:]
+                    header, data = (rows[0], rows[1:])
                     pnl = 0.0
                     conf = 0.0
                     for row in data:
-                        if row[0] == symbol and row[3] == "":
+                        if row[0] == symbol and row[3] == '':
                             entry_t = datetime.fromisoformat(row[1])
                             days = (datetime.now(UTC) - entry_t).days
-                            cls = (
-                                "day_trade"
-                                if days == 0
-                                else "swing_trade" if days < 5 else "long_trade"
-                            )
-                            row[3], row[4], row[8] = (
-                                utc_now_iso(),
-                                exit_price,
-                                cls,
-                            )
-                            # Compute PnL
+                            cls = 'day_trade' if days == 0 else 'swing_trade' if days < 5 else 'long_trade'
+                            row[3], row[4], row[8] = (utc_now_iso(), exit_price, cls)
                             entry_price = float(row[2])
-                            pnl = (exit_price - entry_price) * (
-                                1 if row[6] == "buy" else -1
-                            )
+                            pnl = (exit_price - entry_price) * (1 if row[6] == 'buy' else -1)
                             if len(row) >= 11:
                                 try:
                                     conf = float(row[10])
@@ -2919,63 +1843,30 @@                 finally:
                     portalocker.unlock(f)
         except PermissionError:
-            logger.debug("TradeLogger exit log skipped; path not writable")
+            logger.debug('TradeLogger exit log skipped; path not writable')
             return
-
-        # log reward
         try:
-            with open(REWARD_LOG_FILE, "a", newline="") as rf:
-                csv.writer(rf).writerow(
-                    [
-                        utc_now_iso(),
-                        symbol,
-                        pnl * conf,
-                        pnl,
-                        conf,
-                        ctx.capital_band,
-                    ]
-                )
+            with open(REWARD_LOG_FILE, 'a', newline='') as rf:
+                csv.writer(rf).writerow([utc_now_iso(), symbol, pnl * conf, pnl, conf, ctx.capital_band])
         except Exception as exc:
-            logger.exception("bot.py unexpected", exc_info=exc)
+            logger.exception('bot.py unexpected', exc_info=exc)
             raise
-
-        # Update streak-based kill-switch
         if pnl < 0:
             state.loss_streak += 1
         else:
             state.loss_streak = 0
         if state.loss_streak >= 3:
-            state.streak_halt_until = datetime.now(UTC).astimezone(PACIFIC) + timedelta(
-                minutes=60
-            )
-            logger.warning(
-                "STREAK_HALT_TRIGGERED",
-                extra={
-                    "loss_streak": state.loss_streak,
-                    "halt_until": state.streak_halt_until,
-                },
-            )
-
-        # AI-AGENT-REF: ai_trading/core/bot_engine.py:2960 - Convert import guard to settings-gated import
+            state.streak_halt_until = datetime.now(UTC).astimezone(PACIFIC) + timedelta(minutes=60)
+            logger.warning('STREAK_HALT_TRIGGERED', extra={'loss_streak': state.loss_streak, 'halt_until': state.streak_halt_until})
         from ai_trading.config import get_settings
         settings = get_settings()
-        if settings.enable_sklearn:  # Meta-learning requires sklearn
-            from ai_trading.meta_learning import (
-                _convert_audit_to_meta_format,
-                validate_trade_data_quality,
-            )
-
+        if settings.enable_sklearn:
+            from ai_trading.meta_learning import _convert_audit_to_meta_format, validate_trade_data_quality
             quality_report = validate_trade_data_quality(self.path)
-
-            # If we have audit format rows, trigger conversion for meta-learning
-            if quality_report.get("audit_format_rows", 0) > 0:
-                logger.info(
-                    "METALEARN_TRIGGER_CONVERSION: Converting audit format to meta-learning format"
-                )
-                # The conversion will be handled by the meta-learning system when it reads the log
+            if quality_report.get('audit_format_rows', 0) > 0:
+                logger.info('METALEARN_TRIGGER_CONVERSION: Converting audit format to meta-learning format')
         else:
-            logger.debug("Meta-learning disabled, skipping conversion")
-
+            logger.debug('Meta-learning disabled, skipping conversion')
 
 def _parse_local_positions() -> dict[str, int]:
     """Return current local open positions from the trade log."""
@@ -2983,169 +1874,102 @@     if not os.path.exists(TRADE_LOG_FILE):
         return positions
     try:
-        # AI-AGENT-REF: tolerate malformed CSV lines
-        df = pd.read_csv(
-            TRADE_LOG_FILE,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=["symbol", "qty", "side", "exit_time"],
-            dtype=str,
-        )
+        df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['symbol', 'qty', 'side', 'exit_time'], dtype=str)
         if df.empty:
-            logger.warning("Loaded DataFrame is empty after parsing/fallback")
+            logger.warning('Loaded DataFrame is empty after parsing/fallback')
     except pd.errors.ParserError as e:
-        logging.getLogger(__name__).warning(
-            "Failed to parse TRADE_LOG_FILE (malformed row): %s; returning empty set",
-            e,
-        )
+        logging.getLogger(__name__).warning('Failed to parse TRADE_LOG_FILE (malformed row): %s; returning empty set', e)
         return positions
     for _, row in df.iterrows():
-        if str(row.get("exit_time", "")) != "":
+        if str(row.get('exit_time', '')) != '':
             continue
         qty = int(row.qty)
-        qty = qty if row.side == "buy" else -qty
+        qty = qty if row.side == 'buy' else -qty
         positions[row.symbol] = positions.get(row.symbol, 0) + qty
     positions = {k: v for k, v in positions.items() if v != 0}
     return positions
-
 
 def audit_positions(ctx: BotContext) -> None:
     """
     Fetch local vs. broker positions and submit market orders to correct any mismatch.
     """
-    # 1) Read local open positions from the trade log
     local = _parse_local_positions()
-
-    # 2) Fetch remote (broker) positions
     try:
         remote = {p.symbol: int(p.qty) for p in ctx.api.get_all_positions()}
     except Exception as e:
         logger = logging.getLogger(__name__)
-        logger.exception("bot_engine: failed to fetch remote positions from broker", exc_info=e)
+        logger.exception('bot_engine: failed to fetch remote positions from broker', exc_info=e)
         return
-
-    max_order_size = int(os.getenv("MAX_ORDER_SIZE", "1000"))
-
-    # 3) For any symbol in remote whose remote_qty != local_qty, correct via market order
+    max_order_size = int(os.getenv('MAX_ORDER_SIZE', '1000'))
     for sym, rq in remote.items():
         lq = local.get(sym, 0)
         if lq != rq:
             diff = rq - lq
             if diff > 0:
-                # Broker has more shares than local: sell off the excess
                 try:
-                    req = MarketOrderRequest(
-                        symbol=sym,
-                        qty=min(abs(diff), max_order_size),
-                        side=OrderSide.SELL,
-                        time_in_force=TimeInForce.DAY,
-                    )
+                    req = MarketOrderRequest(symbol=sym, qty=min(abs(diff), max_order_size), side=OrderSide.SELL, time_in_force=TimeInForce.DAY)
                     safe_submit_order(ctx.api, req)
                 except Exception as exc:
-                    logger.exception("bot.py unexpected", exc_info=exc)
+                    logger.exception('bot.py unexpected', exc_info=exc)
                     raise
             else:
-                # Broker has fewer shares than local: buy back the missing shares
                 try:
-                    req = MarketOrderRequest(
-                        symbol=sym,
-                        qty=min(abs(diff), max_order_size),
-                        side=OrderSide.BUY,
-                        time_in_force=TimeInForce.DAY,
-                    )
+                    req = MarketOrderRequest(symbol=sym, qty=min(abs(diff), max_order_size), side=OrderSide.BUY, time_in_force=TimeInForce.DAY)
                     safe_submit_order(ctx.api, req)
                 except Exception as exc:
-                    logger.exception("bot.py unexpected", exc_info=exc)
+                    logger.exception('bot.py unexpected', exc_info=exc)
                     raise
-
-    # 4) For any symbol in local that is not in remote, submit order matching the local side
     for sym, lq in local.items():
         if sym not in remote:
-            # AI-AGENT-REF: prevent oversize orders on unmatched locals
             if abs(lq) > max_order_size:
-                logger.warning(
-                    "Order size %d exceeds maximum %d for %s",
-                    abs(lq),
-                    max_order_size,
-                    sym,
-                )
+                logger.warning('Order size %d exceeds maximum %d for %s', abs(lq), max_order_size, sym)
                 continue
             try:
                 side = OrderSide.BUY if lq > 0 else OrderSide.SELL
-                req = MarketOrderRequest(
-                    symbol=sym,
-                    qty=abs(lq),
-                    side=side,
-                    time_in_force=TimeInForce.DAY,
-                )
+                req = MarketOrderRequest(symbol=sym, qty=abs(lq), side=side, time_in_force=TimeInForce.DAY)
                 safe_submit_order(ctx.api, req)
             except Exception as exc:
-                logger.exception("bot.py unexpected", exc_info=exc)
+                logger.exception('bot.py unexpected', exc_info=exc)
                 raise
-
 
 def validate_open_orders(ctx: BotContext) -> None:
     local = _parse_local_positions()
     if not local:
-        logging.getLogger(__name__).debug(
-            "No local positions parsed; skipping open-order audit"
-        )
+        logging.getLogger(__name__).debug('No local positions parsed; skipping open-order audit')
         return
     try:
         open_orders = ctx.api.get_orders(GetOrdersRequest(status=QueryOrderStatus.OPEN))
     except Exception as e:
         logger = logging.getLogger(__name__)
-        logger.exception("bot_engine: failed to fetch open orders from broker", exc_info=e)
+        logger.exception('bot_engine: failed to fetch open orders from broker', exc_info=e)
         return
-
     now = datetime.now(UTC)
     for od in open_orders:
-        created = pd.to_datetime(getattr(od, "created_at", now))
+        created = pd.to_datetime(getattr(od, 'created_at', now))
         age = (now - created).total_seconds() / 60.0
-
-        if age > 5 and getattr(od, "status", "").lower() in {"new", "accepted"}:
+        if age > 5 and getattr(od, 'status', '').lower() in {'new', 'accepted'}:
             try:
                 ctx.api.cancel_order_by_id(od.id)
-                qty = int(getattr(od, "qty", 0))
-                side = getattr(od, "side", "")
-                if qty > 0 and side in {"buy", "sell"}:
-                    req = MarketOrderRequest(
-                        symbol=od.symbol,
-                        qty=qty,
-                        side=OrderSide.BUY if side == "buy" else OrderSide.SELL,
-                        time_in_force=TimeInForce.DAY,
-                    )
+                qty = int(getattr(od, 'qty', 0))
+                side = getattr(od, 'side', '')
+                if qty > 0 and side in {'buy', 'sell'}:
+                    req = MarketOrderRequest(symbol=od.symbol, qty=qty, side=OrderSide.BUY if side == 'buy' else OrderSide.SELL, time_in_force=TimeInForce.DAY)
                     safe_submit_order(ctx.api, req)
             except Exception as exc:
-                logger.exception("bot.py unexpected", exc_info=exc)
+                logger.exception('bot.py unexpected', exc_info=exc)
                 raise
-
-    # After canceling/replacing any stuck orders, fix any position mismatches
     audit_positions(ctx)
-
-
-# ─── F. SIGNAL MANAGER & HELPER FUNCTIONS ─────────────────────────────────────
 _LAST_PRICE: dict[str, float] = {}
-_SENTIMENT_CACHE: dict[str, tuple[float, float]] = {}  # {ticker: (timestamp, score)}
-PRICE_TTL_PCT = 0.005  # only fetch sentiment if price moved > 0.5%
-SENTIMENT_TTL_SEC = 600  # 10 minutes
-# AI-AGENT-REF: Enhanced sentiment caching for rate limiting
-SENTIMENT_RATE_LIMITED_TTL_SEC = 3600  # 1 hour cache when rate limited
-_SENTIMENT_CIRCUIT_BREAKER = {
-    "failures": 0,
-    "last_failure": 0,
-    "state": "closed",
-}  # closed, open, half-open
-# AI-AGENT-REF: Enhanced sentiment circuit breaker thresholds for better resilience
-SENTIMENT_FAILURE_THRESHOLD = (
-    15  # Increased to 15 failures for more tolerance per problem statement
-)
-SENTIMENT_RECOVERY_TIMEOUT = (
-    1800  # Extended to 30 minutes (1800s) for better recovery per problem statement
-)
-
+_SENTIMENT_CACHE: dict[str, tuple[float, float]] = {}
+PRICE_TTL_PCT = 0.005
+SENTIMENT_TTL_SEC = 600
+SENTIMENT_RATE_LIMITED_TTL_SEC = 3600
+_SENTIMENT_CIRCUIT_BREAKER = {'failures': 0, 'last_failure': 0, 'state': 'closed'}
+SENTIMENT_FAILURE_THRESHOLD = 15
+SENTIMENT_RECOVERY_TIMEOUT = 1800
 
 class SignalManager:
+
     def __init__(self) -> None:
         self.momentum_lookback = 5
         self.mean_rev_lookback = 20
@@ -3155,230 +1979,169 @@ 
     def signal_momentum(self, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
         if df is None or len(df) <= self.momentum_lookback:
-            return -1, 0.0, "momentum"
+            return (-1, 0.0, 'momentum')
         try:
-            df["momentum"] = df["close"].pct_change(
-                self.momentum_lookback, fill_method=None
-            )
-            val = df["momentum"].iloc[-1]
+            df['momentum'] = df['close'].pct_change(self.momentum_lookback, fill_method=None)
+            val = df['momentum'].iloc[-1]
             s = 1 if val > 0 else -1 if val < 0 else -1
             w = min(abs(val) * 10, 1.0)
-            return s, w, "momentum"
+            return (s, w, 'momentum')
         except (KeyError, ValueError, TypeError, IndexError):
-            logger.exception("Error in signal_momentum")
-            return -1, 0.0, "momentum"
-
-    def signal_mean_reversion(
-        self, df: pd.DataFrame, model=None
-    ) -> tuple[int, float, str]:
+            logger.exception('Error in signal_momentum')
+            return (-1, 0.0, 'momentum')
+
+    def signal_mean_reversion(self, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
         if df is None or len(df) < self.mean_rev_lookback:
-            return -1, 0.0, "mean_reversion"
+            return (-1, 0.0, 'mean_reversion')
         try:
-            ma = df["close"].rolling(self.mean_rev_lookback).mean()
-            sd = df["close"].rolling(self.mean_rev_lookback).std()
-            df["zscore"] = (df["close"] - ma) / sd
-            val = df["zscore"].iloc[-1]
-            s = (
-                -1
-                if val > self.mean_rev_zscore_threshold
-                else 1 if val < -self.mean_rev_zscore_threshold else -1
-            )
+            ma = df['close'].rolling(self.mean_rev_lookback).mean()
+            sd = df['close'].rolling(self.mean_rev_lookback).std()
+            df['zscore'] = (df['close'] - ma) / sd
+            val = df['zscore'].iloc[-1]
+            s = -1 if val > self.mean_rev_zscore_threshold else 1 if val < -self.mean_rev_zscore_threshold else -1
             w = min(abs(val) / 3, 1.0)
-            return s, w, "mean_reversion"
+            return (s, w, 'mean_reversion')
         except (KeyError, ValueError, TypeError, IndexError):
-            logger.exception("Error in signal_mean_reversion")
-            return -1, 0.0, "mean_reversion"
+            logger.exception('Error in signal_mean_reversion')
+            return (-1, 0.0, 'mean_reversion')
 
     def signal_stochrsi(self, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
-        if df is None or "stochrsi" not in df or df["stochrsi"].dropna().empty:
-            return -1, 0.0, "stochrsi"
+        if df is None or 'stochrsi' not in df or df['stochrsi'].dropna().empty:
+            return (-1, 0.0, 'stochrsi')
         try:
-            val = df["stochrsi"].iloc[-1]
+            val = df['stochrsi'].iloc[-1]
             s = 1 if val < 0.2 else -1 if val > 0.8 else -1
-            return s, 0.3, "stochrsi"
+            return (s, 0.3, 'stochrsi')
         except (KeyError, ValueError, TypeError, IndexError):
-            logger.exception("Error in signal_stochrsi")
-            return -1, 0.0, "stochrsi"
+            logger.exception('Error in signal_stochrsi')
+            return (-1, 0.0, 'stochrsi')
 
     def signal_obv(self, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
         if df is None or len(df) < 6:
-            return -1, 0.0, "obv"
+            return (-1, 0.0, 'obv')
         try:
-            obv = pd.Series(ta.obv(df["close"], df["volume"]).values)
+            obv = pd.Series(ta.obv(df['close'], df['volume']).values)
             if len(obv) < 5:
-                return -1, 0.0, "obv"
+                return (-1, 0.0, 'obv')
             slope = np.polyfit(range(5), obv.tail(5), 1)[0]
             s = 1 if slope > 0 else -1 if slope < 0 else -1
-            w = min(abs(slope) / 1e6, 1.0)
-            return s, w, "obv"
+            w = min(abs(slope) / 1000000.0, 1.0)
+            return (s, w, 'obv')
         except (KeyError, ValueError, TypeError, IndexError):
-            logger.exception("Error in signal_obv")
-            return -1, 0.0, "obv"
+            logger.exception('Error in signal_obv')
+            return (-1, 0.0, 'obv')
 
     def signal_vsa(self, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
         if df is None or len(df) < 20:
-            return -1, 0.0, "vsa"
+            return (-1, 0.0, 'vsa')
         try:
-            body = abs(df["close"] - df["open"])
-            vsa = df["volume"] * body
+            body = abs(df['close'] - df['open'])
+            vsa = df['volume'] * body
             score = vsa.iloc[-1]
             roll = vsa.rolling(20).mean()
             avg = roll.iloc[-1] if not roll.empty else 0.0
-            s = (
-                1
-                if df["close"].iloc[-1] > df["open"].iloc[-1]
-                else -1 if df["close"].iloc[-1] < df["open"].iloc[-1] else -1
-            )
-            # AI-AGENT-REF: Fix division by zero in VSA signal calculation
+            s = 1 if df['close'].iloc[-1] > df['open'].iloc[-1] else -1 if df['close'].iloc[-1] < df['open'].iloc[-1] else -1
             if avg > 0:
                 w = min(score / avg, 1.0)
             else:
-                w = 0.0  # Safe fallback when average is zero
-            return s, w, "vsa"
+                w = 0.0
+            return (s, w, 'vsa')
         except Exception:
-            logger.exception("Error in signal_vsa")
-            return -1, 0.0, "vsa"
-
-    def signal_ml(
-        self, df: pd.DataFrame, model: Any | None = None, symbol: str | None = None
-    ) -> tuple[int, float, str] | None:
+            logger.exception('Error in signal_vsa')
+            return (-1, 0.0, 'vsa')
+
+    def signal_ml(self, df: pd.DataFrame, model: Any | None=None, symbol: str | None=None) -> tuple[int, float, str] | None:
         """Machine learning prediction signal with probability logging."""
         if model is None and symbol is not None:
             model = _load_ml_model(symbol)
         if model is None:
             return None
         try:
-            if hasattr(model, "feature_names_in_"):
+            if hasattr(model, 'feature_names_in_'):
                 feat = list(model.feature_names_in_)
             else:
-                feat = ["rsi", "macd", "atr", "vwap", "sma_50", "sma_200"]
-
+                feat = ['rsi', 'macd', 'atr', 'vwap', 'sma_50', 'sma_200']
             X = df[feat].iloc[-1].values.reshape(1, -1)
             try:
                 pred = model.predict(X)[0]
                 proba = float(model.predict_proba(X)[0][pred])
             except Exception as e:
-                logger.error("signal_ml predict failed: %s", e)
-                return -1, 0.0, "ml"
+                logger.error('signal_ml predict failed: %s', e)
+                return (-1, 0.0, 'ml')
             s = 1 if pred == 1 else -1
-            logger.info(
-                "ML_SIGNAL", extra={"prediction": int(pred), "probability": proba}
-            )
-            return s, proba, "ml"
+            logger.info('ML_SIGNAL', extra={'prediction': int(pred), 'probability': proba})
+            return (s, proba, 'ml')
         except Exception as e:
-            logger.exception(f"signal_ml failed: {e}")
-            return -1, 0.0, "ml"
-
-    def signal_sentiment(
-        self, ctx: BotContext, ticker: str, df: pd.DataFrame = None, model: Any = None
-    ) -> tuple[int, float, str]:
+            logger.exception(f'signal_ml failed: {e}')
+            return (-1, 0.0, 'ml')
+
+    def signal_sentiment(self, ctx: BotContext, ticker: str, df: pd.DataFrame=None, model: Any=None) -> tuple[int, float, str]:
         """
         Only fetch sentiment if price has moved > PRICE_TTL_PCT; otherwise, return cached/neutral.
         """
         if df is None or df.empty:
-            return -1, 0.0, "sentiment"
-
+            return (-1, 0.0, 'sentiment')
         latest_close = float(get_latest_close(df))
         with sentiment_lock:
             prev_close = _LAST_PRICE.get(ticker)
-
-        # If price hasn’t moved enough, return cached or neutral
-        if (
-            prev_close is not None
-            and abs(latest_close - prev_close) / prev_close < PRICE_TTL_PCT
-        ):
+        if prev_close is not None and abs(latest_close - prev_close) / prev_close < PRICE_TTL_PCT:
             with sentiment_lock:
                 cached = _SENTIMENT_CACHE.get(ticker)
-                if cached and (pytime.time() - cached[0] < SENTIMENT_TTL_SEC):
+                if cached and pytime.time() - cached[0] < SENTIMENT_TTL_SEC:
                     score = cached[1]
                 else:
                     score = 0.0
                     _SENTIMENT_CACHE[ticker] = (pytime.time(), score)
         else:
-            # Price moved enough → fetch fresh sentiment
             try:
                 score = fetch_sentiment(ctx, ticker)
             except Exception as e:
-                logger.warning(f"[signal_sentiment] {ticker} error: {e}")
+                logger.warning(f'[signal_sentiment] {ticker} error: {e}')
                 score = 0.0
-
-        # Update last‐seen price & cache
         with sentiment_lock:
             _LAST_PRICE[ticker] = latest_close
             _SENTIMENT_CACHE[ticker] = (pytime.time(), score)
-
         score = max(-1.0, min(1.0, score))
         s = 1 if score > 0 else -1 if score < 0 else -1
         weight = abs(score)
         if is_high_vol_regime():
             weight *= 1.5
-        return s, weight, "sentiment"
-
-    def signal_regime(
-        self, state: BotState, df: pd.DataFrame, model=None
-    ) -> tuple[int, float, str]:
+        return (s, weight, 'sentiment')
+
+    def signal_regime(self, state: BotState, df: pd.DataFrame, model=None) -> tuple[int, float, str]:
         ok = check_market_regime(state)
         s = 1 if ok else -1
-        return s, 1.0, "regime"
+        return (s, 1.0, 'regime')
 
     def load_signal_weights(self) -> dict[str, float]:
         if not os.path.exists(SIGNAL_WEIGHTS_FILE):
             return {}
         try:
-            df = pd.read_csv(
-                SIGNAL_WEIGHTS_FILE,
-                on_bad_lines="skip",
-                engine="python",
-                usecols=["signal_name", "weight"],
-            )
+            df = pd.read_csv(SIGNAL_WEIGHTS_FILE, on_bad_lines='skip', engine='python', usecols=['signal_name', 'weight'])
             if df.empty:
-                logger.warning("Loaded DataFrame is empty after parsing/fallback")
+                logger.warning('Loaded DataFrame is empty after parsing/fallback')
                 return {}
-            return {row["signal_name"]: row["weight"] for _, row in df.iterrows()}
+            return {row['signal_name']: row['weight'] for _, row in df.iterrows()}
         except ValueError as e:
-            if "usecols" in str(e).lower():
-                logger.warning(
-                    "Signal weights CSV missing expected columns, trying fallback read"
-                )
+            if 'usecols' in str(e).lower():
+                logger.warning('Signal weights CSV missing expected columns, trying fallback read')
                 try:
-                    # Fallback: read all columns and try to map
-                    df = pd.read_csv(
-                        SIGNAL_WEIGHTS_FILE, on_bad_lines="skip", engine="python"
-                    )
-                    if "signal" in df.columns:
-                        # Old format with 'signal' column
-                        return {
-                            row["signal"]: row["weight"] for _, row in df.iterrows()
-                        }
-                    elif "signal_name" in df.columns:
-                        # New format with 'signal_name' column
-                        return {
-                            row["signal_name"]: row["weight"]
-                            for _, row in df.iterrows()
-                        }
+                    df = pd.read_csv(SIGNAL_WEIGHTS_FILE, on_bad_lines='skip', engine='python')
+                    if 'signal' in df.columns:
+                        return {row['signal']: row['weight'] for _, row in df.iterrows()}
+                    elif 'signal_name' in df.columns:
+                        return {row['signal_name']: row['weight'] for _, row in df.iterrows()}
                     else:
-                        logger.error(
-                            "Signal weights CSV has unexpected format: %s",
-                            df.columns.tolist(),
-                        )
+                        logger.error('Signal weights CSV has unexpected format: %s', df.columns.tolist())
                         return {}
                 except Exception as fallback_e:
-                    logger.error(
-                        "Failed to load signal weights with fallback: %s", fallback_e
-                    )
+                    logger.error('Failed to load signal weights with fallback: %s', fallback_e)
                     return {}
             else:
-                logger.error("Failed to load signal weights: %s", e)
+                logger.error('Failed to load signal weights: %s', e)
                 return {}
 
-    def evaluate(
-        self,
-        ctx: BotContext,
-        state: BotState,
-        df: pd.DataFrame,
-        ticker: str,
-        model: Any,
-    ) -> tuple[int, float, str]:
+    def evaluate(self, ctx: BotContext, state: BotState, df: pd.DataFrame, ticker: str, model: Any) -> tuple[int, float, str]:
         """Evaluate all active signals and return a combined decision.
 
         Parameters
@@ -3401,64 +2164,36 @@         """
         signals: list[tuple[int, float, str]] = []
         performance_data = load_global_signal_performance()
-
-        # AI-AGENT-REF: Graceful degradation when no meta-learning data exists
         if performance_data is None:
-            # For new deployments, allow all signal types with warning
-            logger.info(
-                "METALEARN_FALLBACK | No trade history - allowing all signals for new deployment"
-            )
-            allowed_tags = None  # None means allow all tags
+            logger.info('METALEARN_FALLBACK | No trade history - allowing all signals for new deployment')
+            allowed_tags = None
         else:
             allowed_tags = set(performance_data.keys())
             if not allowed_tags:
-                logger.warning(
-                    "METALEARN_NO_QUALIFIED_SIGNALS | No signals meet performance criteria - using basic signals"
-                )
-                # Use a basic set of reliable signal types as fallback
-                allowed_tags = {"sma_cross", "bb_squeeze", "rsi_oversold", "momentum"}
-
+                logger.warning('METALEARN_NO_QUALIFIED_SIGNALS | No signals meet performance criteria - using basic signals')
+                allowed_tags = {'sma_cross', 'bb_squeeze', 'rsi_oversold', 'momentum'}
         self.load_signal_weights()
-
-        # Track total signals evaluated
         if signals_evaluated:
             try:
                 signals_evaluated.inc()
             except Exception as exc:
-                logger.exception("bot.py unexpected", exc_info=exc)
+                logger.exception('bot.py unexpected', exc_info=exc)
                 raise
-
-        # simple moving averages
-        df["sma_50"] = df["close"].rolling(window=50).mean()
-        df["sma_200"] = df["close"].rolling(window=200).mean()
-
-        raw = [
-            self.signal_momentum(df, model),
-            self.signal_mean_reversion(df, model),
-            self.signal_ml(df, model, ticker),
-            self.signal_sentiment(ctx, ticker, df, model),
-            self.signal_regime(state, df, model),
-            self.signal_stochrsi(df, model),
-            self.signal_obv(df, model),
-            self.signal_vsa(df, model),
-        ]
-        # drop skipped signals
+        df['sma_50'] = df['close'].rolling(window=50).mean()
+        df['sma_200'] = df['close'].rolling(window=200).mean()
+        raw = [self.signal_momentum(df, model), self.signal_mean_reversion(df, model), self.signal_ml(df, model, ticker), self.signal_sentiment(ctx, ticker, df, model), self.signal_regime(state, df, model), self.signal_stochrsi(df, model), self.signal_obv(df, model), self.signal_vsa(df, model)]
         signals = [s for s in raw if s is not None]
         if not signals:
-            return 0.0, 0.0, "no_signals"
+            return (0.0, 0.0, 'no_signals')
         self.last_components = signals
-        score = sum(s * w for s, w, _ in signals)
-        confidence = sum(w for _, w, _ in signals)
-        labels = "+".join(label for _, _, label in signals)
-        return math.copysign(1, score), confidence, labels
-
-
-# ─── G. BOT CONTEXT ───────────────────────────────────────────────────────────
+        score = sum((s * w for s, w, _ in signals))
+        confidence = sum((w for _, w, _ in signals))
+        labels = '+'.join((label for _, _, label in signals))
+        return (math.copysign(1, score), confidence, labels)
+
 @dataclass
 class BotContext:
-    # Trading client using the new Alpaca SDK
     api: TradingClient
-    # Separate market data client
     data_client: StockHistoricalDataClient
     data_fetcher: DataFetcher
     signal_manager: SignalManager
@@ -3479,7 +2214,7 @@     params: dict
     sector_cap: float = SECTOR_EXPOSURE_CAP
     correlation_limit: float = CORRELATION_THRESHOLD
-    capital_band: str = "small"
+    capital_band: str = 'small'
     confirmation_count: dict[str, int] = field(default_factory=dict)
     trailing_extremes: dict[str, float] = field(default_factory=dict)
     take_profit_targets: dict[str, float] = field(default_factory=dict)
@@ -3487,7 +2222,6 @@     portfolio_weights: dict[str, float] = field(default_factory=dict)
     tickers: list[str] = field(default_factory=list)
     rebalance_buys: dict[str, datetime] = field(default_factory=dict)
-    # AI-AGENT-REF: track client_order_id base for INITIAL_REBALANCE orders
     rebalance_ids: dict[str, str] = field(default_factory=dict)
     rebalance_attempts: dict[str, int] = field(default_factory=dict)
     trailing_stop_data: dict[str, Any] = field(default_factory=dict)
@@ -3495,22 +2229,16 @@     allocator: StrategyAllocator | None = None
     strategies: list[Any] = field(default_factory=list)
     execution_engine: ExecutionEngine | None = None
-    # AI-AGENT-REF: Add drawdown circuit breaker for real-time protection
     drawdown_circuit_breaker: DrawdownCircuitBreaker | None = None
     logger: logging.Logger = logger
 
-    # AI-AGENT-REF: Add backward compatibility property for alpaca_client
     @property
     def alpaca_client(self):
         """Backward compatibility property for accessing the trading API client."""
         return self.api
-
-
 data_fetcher = DataFetcher()
 signal_manager = SignalManager()
-# AI-AGENT-REF: Lazy initialization for trade logger to speed up imports in testing
 trade_logger = None
-
 
 def get_trade_logger():
     """Get trade logger instance, creating it lazily."""
@@ -3518,98 +2246,61 @@     if trade_logger is None:
         trade_logger = TradeLogger()
     return trade_logger
-
-
 risk_engine = None
 allocator = None
 strategies = None
-
 
 def get_risk_engine():
     global risk_engine
     if risk_engine is None:
         from risk_engine import RiskEngine
-
         risk_engine = RiskEngine()
     return risk_engine
-
 
 def get_allocator():
     global allocator
     if allocator is None:
         from strategy_allocator import StrategyAllocator
-
         allocator = StrategyAllocator()
     return allocator
-
 
 def get_strategies():
     global strategies
     if strategies is None:
-        # AI-AGENT-REF: ai_trading/core/bot_engine.py:3556 - Convert import guard to hard import (internal module)
         from ai_trading.strategies import BaseStrategy
-
-        # Use BaseStrategy as fallback for test environments
         strategies = [BaseStrategy(), BaseStrategy()]
     return strategies
-
-
-# AI-AGENT-REF: Defer credential validation to runtime instead of import-time
-# This prevents crashes during import when environment variables are missing
 API_KEY = ALPACA_API_KEY
 API_SECRET = ALPACA_SECRET_KEY
 BASE_URL = get_settings().alpaca_base_url
 paper = ALPACA_PAPER
-
-# AI-AGENT-REF: Remove import-time credential validation - moved to runtime
-# Credential validation is now handled by _ensure_alpaca_env_or_raise() at runtime
-
-# AI-AGENT-REF: conditional client initialization with graceful fallback
 trading_client = None
 data_client = None
 stream = None
-
 
 def _initialize_alpaca_clients():
     """Initialize Alpaca trading clients lazily to avoid import delays."""
     global trading_client, data_client, stream
     if trading_client is not None:
-        return  # Already initialized
-    # Validate at runtime, now that .env should be present
+        return
     key, secret, base_url = _ensure_alpaca_env_or_raise()
     if not (key and secret):
-        # In SHADOW_MODE we may not have creds; skip client init
-        logger.info(
-            "Shadow mode or missing credentials: skipping Alpaca client initialization"
-        )
+        logger.info('Shadow mode or missing credentials: skipping Alpaca client initialization')
         return
-    # Lazy-import SDK only when needed
     try:
         from alpaca.data.historical import StockHistoricalDataClient
         from alpaca.trading.client import TradingClient
-
-        logger.debug("Successfully imported Alpaca SDK classes")
+        logger.debug('Successfully imported Alpaca SDK classes')
     except Exception as e:
-        logger.error(
-            "alpaca_trade_api import failed; cannot initialize clients", exc_info=e
-        )
-        # In test environments, don't raise - just skip initialization
-        if os.getenv("PYTEST_RUNNING") or os.getenv("TESTING"):
-            logger.info(
-                "Test environment detected, skipping Alpaca client initialization"
-            )
+        logger.error('alpaca_trade_api import failed; cannot initialize clients', exc_info=e)
+        if os.getenv('PYTEST_RUNNING') or os.getenv('TESTING'):
+            logger.info('Test environment detected, skipping Alpaca client initialization')
             return
         raise
-    # Initialize proper alpaca-py clients (do NOT use legacy REST for data)
-    is_paper = base_url.find("paper") != -1  # Determine if paper trading based on URL
+    is_paper = base_url.find('paper') != -1
     trading_client = TradingClient(key, secret, paper=is_paper)
     data_client = StockHistoricalDataClient(key, secret)
-    stream = None  # initialize stream lazily elsewhere if/when required
-
-
-# IMPORTANT: do not initialize Alpaca clients at import time.
-# They will be initialized on-demand by the functions that need them.
-
+    stream = None
 
 async def on_trade_update(event):
     """Handle order status updates from the Alpaca stream."""
@@ -3617,16 +2308,11 @@         symbol = event.order.symbol
         status = event.order.status
     except AttributeError:
-        # Fallback for dict-like event objects
-        symbol = event.order.get("symbol") if isinstance(event.order, dict) else "?"
-        status = event.order.get("status") if isinstance(event.order, dict) else "?"
-    logger.info(f"Trade update for {symbol}: {status}")
-
-
-# AI-AGENT-REF: Global context and engine will be initialized lazily
+        symbol = event.order.get('symbol') if isinstance(event.order, dict) else '?'
+        status = event.order.get('status') if isinstance(event.order, dict) else '?'
+    logger.info(f'Trade update for {symbol}: {status}')
 _ctx = None
 _exec_engine = None
-
 
 class LazyBotContext:
     """Wrapper that initializes the bot context lazily on first access."""
@@ -3639,102 +2325,35 @@         """Ensure the context is initialized."""
         _init_metrics()
         global _ctx, _exec_engine
-
         if self._initialized and self._context is not None:
             return
-
-        # Initialize Alpaca clients first if needed
         _initialize_alpaca_clients()
-
-        # AI-AGENT-REF: add null check for stream to handle Alpaca unavailable gracefully
-        if stream and hasattr(stream, "subscribe_trade_updates"):
+        if stream and hasattr(stream, 'subscribe_trade_updates'):
             try:
                 stream.subscribe_trade_updates(on_trade_update)
             except Exception as e:
-                logger.warning("Failed to subscribe to trade updates: %s", e)
-
-        self._context = BotContext(
-            api=trading_client,
-            data_client=data_client,
-            data_fetcher=data_fetcher,
-            signal_manager=signal_manager,
-            trade_logger=get_trade_logger(),
-            sem=Semaphore(4),
-            volume_threshold=VOLUME_THRESHOLD,
-            entry_start_offset=ENTRY_START_OFFSET,
-            entry_end_offset=ENTRY_END_OFFSET,
-            market_open=MARKET_OPEN,
-            market_close=MARKET_CLOSE,
-            regime_lookback=REGIME_LOOKBACK,
-            regime_atr_threshold=REGIME_ATR_THRESHOLD,
-            daily_loss_limit=DAILY_LOSS_LIMIT,
-            kelly_fraction=params.get("KELLY_FRACTION", 0.6),
-            capital_scaler=CapitalScalingEngine(params),
-            adv_target_pct=0.002,
-            max_position_dollars=10_000,
-            params=params,
-            confirmation_count={},
-            trailing_extremes={},
-            take_profit_targets={},
-            stop_targets={},
-            portfolio_weights={},
-            rebalance_buys={},
-            risk_engine=get_risk_engine(),
-            allocator=get_allocator(),
-            strategies=get_strategies(),
-            # AI-AGENT-REF: Initialize drawdown circuit breaker for real-time protection
-            drawdown_circuit_breaker=(
-                DrawdownCircuitBreaker(
-                    max_drawdown=config.MAX_DRAWDOWN_THRESHOLD, recovery_threshold=0.8
-                )
-                if DrawdownCircuitBreaker
-                else None
-            ),
-        )
-        _exec_engine = ExecutionEngine(
-            self._context,
-            slippage_total=slippage_total,
-            slippage_count=slippage_count,
-            orders_total=orders_total,
-        )
+                logger.warning('Failed to subscribe to trade updates: %s', e)
+        self._context = BotContext(api=trading_client, data_client=data_client, data_fetcher=data_fetcher, signal_manager=signal_manager, trade_logger=get_trade_logger(), sem=Semaphore(4), volume_threshold=VOLUME_THRESHOLD, entry_start_offset=ENTRY_START_OFFSET, entry_end_offset=ENTRY_END_OFFSET, market_open=MARKET_OPEN, market_close=MARKET_CLOSE, regime_lookback=REGIME_LOOKBACK, regime_atr_threshold=REGIME_ATR_THRESHOLD, daily_loss_limit=DAILY_LOSS_LIMIT, kelly_fraction=params.get('KELLY_FRACTION', 0.6), capital_scaler=CapitalScalingEngine(params), adv_target_pct=0.002, max_position_dollars=10000, params=params, confirmation_count={}, trailing_extremes={}, take_profit_targets={}, stop_targets={}, portfolio_weights={}, rebalance_buys={}, risk_engine=get_risk_engine(), allocator=get_allocator(), strategies=get_strategies(), drawdown_circuit_breaker=DrawdownCircuitBreaker(max_drawdown=S.max_drawdown_threshold, recovery_threshold=0.8) if DrawdownCircuitBreaker else None)
+        _exec_engine = ExecutionEngine(self._context, slippage_total=slippage_total, slippage_count=slippage_count, orders_total=orders_total)
         self._context.execution_engine = _exec_engine
-
-        # Propagate the capital_scaler to the risk engine so that position_size
         self._context.risk_engine.capital_scaler = self._context.capital_scaler
-
-        # Complete context setup (only in non-test environments)
-        if not (os.getenv("PYTEST_RUNNING") or os.getenv("TESTING")):
+        if not (os.getenv('PYTEST_RUNNING') or os.getenv('TESTING')):
             _initialize_bot_context_post_setup(self._context)
-
         _ctx = self._context
         self._initialized = True
 
-    def __getattr__(self, name):
-        """Delegate attribute access to the underlying context."""
-        self._ensure_initialized()
-        return getattr(self._context, name)
-
     def __setattr__(self, name, value):
         """Delegate attribute setting to the underlying context."""
-        if name.startswith("_") or name in ("_initialized", "_context"):
+        if name.startswith('_') or name in ('_initialized', '_context'):
             super().__setattr__(name, value)
         else:
             self._ensure_initialized()
             setattr(self._context, name, value)
-
-
-# Create the lazy context that will initialize on first use
 ctx = LazyBotContext()
-
 
 def get_ctx():
     """Get the global bot context (backwards compatibility)."""
     return ctx
-
-
-# AI-AGENT-REF: Defer context initialization to prevent expensive operations during import
-# The context will be created when first accessed via get_ctx() or _get_bot_context()
-
 
 def _initialize_bot_context_post_setup(ctx):
     """Complete bot context setup after creation."""
@@ -3744,15 +2363,11 @@         equity_init = 0.0
     ctx.capital_scaler.update(ctx, equity_init)
     ctx.last_positions = load_portfolio_snapshot()
-
-    # Warm up regime history cache so initial regime checks pass
     try:
         ctx.data_fetcher.get_daily_df(ctx, REGIME_SYMBOLS[0])
     except Exception as e:
-        logger.warning(f"[warm_cache] failed to seed regime history: {e}")
-
+        logger.warning(f'[warm_cache] failed to seed regime history: {e}')
     return ctx
-
 
 def data_source_health_check(ctx: BotContext, symbols: Sequence[str]) -> None:
     """Log warnings if no market data is available on startup."""
@@ -3764,15 +2379,9 @@     if not symbols:
         return
     if len(missing) == len(symbols):
-        logger.error(
-            "DATA_SOURCE_HEALTH_CHECK: No data for any symbol. Possible API outage or market holiday."
-        )
+        logger.error('DATA_SOURCE_HEALTH_CHECK: No data for any symbol. Possible API outage or market holiday.')
     elif missing:
-        logger.warning(
-            "DATA_SOURCE_HEALTH_CHECK: missing data for %s",
-            ", ".join(missing),
-        )
-
+        logger.warning('DATA_SOURCE_HEALTH_CHECK: missing data for %s', ', '.join(missing))
 
 def _ensure_data_fresh(symbols, max_age_seconds: int) -> None:
     """
@@ -3780,36 +2389,28 @@     Logs UTC timestamps and fails fast if any symbol is stale.
     """
     try:
-        from ai_trading.data_fetcher import (
-            last_minute_bar_age_seconds,
-        )  # type: ignore
+        from ai_trading.data_fetcher import last_minute_bar_age_seconds
     except Exception as e:
-        logger.warning("Data freshness check unavailable; skipping", exc_info=e)
+        logger.warning('Data freshness check unavailable; skipping', exc_info=e)
         return
     now_utc = utc_now_iso()
     stale = []
     for sym in symbols:
         age = last_minute_bar_age_seconds(sym)
         if age is None:
-            stale.append((sym, "no_cache"))
+            stale.append((sym, 'no_cache'))
         elif age > max_age_seconds:
-            stale.append((sym, f"age={int(age)}s"))
+            stale.append((sym, f'age={int(age)}s'))
     if stale:
-        details = ", ".join([f"{s}({r})" for s, r in stale])
-        logger.warning("Data staleness detected [UTC now=%s]: %s", now_utc, details)
-        raise RuntimeError(f"Stale minute-cache for symbols: {details}")
-    logger.debug("Data freshness OK [UTC now=%s]", now_utc)
-
-
-# AI-AGENT-REF: Skip expensive health checks during test imports to improve performance
-if not os.getenv("PYTEST_RUNNING"):
+        details = ', '.join([f'{s}({r})' for s, r in stale])
+        logger.warning('Data staleness detected [UTC now=%s]: %s', now_utc, details)
+        raise RuntimeError(f'Stale minute-cache for symbols: {details}')
+    logger.debug('Data freshness OK [UTC now=%s]', now_utc)
+if not os.getenv('PYTEST_RUNNING'):
     data_source_health_check(ctx, REGIME_SYMBOLS)
 
-
-@memory_profile  # AI-AGENT-REF: Monitor memory usage during health checks
-def pre_trade_health_check(
-    ctx: BotContext, symbols: Sequence[str], min_rows: int | None = None
-) -> dict:
+@memory_profile
+def pre_trade_health_check(ctx: BotContext, symbols: Sequence[str], min_rows: int | None=None) -> dict:
     """
     Validate symbol data sufficiency, required columns, and timezone sanity using chunked batch.
 
@@ -3819,290 +2420,186 @@       3) default 120
     Avoids `'BotContext' object has no attribute 'min_rows'` hard failures.
     """
-    # Robust min_rows resolution with precedence
     if min_rows is None:
-        min_rows = getattr(ctx, "min_rows", 120)
+        min_rows = getattr(ctx, 'min_rows', 120)
     min_rows = int(min_rows)
-
-    results = {
-        "checked": 0,
-        "failures": [],
-        "insufficient_rows": [],
-        "missing_columns": [],
-        "timezone_issues": [],
-    }
+    results = {'checked': 0, 'failures': [], 'insufficient_rows': [], 'missing_columns': [], 'timezone_issues': []}
     if not symbols:
         return results
-    # Compute start/end with fallbacks so this function is safe to call early in the loop
     settings = get_settings()
     _now = datetime.now(UTC)
-    _fallback_days = int(getattr(settings, "pretrade_lookback_days", 120))
-    _start = getattr(ctx, "lookback_start", _now - timedelta(days=_fallback_days))
-    _end = getattr(ctx, "lookback_end", _now)
-    frames = _fetch_universe_bars_chunked(
-        ctx=ctx,
-        symbols=symbols,
-        timeframe="1D",
-        start=_start,
-        end=_end,
-        feed=getattr(ctx, "data_feed", None),
-    )
+    _fallback_days = int(getattr(settings, 'pretrade_lookback_days', 120))
+    _start = getattr(ctx, 'lookback_start', _now - timedelta(days=_fallback_days))
+    _end = getattr(ctx, 'lookback_end', _now)
+    frames = _fetch_universe_bars_chunked(ctx=ctx, symbols=symbols, timeframe='1D', start=_start, end=_end, feed=getattr(ctx, 'data_feed', None))
     for sym in symbols:
         df = frames.get(sym)
-        if df is None or getattr(df, "empty", False):
-            results["failures"].append((sym, "no_data"))
+        if df is None or getattr(df, 'empty', False):
+            results['failures'].append((sym, 'no_data'))
             continue
-        results["checked"] += 1
+        results['checked'] += 1
         try:
-            # Use the function parameter, not a non-existent ctx attribute
             if len(df) < min_rows:
-                results["insufficient_rows"].append(sym)
+                results['insufficient_rows'].append(sym)
                 continue
-            _validate_columns(
-                df,
-                required=["timestamp", "open", "high", "low", "close", "volume"],
-                results=results,
-                symbol=sym,
-            )
+            _validate_columns(df, required=['timestamp', 'open', 'high', 'low', 'close', 'volume'], results=results, symbol=sym)
             _validate_timezones(df, results, sym)
         except Exception as exc:
-            results["failures"].append((sym, str(exc)))
+            results['failures'].append((sym, str(exc)))
     return results
-
 
 def _validate_columns(df, required, results, symbol):
     """Helper to validate required columns are present."""
     missing = [c for c in required if c not in df.columns]
     if missing:
-        results["missing_columns"].append(symbol)
-
+        results['missing_columns'].append(symbol)
 
 def _validate_timezones(df, results, symbol):
     """Helper to validate timezone information."""
-    if hasattr(df, "index") and hasattr(df.index, "tz") and df.index.tz is None:
-        results["timezone_issues"].append(symbol)
-
-
-# ─── H. MARKET HOURS GUARD ────────────────────────────────────────────────────
-
+    if hasattr(df, 'index') and hasattr(df.index, 'tz') and (df.index.tz is None):
+        results['timezone_issues'].append(symbol)
 
 def in_trading_hours(ts: pd.Timestamp) -> bool:
     if is_holiday(ts):
-        logger.warning(
-            f"No NYSE market schedule for {ts.date()}; skipping market open/close check."
-        )
+        logger.warning(f'No NYSE market schedule for {ts.date()}; skipping market open/close check.')
         return False
     try:
         return NY.open_at_time(get_market_schedule(), ts)
     except ValueError as exc:
-        logger.warning(f"Invalid schedule time {ts}: {exc}; assuming market closed")
+        logger.warning(f'Invalid schedule time {ts}: {exc}; assuming market closed')
         return False
 
-
-# ─── I. SENTIMENT & EVENTS ────────────────────────────────────────────────────
 @sleep_and_retry
 @limits(calls=30, period=60)
-@retry(
-    stop=stop_after_attempt(3),
-    wait=wait_exponential(multiplier=1, min=1, max=10),
-    retry=retry_if_exception_type(requests.RequestException),
-)
+@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10), retry=retry_if_exception_type(requests.RequestException))
 def get_sec_headlines(ctx: BotContext, ticker: str) -> str:
     with ctx.sem:
-        r = requests.get(
-            f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany"
-            f"&CIK={ticker}&type=8-K&count=5",
-            headers={"User-Agent": "AI Trading Bot"},
-            timeout=10,
-        )
+        r = requests.get(f'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}&type=8-K&count=5', headers={'User-Agent': 'AI Trading Bot'}, timeout=10)
         r.raise_for_status()
-
-    try:
-        soup = BeautifulSoup(r.content, "lxml")
+    try:
+        soup = BeautifulSoup(r.content, 'lxml')
         texts = []
-        for a in soup.find_all("a", string=re.compile(r"8[- ]?K")):
-            tr = a.find_parent("tr")
-            tds = tr.find_all("td") if tr else []
+        for a in soup.find_all('a', string=re.compile('8[- ]?K')):
+            tr = a.find_parent('tr')
+            tds = tr.find_all('td') if tr else []
             if len(tds) >= 4:
                 texts.append(tds[-1].get_text(strip=True))
-        return " ".join(texts)
+        return ' '.join(texts)
     except Exception as e:
-        logger.warning(f"[get_sec_headlines] parse failed for {ticker}: {e}")
-        return ""
-
+        logger.warning(f'[get_sec_headlines] parse failed for {ticker}: {e}')
+        return ''
 
 def _check_sentiment_circuit_breaker() -> bool:
     """Check if sentiment circuit breaker allows requests."""
     global _SENTIMENT_CIRCUIT_BREAKER
     now = pytime.time()
     cb = _SENTIMENT_CIRCUIT_BREAKER
-
-    if cb["state"] == "open":
-        if now - cb["last_failure"] > SENTIMENT_RECOVERY_TIMEOUT:
-            cb["state"] = "half-open"
-            logger.info("Sentiment circuit breaker moved to half-open state")
+    if cb['state'] == 'open':
+        if now - cb['last_failure'] > SENTIMENT_RECOVERY_TIMEOUT:
+            cb['state'] = 'half-open'
+            logger.info('Sentiment circuit breaker moved to half-open state')
             return True
         return False
     return True
 
-
 def _record_sentiment_success():
     """Record successful sentiment API call."""
     global _SENTIMENT_CIRCUIT_BREAKER
-    _SENTIMENT_CIRCUIT_BREAKER["failures"] = 0
-    if _SENTIMENT_CIRCUIT_BREAKER["state"] == "half-open":
-        _SENTIMENT_CIRCUIT_BREAKER["state"] = "closed"
-        logger.info("Sentiment circuit breaker closed - service recovered")
-
+    _SENTIMENT_CIRCUIT_BREAKER['failures'] = 0
+    if _SENTIMENT_CIRCUIT_BREAKER['state'] == 'half-open':
+        _SENTIMENT_CIRCUIT_BREAKER['state'] = 'closed'
+        logger.info('Sentiment circuit breaker closed - service recovered')
 
 def _record_sentiment_failure():
     """Record failed sentiment API call and update circuit breaker."""
     global _SENTIMENT_CIRCUIT_BREAKER
     cb = _SENTIMENT_CIRCUIT_BREAKER
-    cb["failures"] += 1
-    cb["last_failure"] = pytime.time()
-
-    if cb["failures"] >= SENTIMENT_FAILURE_THRESHOLD:
-        cb["state"] = "open"
-        logger.warning(
-            f"Sentiment circuit breaker opened after {cb['failures']} failures"
-        )
-
-
-@retry(
-    stop=stop_after_attempt(
-        2
-    ),  # Reduced from 3 to avoid hitting rate limits too quickly
-    wait=wait_exponential(multiplier=1, min=2, max=10),  # Increased delays
-    retry=retry_if_exception_type((requests.RequestException,)),
-)
+    cb['failures'] += 1
+    cb['last_failure'] = pytime.time()
+    if cb['failures'] >= SENTIMENT_FAILURE_THRESHOLD:
+        cb['state'] = 'open'
+        logger.warning(f"Sentiment circuit breaker opened after {cb['failures']} failures")
+
+@retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=10), retry=retry_if_exception_type((requests.RequestException,)))
 def fetch_sentiment(ctx: BotContext, ticker: str) -> float:
     """
     Fetch sentiment via NewsAPI + FinBERT + Form 4 signal.
     Uses a simple in-memory TTL cache to avoid hitting NewsAPI too often.
     If FinBERT isn’t available, return neutral 0.0.
     """
-    # Use new SENTIMENT_API_KEY or fallback to NEWS_API_KEY for backwards compatibility
     api_key = SENTIMENT_API_KEY or NEWS_API_KEY
     if not api_key:
-        logger.debug(
-            "No sentiment API key configured (checked SENTIMENT_API_KEY and NEWS_API_KEY)"
-        )
+        logger.debug('No sentiment API key configured (checked SENTIMENT_API_KEY and NEWS_API_KEY)')
         return 0.0
-
     now_ts = pytime.time()
-
-    # AI-AGENT-REF: Enhanced caching with longer TTL during rate limiting
     with sentiment_lock:
         cached = _SENTIMENT_CACHE.get(ticker)
         if cached:
             last_ts, last_score = cached
-            # Use longer cache during circuit breaker open state
-            cache_ttl = (
-                SENTIMENT_RATE_LIMITED_TTL_SEC
-                if _SENTIMENT_CIRCUIT_BREAKER["state"] == "open"
-                else SENTIMENT_TTL_SEC
-            )
+            cache_ttl = SENTIMENT_RATE_LIMITED_TTL_SEC if _SENTIMENT_CIRCUIT_BREAKER['state'] == 'open' else SENTIMENT_TTL_SEC
             if now_ts - last_ts < cache_ttl:
-                logger.debug(
-                    f"Sentiment cache hit for {ticker} (age: {(now_ts - last_ts)/60:.1f}m)"
-                )
+                logger.debug(f'Sentiment cache hit for {ticker} (age: {(now_ts - last_ts) / 60:.1f}m)')
                 return last_score
-
-    # Cache miss or stale → fetch fresh
-    # AI-AGENT-REF: Circuit breaker pattern for graceful degradation
     if not _check_sentiment_circuit_breaker():
-        logger.info(
-            f"Sentiment circuit breaker open, returning cached/neutral for {ticker}"
-        )
+        logger.info(f'Sentiment circuit breaker open, returning cached/neutral for {ticker}')
         with sentiment_lock:
-            # Try to use any existing cache, even if stale
             cached = _SENTIMENT_CACHE.get(ticker)
             if cached:
                 _, last_score = cached
-                logger.debug(f"Using stale cached sentiment {last_score} for {ticker}")
+                logger.debug(f'Using stale cached sentiment {last_score} for {ticker}')
                 return last_score
-            # No cache available, store and return neutral
             _SENTIMENT_CACHE[ticker] = (now_ts, 0.0)
             return 0.0
-
-    try:
-        # 1) Fetch NewsAPI articles using configurable URL
-        url = (
-            f"{SENTIMENT_API_URL}?"
-            f"q={ticker}&sortBy=publishedAt&language=en&pageSize=5"
-            f"&apiKey={api_key}"
-        )
+    try:
+        url = f'{SENTIMENT_API_URL}?q={ticker}&sortBy=publishedAt&language=en&pageSize=5&apiKey={api_key}'
         resp = requests.get(url, timeout=10)
-
         if resp.status_code == 429:
-            # AI-AGENT-REF: Enhanced rate limiting handling
-            logger.warning(
-                f"fetch_sentiment({ticker}) rate-limited → caching neutral with extended TTL"
-            )
+            logger.warning(f'fetch_sentiment({ticker}) rate-limited → caching neutral with extended TTL')
             _record_sentiment_failure()
             with sentiment_lock:
-                # Cache neutral score with extended TTL during rate limiting
                 _SENTIMENT_CACHE[ticker] = (now_ts, 0.0)
             return 0.0
-
         resp.raise_for_status()
-
         payload = resp.json()
-        articles = payload.get("articles", [])
+        articles = payload.get('articles', [])
         scores = []
         if articles:
             for art in articles:
-                text = (art.get("title") or "") + ". " + (art.get("description") or "")
+                text = (art.get('title') or '') + '. ' + (art.get('description') or '')
                 if text.strip():
                     scores.append(predict_text_sentiment(text))
         news_score = float(sum(scores) / len(scores)) if scores else 0.0
-
-        # 2) Fetch Form 4 data (insider trades) - with error handling
         form4_score = 0.0
         try:
             form4 = fetch_form4_filings(ticker)
-            # If any insider buy in last 7 days > $50k, boost sentiment
             for filing in form4:
-                if filing["type"] == "buy" and filing["dollar_amount"] > 50_000:
+                if filing['type'] == 'buy' and filing['dollar_amount'] > 50000:
                     form4_score += 0.1
         except Exception as e:
-            logger.debug(
-                f"Form4 fetch failed for {ticker}: {e}"
-            )  # Reduced to debug level
-
+            logger.debug(f'Form4 fetch failed for {ticker}: {e}')
         final_score = 0.8 * news_score + 0.2 * form4_score
         final_score = max(-1.0, min(1.0, final_score))
-
-        # AI-AGENT-REF: Record success and update cache
         _record_sentiment_success()
         with sentiment_lock:
             _SENTIMENT_CACHE[ticker] = (now_ts, final_score)
         return final_score
-
     except requests.exceptions.RequestException as e:
-        logger.warning(f"Sentiment API request failed for {ticker}: {e}")
+        logger.warning(f'Sentiment API request failed for {ticker}: {e}')
         _record_sentiment_failure()
-
-        # AI-AGENT-REF: Fallback to cached data or neutral if no cache
         with sentiment_lock:
             cached = _SENTIMENT_CACHE.get(ticker)
             if cached:
                 _, last_score = cached
-                logger.debug(
-                    f"Using cached sentiment fallback {last_score} for {ticker}"
-                )
+                logger.debug(f'Using cached sentiment fallback {last_score} for {ticker}')
                 return last_score
-            # No cache available, return neutral
             _SENTIMENT_CACHE[ticker] = (now_ts, 0.0)
             return 0.0
     except Exception as e:
-        logger.error(f"Unexpected error fetching sentiment for {ticker}: {e}")
+        logger.error(f'Unexpected error fetching sentiment for {ticker}: {e}')
         _record_sentiment_failure()
         with sentiment_lock:
             _SENTIMENT_CACHE[ticker] = (now_ts, 0.0)
         return 0.0
-
 
 def predict_text_sentiment(text: str) -> float:
     """
@@ -4111,65 +2608,48 @@     """
     if _HUGGINGFACE_AVAILABLE and _FINBERT_MODEL and _FINBERT_TOKENIZER:
         try:
-            inputs = _FINBERT_TOKENIZER(
-                text,
-                return_tensors="pt",
-                truncation=True,
-                max_length=128,
-            )
+            inputs = _FINBERT_TOKENIZER(text, return_tensors='pt', truncation=True, max_length=128)
             with torch.no_grad():
                 outputs = _FINBERT_MODEL(**inputs)
-                logits = outputs.logits[0]  # shape = (3,)
-                probs = torch.softmax(logits, dim=0)  # [p_neg, p_neu, p_pos]
-
+                logits = outputs.logits[0]
+                probs = torch.softmax(logits, dim=0)
             neg, neu, pos = probs.tolist()
             return float(pos - neg)
         except Exception as e:
-            logger.warning(
-                f"[predict_text_sentiment] FinBERT inference failed ({e}); returning neutral"
-            )
+            logger.warning(f'[predict_text_sentiment] FinBERT inference failed ({e}); returning neutral')
     return 0.0
-
 
 def fetch_form4_filings(ticker: str) -> list[dict]:
     """
     Scrape SEC Form 4 filings for insider trade info.
     Returns a list of dicts: {"date": datetime, "type": "buy"/"sell", "dollar_amount": float}.
     """
-    url = f"https://www.sec.gov/cgi-bin/own-disp?action=getowner&CIK={ticker}&type=4"
-    r = requests.get(url, headers={"User-Agent": "AI Trading Bot"}, timeout=10)
+    url = f'https://www.sec.gov/cgi-bin/own-disp?action=getowner&CIK={ticker}&type=4'
+    r = requests.get(url, headers={'User-Agent': 'AI Trading Bot'}, timeout=10)
     r.raise_for_status()
-    soup = BeautifulSoup(r.content, "lxml")
+    soup = BeautifulSoup(r.content, 'lxml')
     filings = []
-    # Parse table rows (approximate)
-    table = soup.find("table", {"class": "tableFile2"})
+    table = soup.find('table', {'class': 'tableFile2'})
     if not table:
         return filings
-    rows = table.find_all("tr")[1:]  # skip header
+    rows = table.find_all('tr')[1:]
     for row in rows:
-        cols = row.find_all("td")
+        cols = row.find_all('td')
         if len(cols) < 6:
             continue
         date_str = cols[3].get_text(strip=True)
         try:
-            fdate = datetime.strptime(date_str, "%Y-%m-%d")
+            fdate = datetime.strptime(date_str, '%Y-%m-%d')
         except Exception:
             continue
-        txn_type = cols[4].get_text(strip=True).lower()  # "purchase" or "sale"
-        amt_str = cols[5].get_text(strip=True).replace("$", "").replace(",", "")
+        txn_type = cols[4].get_text(strip=True).lower()
+        amt_str = cols[5].get_text(strip=True).replace('$', '').replace(',', '')
         try:
             amt = float(amt_str)
         except Exception:
             amt = 0.0
-        filings.append(
-            {
-                "date": fdate,
-                "type": ("buy" if "purchase" in txn_type else "sell"),
-                "dollar_amount": amt,
-            }
-        )
+        filings.append({'date': fdate, 'type': 'buy' if 'purchase' in txn_type else 'sell', 'dollar_amount': amt})
     return filings
-
 
 def _can_fetch_events(symbol: str) -> bool:
     now_ts = pytime.time()
@@ -4179,16 +2659,13 @@             try:
                 event_cooldown_hits.inc()
             except Exception as exc:
-                logger.exception("bot.py unexpected", exc_info=exc)
+                logger.exception('bot.py unexpected', exc_info=exc)
                 raise
         return False
     _LAST_EVENT_TS[symbol] = now_ts
     return True
-
-
 _calendar_cache: dict[str, pd.DataFrame] = {}
 _calendar_last_fetch: dict[str, date] = {}
-
 
 def get_calendar_safe(symbol: str) -> pd.DataFrame:
     today_date = date.today()
@@ -4197,130 +2674,95 @@     try:
         cal = yf.Ticker(symbol).calendar
     except HTTPError:
-        logger.warning(f"[Events] Rate limited for {symbol}; skipping events.")
+        logger.warning(f'[Events] Rate limited for {symbol}; skipping events.')
         cal = pd.DataFrame()
     except Exception as e:
-        logger.error(f"[Events] Error fetching calendar for {symbol}: {e}")
+        logger.error(f'[Events] Error fetching calendar for {symbol}: {e}')
         cal = pd.DataFrame()
     _calendar_cache[symbol] = cal
     _calendar_last_fetch[symbol] = today_date
     return cal
 
-
-def is_near_event(symbol: str, days: int = 3) -> bool:
+def is_near_event(symbol: str, days: int=3) -> bool:
     cal = get_calendar_safe(symbol)
-    if not hasattr(cal, "empty") or cal.empty:
+    if not hasattr(cal, 'empty') or cal.empty:
         return False
     try:
         dates = []
         for col in cal.columns:
-            if "Value" not in cal.index or col not in cal.columns:
+            if 'Value' not in cal.index or col not in cal.columns:
                 continue
-            raw = cal.at["Value", col]
+            raw = cal.at['Value', col]
             if isinstance(raw, list | tuple):
                 raw = raw[0]
             dates.append(pd.to_datetime(raw))
     except Exception:
-        logger.debug(
-            f"[Events] Malformed calendar for {symbol}, columns={getattr(cal, 'columns', None)}"
-        )
+        logger.debug(f"[Events] Malformed calendar for {symbol}, columns={getattr(cal, 'columns', None)}")
         return False
     today_ts = pd.Timestamp.now().normalize()
     cutoff = today_ts + pd.Timedelta(days=days)
-    return any(today_ts <= d <= cutoff for d in dates)
-
-
-# ─── J. RISK & GUARDS ─────────────────────────────────────────────────────────
-
+    return any((today_ts <= d <= cutoff for d in dates))
 
 @sleep_and_retry
 @limits(calls=200, period=60)
-@retry(
-    stop=stop_after_attempt(3),
-    wait=wait_exponential(multiplier=0.5, min=0.5, max=4),
-    retry=retry_if_exception_type(APIError),
-)
+@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.5, min=0.5, max=4), retry=retry_if_exception_type(APIError))
 def check_daily_loss(ctx: BotContext, state: BotState) -> bool:
     acct = safe_alpaca_get_account(ctx)
     if acct is None:
-        logger.warning("Daily loss check skipped - Alpaca account unavailable")
+        logger.warning('Daily loss check skipped - Alpaca account unavailable')
         return False
     equity = float(acct.equity)
     today_date = date.today()
-    limit = params.get("DAILY_LOSS_LIMIT", 0.07)
-
+    limit = params.get('DAILY_LOSS_LIMIT', 0.07)
     if state.day_start_equity is None or state.day_start_equity[0] != today_date:
         if state.last_drawdown >= 0.05:
             limit = 0.03
-        state.last_drawdown = (
-            (state.day_start_equity[1] - equity) / state.day_start_equity[1]
-            if state.day_start_equity
-            else 0.0
-        )
+        state.last_drawdown = (state.day_start_equity[1] - equity) / state.day_start_equity[1] if state.day_start_equity else 0.0
         state.day_start_equity = (today_date, equity)
         daily_drawdown.set(0.0)
         return False
-
     loss = (state.day_start_equity[1] - equity) / state.day_start_equity[1]
     daily_drawdown.set(loss)
     if loss > 0.05:
-        logger.warning("[WARNING] Daily drawdown = %.2f%%", loss * 100)
+        logger.warning('[WARNING] Daily drawdown = %.2f%%', loss * 100)
     return loss >= limit
-
 
 def check_weekly_loss(ctx: BotContext, state: BotState) -> bool:
     """Weekly portfolio drawdown guard."""
     acct = safe_alpaca_get_account(ctx)
     if acct is None:
-        logger.warning("Weekly loss check skipped - Alpaca account unavailable")
+        logger.warning('Weekly loss check skipped - Alpaca account unavailable')
         return False
     equity = float(acct.equity)
     today_date = date.today()
     week_start = today_date - timedelta(days=today_date.weekday())
-
     if state.week_start_equity is None or state.week_start_equity[0] != week_start:
         state.week_start_equity = (week_start, equity)
         weekly_drawdown.set(0.0)
         return False
-
     loss = (state.week_start_equity[1] - equity) / state.week_start_equity[1]
     weekly_drawdown.set(loss)
     return loss >= WEEKLY_DRAWDOWN_LIMIT
 
-
 def count_day_trades() -> int:
     if not os.path.exists(TRADE_LOG_FILE):
         return 0
-    df = pd.read_csv(
-        TRADE_LOG_FILE,
-        on_bad_lines="skip",
-        engine="python",
-        usecols=["entry_time", "exit_time"],
-    )
+    df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['entry_time', 'exit_time'])
     if df.empty:
-        logger.warning("Loaded DataFrame is empty after parsing/fallback")
-    df["entry_time"] = pd.to_datetime(df["entry_time"], errors="coerce")
-    df["exit_time"] = pd.to_datetime(df["exit_time"], errors="coerce")
-    df = df.dropna(subset=["entry_time", "exit_time"])
+        logger.warning('Loaded DataFrame is empty after parsing/fallback')
+    df['entry_time'] = pd.to_datetime(df['entry_time'], errors='coerce')
+    df['exit_time'] = pd.to_datetime(df['exit_time'], errors='coerce')
+    df = df.dropna(subset=['entry_time', 'exit_time'])
     today_ts = pd.Timestamp.now().normalize()
     bdays = pd.bdate_range(end=today_ts, periods=5)
-    df["entry_date"] = df["entry_time"].dt.normalize()
-    df["exit_date"] = df["exit_time"].dt.normalize()
-    mask = (
-        (df["entry_date"].isin(bdays))
-        & (df["exit_date"].isin(bdays))
-        & (df["entry_date"] == df["exit_date"])
-    )
+    df['entry_date'] = df['entry_time'].dt.normalize()
+    df['exit_date'] = df['exit_time'].dt.normalize()
+    mask = df['entry_date'].isin(bdays) & df['exit_date'].isin(bdays) & (df['entry_date'] == df['exit_date'])
     return int(mask.sum())
-
 
 @sleep_and_retry
 @limits(calls=200, period=60)
-@retry(
-    stop=stop_after_attempt(3),
-    wait=wait_exponential(multiplier=0.5, min=0.5, max=4),
-    retry=retry_if_exception_type(APIError),
-)
+@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.5, min=0.5, max=4), retry=retry_if_exception_type(APIError))
 def check_pdt_rule(ctx: BotContext) -> bool:
     """Check PDT rule with graceful degradation when Alpaca is unavailable.
 
@@ -4328,194 +2770,123 @@     operating in simulation mode.
     """
     acct = safe_alpaca_get_account(ctx)
-
-    # If account is unavailable (Alpaca not available), assume no PDT blocking
     if acct is None:
-        logger.info(
-            "PDT_CHECK_SKIPPED - Alpaca unavailable, assuming no PDT restrictions"
-        )
+        logger.info('PDT_CHECK_SKIPPED - Alpaca unavailable, assuming no PDT restrictions')
         return False
-
     try:
         equity = float(acct.equity)
     except (AttributeError, TypeError, ValueError):
-        logger.warning(
-            "PDT_CHECK_FAILED - Invalid equity value, assuming no PDT restrictions"
-        )
+        logger.warning('PDT_CHECK_FAILED - Invalid equity value, assuming no PDT restrictions')
         return False
-
-    # AI-AGENT-REF: Improve API null value handling for PDT checks
-    api_day_trades = (
-        getattr(acct, "pattern_day_trades", None)
-        or getattr(acct, "pattern_day_trades_count", None)
-        or 0  # Default to 0 if API returns null
-    )
-    api_buying_pw = (
-        getattr(acct, "daytrade_buying_power", None)
-        or getattr(acct, "day_trade_buying_power", None)
-        or getattr(acct, "buying_power", None)  # Fallback to regular buying power
-        or 0  # Default to 0 if API returns null
-    )
-
-    # Convert to proper types and handle potential string values
+    api_day_trades = getattr(acct, 'pattern_day_trades', None) or getattr(acct, 'pattern_day_trades_count', None) or 0
+    api_buying_pw = getattr(acct, 'daytrade_buying_power', None) or getattr(acct, 'day_trade_buying_power', None) or getattr(acct, 'buying_power', None) or 0
     try:
         api_day_trades = int(api_day_trades) if api_day_trades is not None else 0
     except (ValueError, TypeError):
-        logger.warning(
-            "Invalid day_trades value from API: %s, defaulting to 0", api_day_trades
-        )
+        logger.warning('Invalid day_trades value from API: %s, defaulting to 0', api_day_trades)
         api_day_trades = 0
-
     try:
         api_buying_pw = float(api_buying_pw) if api_buying_pw is not None else 0.0
     except (ValueError, TypeError):
-        logger.warning(
-            "Invalid buying_power value from API: %s, defaulting to 0", api_buying_pw
-        )
+        logger.warning('Invalid buying_power value from API: %s, defaulting to 0', api_buying_pw)
         api_buying_pw = 0.0
-
-    logger.info(
-        "PDT_CHECK",
-        extra={
-            "equity": equity,
-            "api_day_trades": api_day_trades,
-            "api_buying_pw": api_buying_pw,
-        },
-    )
-
+    logger.info('PDT_CHECK', extra={'equity': equity, 'api_day_trades': api_day_trades, 'api_buying_pw': api_buying_pw})
     if api_day_trades is not None and api_day_trades >= PDT_DAY_TRADE_LIMIT:
-        logger.info("SKIP_PDT_RULE", extra={"api_day_trades": api_day_trades})
+        logger.info('SKIP_PDT_RULE', extra={'api_day_trades': api_day_trades})
         return True
-
     if equity < PDT_EQUITY_THRESHOLD:
         if api_buying_pw and float(api_buying_pw) > 0:
-            logger.warning(
-                "PDT_EQUITY_LOW", extra={"equity": equity, "buying_pw": api_buying_pw}
-            )
+            logger.warning('PDT_EQUITY_LOW', extra={'equity': equity, 'buying_pw': api_buying_pw})
         else:
-            logger.warning(
-                "PDT_EQUITY_LOW_NO_BP",
-                extra={"equity": equity, "buying_pw": api_buying_pw},
-            )
+            logger.warning('PDT_EQUITY_LOW_NO_BP', extra={'equity': equity, 'buying_pw': api_buying_pw})
             return True
-
     return False
-
 
 def set_halt_flag(reason: str) -> None:
     """Persist a halt flag with the provided reason."""
     try:
-        with open(HALT_FLAG_PATH, "w") as f:
-            f.write(f"{reason} " + dt_.now(UTC).isoformat())
-        logger.info(f"TRADING_HALTED set due to {reason}")
-    except Exception as exc:  # pragma: no cover - disk issues
-        logger.error(f"Failed to write halt flag: {exc}")
-
-
-def check_halt_flag(ctx: BotContext | None = None) -> bool:
-    if config.FORCE_TRADES:
-        logger.warning("FORCE_TRADES override active: ignoring halt flag.")
+        with open(HALT_FLAG_PATH, 'w') as f:
+            f.write(f'{reason} ' + dt_.now(UTC).isoformat())
+        logger.info(f'TRADING_HALTED set due to {reason}')
+    except Exception as exc:
+        logger.error(f'Failed to write halt flag: {exc}')
+
+def check_halt_flag(ctx: BotContext | None=None) -> bool:
+    if S.force_trades:
+        logger.warning('FORCE_TRADES override active: ignoring halt flag.')
         return False
-
     reason = None
     if os.path.exists(HALT_FLAG_PATH):
         try:
             with open(HALT_FLAG_PATH) as f:
                 content = f.read().strip()
         except Exception:
-            content = ""
-        if content.startswith("MANUAL"):
-            reason = "manual override"
-        elif content.startswith("DISASTER"):
-            reason = "disaster flag"
+            content = ''
+        if content.startswith('MANUAL'):
+            reason = 'manual override'
+        elif content.startswith('DISASTER'):
+            reason = 'disaster flag'
     if ctx:
         dd = _current_drawdown()
         if dd >= DISASTER_DD_LIMIT:
-            reason = f"portfolio drawdown {dd:.2%} >= {DISASTER_DD_LIMIT:.2%}"
+            reason = f'portfolio drawdown {dd:.2%} >= {DISASTER_DD_LIMIT:.2%}'
         else:
             try:
                 acct = ctx.api.get_account()
-                if float(getattr(acct, "maintenance_margin", 0)) > float(acct.equity):
-                    reason = "margin breach"
+                if float(getattr(acct, 'maintenance_margin', 0)) > float(acct.equity):
+                    reason = 'margin breach'
             except Exception as e:
-                # Failed to check margin status - log error and treat as potential risk
-                logger.warning(
-                    "Failed to check margin status for halt condition: %s", e
-                )
-                reason = "margin check failure"
-
+                logger.warning('Failed to check margin status for halt condition: %s', e)
+                reason = 'margin check failure'
     if reason:
-        logger.info(f"TRADING_HALTED set due to {reason}")
+        logger.info(f'TRADING_HALTED set due to {reason}')
         return True
     return False
 
-
-def too_many_positions(ctx: BotContext, symbol: str | None = None) -> bool:
+def too_many_positions(ctx: BotContext, symbol: str | None=None) -> bool:
     """Check if there are too many positions, with allowance for rebalancing."""
     try:
         current_positions = ctx.api.get_all_positions()
         position_count = len(current_positions)
-
-        # If we're not at the limit, allow new positions
         if position_count < MAX_PORTFOLIO_POSITIONS:
             return False
-
-        # If we're at the limit, check if this is a rebalancing opportunity
         if symbol and position_count >= MAX_PORTFOLIO_POSITIONS:
-            # Allow trades for symbols we already have positions in (rebalancing)
             existing_symbols = {pos.symbol for pos in current_positions}
             if symbol in existing_symbols:
-                logger.info(
-                    f"ALLOW_REBALANCING | symbol={symbol} existing_positions={position_count}"
-                )
+                logger.info(f'ALLOW_REBALANCING | symbol={symbol} existing_positions={position_count}')
                 return False
-
-            # For new symbols at position limit, check if we can close underperforming positions
-            # This implements intelligent position management
-            logger.info(
-                f"POSITION_LIMIT_REACHED | current={position_count} max={MAX_PORTFOLIO_POSITIONS} new_symbol={symbol}"
-            )
-
+            logger.info(f'POSITION_LIMIT_REACHED | current={position_count} max={MAX_PORTFOLIO_POSITIONS} new_symbol={symbol}')
         return position_count >= MAX_PORTFOLIO_POSITIONS
-
     except Exception as e:
-        logger.warning(f"[too_many_positions] Could not fetch positions: {e}")
+        logger.warning(f'[too_many_positions] Could not fetch positions: {e}')
         return False
-
 
 def too_correlated(ctx: BotContext, sym: str) -> bool:
     if not os.path.exists(TRADE_LOG_FILE):
         return False
-    df = pd.read_csv(
-        TRADE_LOG_FILE,
-        on_bad_lines="skip",
-        engine="python",
-        usecols=["symbol", "exit_time"],
-    )
+    df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['symbol', 'exit_time'])
     if df.empty:
-        logger.warning("Loaded DataFrame is empty after parsing/fallback")
-    if "exit_time" not in df.columns or "symbol" not in df.columns:
+        logger.warning('Loaded DataFrame is empty after parsing/fallback')
+    if 'exit_time' not in df.columns or 'symbol' not in df.columns:
         return False
-    open_syms = df.loc[df.exit_time == "", "symbol"].unique().tolist() + [sym]
+    open_syms = df.loc[df.exit_time == '', 'symbol'].unique().tolist() + [sym]
     rets: dict[str, pd.Series] = {}
     for s in open_syms:
         d = ctx.data_fetcher.get_daily_df(ctx, s)
         if d is None or d.empty:
             continue
-        # Handle DataFrame with MultiIndex columns (symbol, field) or single-level
         if isinstance(d.columns, _RealMultiIndex):
-            if (s, "close") in d.columns:
-                series = d[(s, "close")].pct_change(fill_method=None).dropna()
+            if (s, 'close') in d.columns:
+                series = d[s, 'close'].pct_change(fill_method=None).dropna()
             else:
                 continue
         else:
-            series = d["close"].pct_change(fill_method=None).dropna()
+            series = d['close'].pct_change(fill_method=None).dropna()
         if not series.empty:
             rets[s] = series
-
     if len(rets) < 2:
         return False
-    min_len = min(len(r) for r in rets.values())
+    min_len = min((len(r) for r in rets.values()))
     if min_len < 1:
         return False
     good_syms = [s for s, r in rets.items() if len(r) >= min_len]
@@ -4523,9 +2894,8 @@     mat = pd.DataFrame({s: rets[s].tail(min_len).values for s in good_syms}, index=idx)
     corr_matrix = mat.corr().abs()
     avg_corr = corr_matrix.where(~np.eye(len(good_syms), dtype=bool)).stack().mean()
-    limit = getattr(ctx, "correlation_limit", CORRELATION_THRESHOLD)
+    limit = getattr(ctx, 'correlation_limit', CORRELATION_THRESHOLD)
     return avg_corr > limit
-
 
 def get_sector(symbol: str) -> str:
     """
@@ -4534,153 +2904,26 @@     """
     if symbol in _SECTOR_CACHE:
         return _SECTOR_CACHE[symbol]
-
-    # AI-AGENT-REF: Fallback sector mappings for common stocks when yfinance fails
-    SECTOR_MAPPINGS = {
-        # Technology
-        "AAPL": "Technology",
-        "MSFT": "Technology",
-        "GOOGL": "Technology",
-        "GOOG": "Technology",
-        "AMZN": "Technology",
-        "TSLA": "Technology",
-        "META": "Technology",
-        "NVDA": "Technology",
-        "NFLX": "Technology",
-        "AMD": "Technology",
-        "INTC": "Technology",
-        "ORCL": "Technology",
-        "CRM": "Technology",
-        "ADBE": "Technology",
-        "PYPL": "Technology",
-        "UBER": "Technology",
-        "SQ": "Technology",
-        "SHOP": "Technology",
-        "TWLO": "Technology",
-        "ZM": "Technology",
-        "PLTR": "Technology",  # AI-AGENT-REF: Added PLTR to Technology sector per problem statement
-        "BABA": "Technology",  # AI-AGENT-REF: Added BABA to Technology sector per problem statement
-        "JD": "Technology",
-        "PDD": "Technology",
-        "TCEHY": "Technology",  # Additional Chinese tech stocks
-        # Financial Services
-        "JPM": "Financial Services",
-        "BAC": "Financial Services",
-        "WFC": "Financial Services",
-        "GS": "Financial Services",
-        "MS": "Financial Services",
-        "C": "Financial Services",
-        "V": "Financial Services",
-        "MA": "Financial Services",
-        "BRK.B": "Financial Services",
-        "AXP": "Financial Services",
-        # Healthcare
-        "JNJ": "Healthcare",
-        "PFE": "Healthcare",
-        "ABBV": "Healthcare",
-        "MRK": "Healthcare",
-        "UNH": "Healthcare",
-        "TMO": "Healthcare",
-        "MDT": "Healthcare",
-        "ABT": "Healthcare",
-        "LLY": "Healthcare",
-        "BMY": "Healthcare",
-        "AMGN": "Healthcare",
-        "GILD": "Healthcare",
-        # Consumer Cyclical
-        "AMZN": "Consumer Cyclical",
-        "HD": "Consumer Cyclical",
-        "NKE": "Consumer Cyclical",
-        "MCD": "Consumer Cyclical",
-        "SBUX": "Consumer Cyclical",
-        "DIS": "Consumer Cyclical",
-        "LOW": "Consumer Cyclical",
-        "TGT": "Consumer Cyclical",
-        # Consumer Defensive
-        "PG": "Consumer Defensive",
-        "KO": "Consumer Defensive",
-        "PEP": "Consumer Defensive",
-        "WMT": "Consumer Defensive",
-        "COST": "Consumer Defensive",
-        "CL": "Consumer Defensive",
-        # Communication Services
-        "GOOGL": "Communication Services",
-        "GOOG": "Communication Services",
-        "META": "Communication Services",
-        "NFLX": "Communication Services",
-        "DIS": "Communication Services",
-        "VZ": "Communication Services",
-        "T": "Communication Services",
-        "CMCSA": "Communication Services",
-        # Energy
-        "XOM": "Energy",
-        "CVX": "Energy",
-        "COP": "Energy",
-        "EOG": "Energy",
-        "SLB": "Energy",
-        # Industrials
-        "BA": "Industrials",
-        "CAT": "Industrials",
-        "GE": "Industrials",
-        "MMM": "Industrials",
-        "UPS": "Industrials",
-        "HON": "Industrials",
-        "LMT": "Industrials",
-        "RTX": "Industrials",
-        # Utilities
-        "NEE": "Utilities",
-        "DUK": "Utilities",
-        "SO": "Utilities",
-        "D": "Utilities",
-        # Real Estate
-        "AMT": "Real Estate",
-        "CCI": "Real Estate",
-        "EQIX": "Real Estate",
-        "PSA": "Real Estate",
-        # Materials
-        "LIN": "Basic Materials",
-        "APD": "Basic Materials",
-        "ECL": "Basic Materials",
-        "DD": "Basic Materials",
-        # ETFs - treat as diversified
-        "SPY": "Diversified",
-        "QQQ": "Technology",
-        "IWM": "Diversified",
-        "VTI": "Diversified",
-        "VOO": "Diversified",
-        "VEA": "Diversified",
-        "VWO": "Diversified",
-        "BND": "Fixed Income",
-        "TLT": "Fixed Income",
-        "GLD": "Commodities",
-        "SLV": "Commodities",
-    }
-
-    # First try fallback mapping
+    SECTOR_MAPPINGS = {'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology', 'GOOG': 'Technology', 'AMZN': 'Technology', 'TSLA': 'Technology', 'META': 'Technology', 'NVDA': 'Technology', 'NFLX': 'Technology', 'AMD': 'Technology', 'INTC': 'Technology', 'ORCL': 'Technology', 'CRM': 'Technology', 'ADBE': 'Technology', 'PYPL': 'Technology', 'UBER': 'Technology', 'SQ': 'Technology', 'SHOP': 'Technology', 'TWLO': 'Technology', 'ZM': 'Technology', 'PLTR': 'Technology', 'BABA': 'Technology', 'JD': 'Technology', 'PDD': 'Technology', 'TCEHY': 'Technology', 'JPM': 'Financial Services', 'BAC': 'Financial Services', 'WFC': 'Financial Services', 'GS': 'Financial Services', 'MS': 'Financial Services', 'C': 'Financial Services', 'V': 'Financial Services', 'MA': 'Financial Services', 'BRK.B': 'Financial Services', 'AXP': 'Financial Services', 'JNJ': 'Healthcare', 'PFE': 'Healthcare', 'ABBV': 'Healthcare', 'MRK': 'Healthcare', 'UNH': 'Healthcare', 'TMO': 'Healthcare', 'MDT': 'Healthcare', 'ABT': 'Healthcare', 'LLY': 'Healthcare', 'BMY': 'Healthcare', 'AMGN': 'Healthcare', 'GILD': 'Healthcare', 'AMZN': 'Consumer Cyclical', 'HD': 'Consumer Cyclical', 'NKE': 'Consumer Cyclical', 'MCD': 'Consumer Cyclical', 'SBUX': 'Consumer Cyclical', 'DIS': 'Consumer Cyclical', 'LOW': 'Consumer Cyclical', 'TGT': 'Consumer Cyclical', 'PG': 'Consumer Defensive', 'KO': 'Consumer Defensive', 'PEP': 'Consumer Defensive', 'WMT': 'Consumer Defensive', 'COST': 'Consumer Defensive', 'CL': 'Consumer Defensive', 'GOOGL': 'Communication Services', 'GOOG': 'Communication Services', 'META': 'Communication Services', 'NFLX': 'Communication Services', 'DIS': 'Communication Services', 'VZ': 'Communication Services', 'T': 'Communication Services', 'CMCSA': 'Communication Services', 'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'EOG': 'Energy', 'SLB': 'Energy', 'BA': 'Industrials', 'CAT': 'Industrials', 'GE': 'Industrials', 'MMM': 'Industrials', 'UPS': 'Industrials', 'HON': 'Industrials', 'LMT': 'Industrials', 'RTX': 'Industrials', 'NEE': 'Utilities', 'DUK': 'Utilities', 'SO': 'Utilities', 'D': 'Utilities', 'AMT': 'Real Estate', 'CCI': 'Real Estate', 'EQIX': 'Real Estate', 'PSA': 'Real Estate', 'LIN': 'Basic Materials', 'APD': 'Basic Materials', 'ECL': 'Basic Materials', 'DD': 'Basic Materials', 'SPY': 'Diversified', 'QQQ': 'Technology', 'IWM': 'Diversified', 'VTI': 'Diversified', 'VOO': 'Diversified', 'VEA': 'Diversified', 'VWO': 'Diversified', 'BND': 'Fixed Income', 'TLT': 'Fixed Income', 'GLD': 'Commodities', 'SLV': 'Commodities'}
     if symbol in SECTOR_MAPPINGS:
         sector = SECTOR_MAPPINGS[symbol]
         _SECTOR_CACHE[symbol] = sector
-        logger.debug(f"Using fallback sector mapping for {symbol}: {sector}")
+        logger.debug(f'Using fallback sector mapping for {symbol}: {sector}')
         return sector
-
-    # Then try yfinance if available
     if YFINANCE_AVAILABLE:
         try:
             ticker_info = yf.Ticker(symbol).info
-            sector = ticker_info.get("sector", "Unknown")
-            if sector and sector != "Unknown":
+            sector = ticker_info.get('sector', 'Unknown')
+            if sector and sector != 'Unknown':
                 _SECTOR_CACHE[symbol] = sector
-                logger.debug(f"Retrieved sector from yfinance for {symbol}: {sector}")
+                logger.debug(f'Retrieved sector from yfinance for {symbol}: {sector}')
                 return sector
         except Exception as e:
-            logger.debug(f"yfinance sector lookup failed for {symbol}: {e}")
-
-    # Default to Unknown if all methods fail
-    sector = "Unknown"
+            logger.debug(f'yfinance sector lookup failed for {symbol}: {e}')
+    sector = 'Unknown'
     _SECTOR_CACHE[symbol] = sector
-    logger.warning(f"Could not determine sector for {symbol}, using Unknown")
+    logger.warning(f'Could not determine sector for {symbol}, using Unknown')
     return sector
-
 
 def sector_exposure(ctx: BotContext) -> dict[str, float]:
     """Return current portfolio exposure by sector as fraction of equity."""
@@ -4694,18 +2937,15 @@         total = 0.0
     exposure: dict[str, float] = {}
     for pos in positions:
-        qty = abs(int(getattr(pos, "qty", 0)))
-        price = float(
-            getattr(pos, "current_price", 0) or getattr(pos, "avg_entry_price", 0) or 0
-        )
-        sec = get_sector(getattr(pos, "symbol", ""))
+        qty = abs(int(getattr(pos, 'qty', 0)))
+        price = float(getattr(pos, 'current_price', 0) or getattr(pos, 'avg_entry_price', 0) or 0)
+        sec = get_sector(getattr(pos, 'symbol', ''))
         val = qty * price
         exposure[sec] = exposure.get(sec, 0.0) + val
     if total <= 0:
         return dict.fromkeys(exposure, 0.0)
     return {k: v / total for k, v in exposure.items()}
 
-
 def sector_exposure_ok(ctx: BotContext, symbol: str, qty: int, price: float) -> bool:
     """Return True if adding qty*price of symbol keeps sector exposure within cap."""
     sec = get_sector(symbol)
@@ -4713,479 +2953,256 @@     try:
         total = float(ctx.api.get_account().portfolio_value)
     except Exception as e:
-        logger.warning(
-            f"SECTOR_EXPOSURE_PORTFOLIO_ERROR: Failed to get portfolio value for {symbol}: {e}"
-        )
+        logger.warning(f'SECTOR_EXPOSURE_PORTFOLIO_ERROR: Failed to get portfolio value for {symbol}: {e}')
         total = 0.0
-
-    # Calculate trade value and exposure metrics
     trade_value = qty * price
     current_sector_exposure = exposures.get(sec, 0.0)
-    projected_exposure = (
-        current_sector_exposure + (trade_value / total) if total > 0 else 0.0
-    )
-    cap = getattr(ctx, "sector_cap", SECTOR_EXPOSURE_CAP)
-
-    # AI-AGENT-REF: Enhanced sector cap logic with clear reasoning
+    projected_exposure = current_sector_exposure + trade_value / total if total > 0 else 0.0
+    cap = getattr(ctx, 'sector_cap', SECTOR_EXPOSURE_CAP)
     if total <= 0:
-        # For empty portfolios, allow initial positions as they can't exceed sector caps
-        logger.info(
-            f"SECTOR_EXPOSURE_EMPTY_PORTFOLIO: Allowing initial position for {symbol} (sector: {sec})"
-        )
+        logger.info(f'SECTOR_EXPOSURE_EMPTY_PORTFOLIO: Allowing initial position for {symbol} (sector: {sec})')
         return True
-
-    # AI-AGENT-REF: Special handling for "Unknown" sector to prevent false concentration
-    if sec == "Unknown":
-        # Use a higher cap for Unknown sector since it's a catch-all category
-        # and may contain diversified stocks that couldn't be classified
-        unknown_cap = min(
-            cap * 2.0, 0.8
-        )  # Allow up to 2x normal cap or 80%, whichever is lower
-        logger.debug(
-            f"SECTOR_EXPOSURE_UNKNOWN: Using relaxed cap {unknown_cap:.1%} for Unknown sector"
-        )
+    if sec == 'Unknown':
+        unknown_cap = min(cap * 2.0, 0.8)
+        logger.debug(f'SECTOR_EXPOSURE_UNKNOWN: Using relaxed cap {unknown_cap:.1%} for Unknown sector')
         cap = unknown_cap
-
-    # Log detailed exposure analysis
     exposure_pct = current_sector_exposure * 100
     projected_pct = projected_exposure * 100
     cap_pct = cap * 100
-
-    # AI-AGENT-REF: Enhanced debugging for sector exposure analysis
-    logger.info(
-        f"SECTOR_EXPOSURE_DEBUG: {symbol} analysis - "
-        f"Sector: {sec}, Trade Value: ${trade_value:,.2f}, "
-        f"Portfolio Value: ${total:,.2f}, "
-        f"Current Sector Exposure: {exposure_pct:.1f}%, "
-        f"Projected Exposure: {projected_pct:.1f}%, "
-        f"Sector Cap: {cap_pct:.1f}%"
-    )
-
-    logger.debug(
-        f"SECTOR_EXPOSURE_ANALYSIS: {symbol} (sector: {sec}) - "
-        f"Current: {exposure_pct:.1f}%, Projected: {projected_pct:.1f}%, Cap: {cap_pct:.1f}%"
-    )
-
+    logger.info(f'SECTOR_EXPOSURE_DEBUG: {symbol} analysis - Sector: {sec}, Trade Value: ${trade_value:,.2f}, Portfolio Value: ${total:,.2f}, Current Sector Exposure: {exposure_pct:.1f}%, Projected Exposure: {projected_pct:.1f}%, Sector Cap: {cap_pct:.1f}%')
+    logger.debug(f'SECTOR_EXPOSURE_ANALYSIS: {symbol} (sector: {sec}) - Current: {exposure_pct:.1f}%, Projected: {projected_pct:.1f}%, Cap: {cap_pct:.1f}%')
     if projected_exposure <= cap:
-        logger.debug(
-            f"SECTOR_EXPOSURE_OK: {symbol} trade approved - projected exposure {projected_pct:.1f}% within {cap_pct:.1f}% cap"
-        )
+        logger.debug(f'SECTOR_EXPOSURE_OK: {symbol} trade approved - projected exposure {projected_pct:.1f}% within {cap_pct:.1f}% cap')
         return True
     else:
-        # Provide clear reasoning for sector cap rejection
         excess_pct = (projected_exposure - cap) * 100
-        logger.warning(
-            f"SECTOR_EXPOSURE_EXCEEDED: {symbol} trade rejected - "
-            f"projected exposure {projected_pct:.1f}% exceeds {cap_pct:.1f}% cap by {excess_pct:.1f}%",
-            extra={
-                "symbol": symbol,
-                "sector": sec,
-                "current_exposure_pct": exposure_pct,
-                "projected_exposure_pct": projected_pct,
-                "cap_pct": cap_pct,
-                "excess_pct": excess_pct,
-                "trade_value": trade_value,
-                "portfolio_value": total,
-                "reason": "sector_concentration_risk",
-            },
-        )
+        logger.warning(f'SECTOR_EXPOSURE_EXCEEDED: {symbol} trade rejected - projected exposure {projected_pct:.1f}% exceeds {cap_pct:.1f}% cap by {excess_pct:.1f}%', extra={'symbol': symbol, 'sector': sec, 'current_exposure_pct': exposure_pct, 'projected_exposure_pct': projected_pct, 'cap_pct': cap_pct, 'excess_pct': excess_pct, 'trade_value': trade_value, 'portfolio_value': total, 'reason': 'sector_concentration_risk'})
         return False
 
-
-# ─── K. SIZING & EXECUTION HELPERS ─────────────────────────────────────────────
 def is_within_entry_window(ctx: BotContext, state: BotState) -> bool:
     """Return True if current time is during regular Eastern trading hours."""
-    now_et = datetime.now(UTC).astimezone(ZoneInfo("America/New_York"))
+    now_et = datetime.now(UTC).astimezone(ZoneInfo('America/New_York'))
     start = dt_time(9, 30)
     end = dt_time(16, 0)
-    if not (start <= now_et.time() <= end):
-        logger.info(
-            "SKIP_ENTRY_WINDOW",
-            extra={"start": start, "end": end, "now": now_et.time()},
-        )
+    if not start <= now_et.time() <= end:
+        logger.info('SKIP_ENTRY_WINDOW', extra={'start': start, 'end': end, 'now': now_et.time()})
         return False
-    if (
-        state.streak_halt_until
-        and datetime.now(UTC).astimezone(PACIFIC) < state.streak_halt_until
-    ):
-        logger.info("SKIP_STREAK_HALT", extra={"until": state.streak_halt_until})
+    if state.streak_halt_until and datetime.now(UTC).astimezone(PACIFIC) < state.streak_halt_until:
+        logger.info('SKIP_STREAK_HALT', extra={'until': state.streak_halt_until})
         return False
     return True
 
-
-def scaled_atr_stop(
-    entry_price: float,
-    atr: float,
-    now: datetime,
-    market_open: datetime,
-    market_close: datetime,
-    max_factor: float = 2.0,
-    min_factor: float = 0.5,
-) -> tuple[float, float]:
+def scaled_atr_stop(entry_price: float, atr: float, now: datetime, market_open: datetime, market_close: datetime, max_factor: float=2.0, min_factor: float=0.5) -> tuple[float, float]:
     """Calculate scaled ATR stop-loss and take-profit with comprehensive validation."""
     try:
-        # AI-AGENT-REF: Add comprehensive input validation for stop-loss calculation
-
-        # Validate entry price
         if not isinstance(entry_price, int | float) or entry_price <= 0:
-            logger.error("Invalid entry price for ATR stop: %s", entry_price)
-            return (
-                entry_price * 0.95,
-                entry_price * 1.05,
-            )  # Return conservative defaults
-
-        # Validate ATR
+            logger.error('Invalid entry price for ATR stop: %s', entry_price)
+            return (entry_price * 0.95, entry_price * 1.05)
         if not isinstance(atr, int | float) or atr < 0:
-            logger.error("Invalid ATR for stop calculation: %s", atr)
-            return entry_price * 0.95, entry_price * 1.05
-
+            logger.error('Invalid ATR for stop calculation: %s', atr)
+            return (entry_price * 0.95, entry_price * 1.05)
         if atr == 0:
-            logger.warning("ATR is zero, using 1% stop/take levels")
-            return entry_price * 0.99, entry_price * 1.01
-
-        # Validate datetime inputs
-        if not all(isinstance(dt, datetime) for dt in [now, market_open, market_close]):
-            logger.error("Invalid datetime inputs for ATR stop calculation")
-            return entry_price * 0.95, entry_price * 1.05
-
-        # Validate market times make sense
+            logger.warning('ATR is zero, using 1% stop/take levels')
+            return (entry_price * 0.99, entry_price * 1.01)
+        if not all((isinstance(dt, datetime) for dt in [now, market_open, market_close])):
+            logger.error('Invalid datetime inputs for ATR stop calculation')
+            return (entry_price * 0.95, entry_price * 1.05)
         if market_close <= market_open:
-            logger.error(
-                "Invalid market times: close=%s <= open=%s", market_close, market_open
-            )
-            return entry_price * 0.95, entry_price * 1.05
-
-        # Validate factors
+            logger.error('Invalid market times: close=%s <= open=%s', market_close, market_open)
+            return (entry_price * 0.95, entry_price * 1.05)
         if not isinstance(max_factor, int | float) or max_factor <= 0:
-            logger.warning("Invalid max_factor %s, using default 2.0", max_factor)
+            logger.warning('Invalid max_factor %s, using default 2.0', max_factor)
             max_factor = 2.0
-
         if not isinstance(min_factor, int | float) or min_factor < 0:
-            logger.warning("Invalid min_factor %s, using default 0.5", min_factor)
+            logger.warning('Invalid min_factor %s, using default 0.5', min_factor)
             min_factor = 0.5
-
         if min_factor > max_factor:
-            logger.warning(
-                "min_factor %s > max_factor %s, swapping", min_factor, max_factor
-            )
-            min_factor, max_factor = max_factor, min_factor
-
-        # Calculate time-based scaling factor
+            logger.warning('min_factor %s > max_factor %s, swapping', min_factor, max_factor)
+            min_factor, max_factor = (max_factor, min_factor)
         total = (market_close - market_open).total_seconds()
         elapsed = (now - market_open).total_seconds()
-
-        # Handle edge cases
         if total <= 0:
-            logger.warning("Invalid market session duration: %s seconds", total)
-            α = 0.5  # Use middle factor
+            logger.warning('Invalid market session duration: %s seconds', total)
+            α = 0.5
         else:
             α = max(0, min(1, 1 - elapsed / total))
-
         factor = min_factor + α * (max_factor - min_factor)
-
-        # Validate factor is reasonable
-        if factor <= 0 or factor > 10:  # Sanity check - no more than 10x ATR
-            logger.warning("Calculated factor %s out of bounds, capping", factor)
+        if factor <= 0 or factor > 10:
+            logger.warning('Calculated factor %s out of bounds, capping', factor)
             factor = max(0.1, min(factor, 10.0))
-
         stop = entry_price - factor * atr
         take = entry_price + factor * atr
-
-        # Validate calculated levels are reasonable
         if stop < 0:
-            logger.warning("Calculated stop price %s is negative, adjusting", stop)
-            stop = entry_price * 0.5  # Minimum 50% stop
-
+            logger.warning('Calculated stop price %s is negative, adjusting', stop)
+            stop = entry_price * 0.5
         if take <= entry_price:
-            logger.warning(
-                "Calculated take profit %s <= entry price %s, adjusting",
-                take,
-                entry_price,
-            )
-            take = entry_price * 1.1  # Minimum 10% profit target
-
-        # Ensure stop is below entry and take is above entry
+            logger.warning('Calculated take profit %s <= entry price %s, adjusting', take, entry_price)
+            take = entry_price * 1.1
         if stop >= entry_price:
-            logger.warning(
-                "Stop price %s >= entry price %s, adjusting", stop, entry_price
-            )
+            logger.warning('Stop price %s >= entry price %s, adjusting', stop, entry_price)
             stop = entry_price * 0.95
-
         if take <= entry_price:
-            logger.warning(
-                "Take profit %s <= entry price %s, adjusting", take, entry_price
-            )
+            logger.warning('Take profit %s <= entry price %s, adjusting', take, entry_price)
             take = entry_price * 1.05
-
-        logger.debug(
-            "ATR stop calculation: entry=%s, atr=%s, factor=%s, stop=%s, take=%s",
-            entry_price,
-            atr,
-            factor,
-            stop,
-            take,
-        )
-
-        return stop, take
-
+        logger.debug('ATR stop calculation: entry=%s, atr=%s, factor=%s, stop=%s, take=%s', entry_price, atr, factor, stop, take)
+        return (stop, take)
     except Exception as e:
-        logger.error("Error in ATR stop calculation: %s", e)
-        # Return conservative defaults on error
-        return entry_price * 0.95, entry_price * 1.05
-
+        logger.error('Error in ATR stop calculation: %s', e)
+        return (entry_price * 0.95, entry_price * 1.05)
 
 def liquidity_factor(ctx: BotContext, symbol: str) -> float:
     try:
         df = fetch_minute_df_safe(symbol)
     except DataFetchError:
-        logger.warning("[liquidity_factor] no data for %s", symbol)
+        logger.warning('[liquidity_factor] no data for %s', symbol)
         return 0.0
     if df is None or df.empty:
         return 0.0
-    if "volume" not in df.columns:
+    if 'volume' not in df.columns:
         return 0.0
-    avg_vol = df["volume"].tail(30).mean()
+    avg_vol = df['volume'].tail(30).mean()
     try:
         req = StockLatestQuoteRequest(symbol_or_symbols=[symbol])
         quote: Quote = ctx.data_client.get_stock_latest_quote(req)
-        spread = (
-            (quote.ask_price - quote.bid_price)
-            if quote.ask_price and quote.bid_price
-            else 0.0
-        )
+        spread = quote.ask_price - quote.bid_price if quote.ask_price and quote.bid_price else 0.0
     except APIError as e:
-        logger.warning(f"[liquidity_factor] Alpaca quote failed for {symbol}: {e}")
+        logger.warning(f'[liquidity_factor] Alpaca quote failed for {symbol}: {e}')
         spread = 0.0
     except Exception:
         spread = 0.0
     vol_score = min(1.0, avg_vol / ctx.volume_threshold) if avg_vol else 0.0
-
-    # AI-AGENT-REF: More reasonable spread scoring to reduce excessive retries
-    # Dynamic spread threshold based on volume - high volume stocks can handle wider spreads
     base_spread_threshold = 0.05
-    volume_adjusted_threshold = base_spread_threshold * (
-        1 + min(1.0, avg_vol / 1000000)
-    )
-    spread_score = max(
-        0.2, 1 - spread / volume_adjusted_threshold
-    )  # Min 0.2 instead of 0.0
-
-    # Combine scores with less aggressive penalization
-    final_score = (vol_score * 0.7) + (
-        spread_score * 0.3
-    )  # Weight volume more than spread
-
-    return max(0.1, min(1.0, final_score))  # Min 0.1 to avoid complete blocking
-
-
-def fractional_kelly_size(
-    ctx: BotContext,
-    balance: float,
-    price: float,
-    atr: float,
-    win_prob: float,
-    payoff_ratio: float = 1.5,
-) -> int:
+    volume_adjusted_threshold = base_spread_threshold * (1 + min(1.0, avg_vol / 1000000))
+    spread_score = max(0.2, 1 - spread / volume_adjusted_threshold)
+    final_score = vol_score * 0.7 + spread_score * 0.3
+    return max(0.1, min(1.0, final_score))
+
+def fractional_kelly_size(ctx: BotContext, balance: float, price: float, atr: float, win_prob: float, payoff_ratio: float=1.5) -> int:
     """Calculate position size using fractional Kelly criterion with comprehensive validation."""
-    # AI-AGENT-REF: Add comprehensive input validation for Kelly calculation
-    try:
-        # Validate inputs
+    try:
         if not isinstance(balance, int | float) or balance <= 0:
-            logger.error("Invalid balance for Kelly calculation: %s", balance)
+            logger.error('Invalid balance for Kelly calculation: %s', balance)
             return 0
-
         if not isinstance(price, int | float) or price <= 0:
-            logger.error("Invalid price for Kelly calculation: %s", price)
+            logger.error('Invalid price for Kelly calculation: %s', price)
             return 0
-
         if not isinstance(atr, int | float) or atr < 0:
-            logger.warning(
-                "Invalid ATR for Kelly calculation: %s, using minimum position", atr
-            )
+            logger.warning('Invalid ATR for Kelly calculation: %s, using minimum position', atr)
             return 1
-
-        # AI-AGENT-REF: Normalize confidence values to valid probability range
         if not isinstance(win_prob, int | float):
-            logger.error(
-                "Invalid win probability type for Kelly calculation: %s", win_prob
-            )
+            logger.error('Invalid win probability type for Kelly calculation: %s', win_prob)
             return 0
-
-        # Handle confidence values that exceed 1.0 by normalizing them
         if win_prob > 1.0:
-            logger.debug("Normalizing confidence value %s to probability", win_prob)
-            # Use sigmoid function to map confidence to probability range [0,1]
-            # This preserves the relative ordering while constraining to valid range
+            logger.debug('Normalizing confidence value %s to probability', win_prob)
             win_prob = 1.0 / (1.0 + math.exp(-win_prob + 1.0))
-            logger.debug("Normalized win probability: %s", win_prob)
+            logger.debug('Normalized win probability: %s', win_prob)
         elif win_prob < 0:
-            logger.warning("Negative confidence value %s, using 0.0", win_prob)
+            logger.warning('Negative confidence value %s, using 0.0', win_prob)
             win_prob = 0.0
-
         if not isinstance(payoff_ratio, int | float) or payoff_ratio <= 0:
-            logger.error("Invalid payoff ratio for Kelly calculation: %s", payoff_ratio)
+            logger.error('Invalid payoff ratio for Kelly calculation: %s', payoff_ratio)
             return 0
-
-        # Validate ctx object and its attributes
-        if not hasattr(ctx, "kelly_fraction") or not isinstance(
-            ctx.kelly_fraction, int | float
-        ):
-            logger.error("Invalid kelly_fraction in context")
+        if not hasattr(ctx, 'kelly_fraction') or not isinstance(ctx.kelly_fraction, int | float):
+            logger.error('Invalid kelly_fraction in context')
             return 0
-
-        if not hasattr(ctx, "max_position_dollars") or not isinstance(
-            ctx.max_position_dollars, int | float
-        ):
-            logger.error("Invalid max_position_dollars in context")
+        if not hasattr(ctx, 'max_position_dollars') or not isinstance(ctx.max_position_dollars, int | float):
+            logger.error('Invalid max_position_dollars in context')
             return 0
-
-        # AI-AGENT-REF: adaptive kelly fraction based on historical peak equity
         if os.path.exists(PEAK_EQUITY_FILE):
             try:
-                with open(PEAK_EQUITY_FILE, "r+") as lock:
+                with open(PEAK_EQUITY_FILE, 'r+') as lock:
                     portalocker.lock(lock, portalocker.LOCK_EX)
                     try:
                         try:
                             data = lock.read()
                         except io.UnsupportedOperation:
-                            logger.warning(
-                                "Cannot read peak equity file, using current balance"
-                            )
+                            logger.warning('Cannot read peak equity file, using current balance')
                             return 0
                         prev_peak = float(data) if data else balance
                         if prev_peak <= 0:
-                            logger.warning(
-                                "Invalid peak equity %s, using current balance",
-                                prev_peak,
-                            )
+                            logger.warning('Invalid peak equity %s, using current balance', prev_peak)
                             prev_peak = balance
                     finally:
                         portalocker.unlock(lock)
             except (OSError, ValueError) as e:
-                logger.warning(
-                    "Error reading peak equity file: %s, using current balance", e
-                )
+                logger.warning('Error reading peak equity file: %s, using current balance', e)
                 prev_peak = balance
         else:
             prev_peak = balance
-
         base_frac = ctx.kelly_fraction * ctx.capital_scaler.compression_factor(balance)
-
-        # Validate base_frac
         if not isinstance(base_frac, int | float) or base_frac < 0 or base_frac > 1:
-            logger.error("Invalid base fraction calculated: %s", base_frac)
+            logger.error('Invalid base fraction calculated: %s', base_frac)
             return 0
-
         drawdown = (prev_peak - balance) / prev_peak if prev_peak > 0 else 0
-
-        # Apply drawdown-based risk reduction
-        if drawdown > 0.10:
+        if drawdown > 0.1:
             frac = 0.3
         elif drawdown > 0.05:
             frac = 0.45
         else:
             frac = base_frac
-
-        # Apply volatility-based risk reduction
         try:
             if is_high_vol_thr_spy():
                 frac *= 0.5
         except Exception as e:
-            logger.warning("Error checking SPY volatility: %s", e)
-
+            logger.warning('Error checking SPY volatility: %s', e)
         cap_scale = frac / base_frac if base_frac > 0 else 1.0
-
-        # Calculate Kelly edge with validation
-        # AI-AGENT-REF: Fix division by zero in Kelly criterion calculation
         if payoff_ratio <= 0:
-            logger.warning(
-                "Invalid payoff_ratio %s for Kelly calculation, using zero position",
-                payoff_ratio,
-            )
+            logger.warning('Invalid payoff_ratio %s for Kelly calculation, using zero position', payoff_ratio)
             edge = 0
             kelly = 0
         else:
             edge = win_prob - (1 - win_prob) / payoff_ratio
             kelly = max(edge / payoff_ratio, 0) * frac
-
-        # Validate Kelly fraction is reasonable
         if kelly < 0 or kelly > 1:
-            logger.warning("Kelly fraction %s out of bounds, capping", kelly)
+            logger.warning('Kelly fraction %s out of bounds, capping', kelly)
             kelly = max(0, min(kelly, 1))
-
         dollars_to_risk = kelly * balance
-
         if atr <= 0:
-            logger.warning("ATR is zero or negative, using minimum position size")
+            logger.warning('ATR is zero or negative, using minimum position size')
             try:
                 new_peak = max(balance, prev_peak)
-                with open(PEAK_EQUITY_FILE, "w") as lock:
+                with open(PEAK_EQUITY_FILE, 'w') as lock:
                     portalocker.lock(lock, portalocker.LOCK_EX)
                     try:
                         lock.write(str(new_peak))
                     finally:
                         portalocker.unlock(lock)
             except OSError as e:
-                logger.warning("Error updating peak equity file: %s", e)
+                logger.warning('Error updating peak equity file: %s', e)
             return 1
-
-        # Calculate position sizes with multiple caps
         raw_pos = dollars_to_risk / atr if atr > 0 else 0
-        cap_pos = (balance * CAPITAL_CAP * cap_scale) / price if price > 0 else 0
-        risk_cap = (balance * DOLLAR_RISK_LIMIT) / atr if atr > 0 else raw_pos
+        cap_pos = balance * CAPITAL_CAP * cap_scale / price if price > 0 else 0
+        risk_cap = balance * DOLLAR_RISK_LIMIT / atr if atr > 0 else raw_pos
         dollar_cap = ctx.max_position_dollars / price if price > 0 else raw_pos
-
-        # Apply all limits
-        size = int(
-            round(min(raw_pos, cap_pos, risk_cap, dollar_cap, MAX_POSITION_SIZE))
-        )
-        size = max(size, 1)  # Ensure minimum position size
-
-        # Validate final size is reasonable
+        size = int(round(min(raw_pos, cap_pos, risk_cap, dollar_cap, MAX_POSITION_SIZE)))
+        size = max(size, 1)
         if size > MAX_POSITION_SIZE:
-            logger.warning("Position size %s exceeds maximum, capping", size)
+            logger.warning('Position size %s exceeds maximum, capping', size)
             size = MAX_POSITION_SIZE
-
-        # Update peak equity
         try:
             new_peak = max(balance, prev_peak)
-            with open(PEAK_EQUITY_FILE, "w") as lock:
+            with open(PEAK_EQUITY_FILE, 'w') as lock:
                 portalocker.lock(lock, portalocker.LOCK_EX)
                 try:
                     lock.write(str(new_peak))
                 finally:
                     portalocker.unlock(lock)
         except OSError as e:
-            logger.warning("Error updating peak equity file: %s", e)
-
-        logger.debug(
-            "Kelly calculation: balance=%s, price=%s, atr=%s, win_prob=%s, size=%s",
-            balance,
-            price,
-            atr,
-            win_prob,
-            size,
-        )
-
+            logger.warning('Error updating peak equity file: %s', e)
+        logger.debug('Kelly calculation: balance=%s, price=%s, atr=%s, win_prob=%s, size=%s', balance, price, atr, win_prob, size)
         return size
-
     except Exception as e:
-        logger.error("Error in Kelly calculation: %s", e)
+        logger.error('Error in Kelly calculation: %s', e)
         return 0
-
     return size
 
-
-def vol_target_position_size(
-    cash: float, price: float, returns: np.ndarray, target_vol: float = 0.02
-) -> int:
+def vol_target_position_size(cash: float, price: float, returns: np.ndarray, target_vol: float=0.02) -> int:
     sigma = np.std(returns)
     if sigma <= 0 or price <= 0:
         return 1
     dollar_alloc = cash * (target_vol / sigma)
     qty = int(round(dollar_alloc / price))
     return max(qty, 1)
-
 
 def compute_kelly_scale(vol: float, sentiment: float) -> float:
     """Return basic Kelly scaling factor."""
@@ -5196,55 +3213,36 @@         base *= 0.5
     return max(base, 0.1)
 
-
 def adjust_position_size(position, scale: float) -> None:
     """Placeholder for adjusting position quantity."""
     try:
         position.qty = str(int(int(position.qty) * scale))
     except Exception:
-        logger.debug("adjust_position_size no-op")
-
+        logger.debug('adjust_position_size no-op')
 
 def adjust_trailing_stop(position, new_stop: float) -> None:
     """Placeholder for adjusting trailing stop price."""
-    logger.debug("adjust_trailing_stop %s -> %.2f", position.symbol, new_stop)
-
-
-@retry(
-    stop=stop_after_attempt(3),
-    wait=wait_exponential(multiplier=1, min=1, max=10),
-    retry=retry_if_exception_type(APIError),
-)
+    logger.debug('adjust_trailing_stop %s -> %.2f', position.symbol, new_stop)
+
+@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10), retry=retry_if_exception_type(APIError))
 def submit_order(ctx: BotContext, symbol: str, qty: int, side: str) -> Order | None:
     """Submit an order using the institutional execution engine."""
     if not market_is_open():
-        logger.warning("MARKET_CLOSED_ORDER_SKIP", extra={"symbol": symbol})
+        logger.warning('MARKET_CLOSED_ORDER_SKIP', extra={'symbol': symbol})
         return None
-
-    # AI-AGENT-REF: Add validation for execution engine initialization
     if _exec_engine is None:
-        logger.error(
-            "EXEC_ENGINE_NOT_INITIALIZED",
-            extra={"symbol": symbol, "qty": qty, "side": side},
-        )
-        raise RuntimeError("Execution engine not initialized. Cannot execute orders.")
-
+        logger.error('EXEC_ENGINE_NOT_INITIALIZED', extra={'symbol': symbol, 'qty': qty, 'side': side})
+        raise RuntimeError('Execution engine not initialized. Cannot execute orders.')
     try:
         return _exec_engine.execute_order(symbol, qty, side)
     except Exception as e:
-        logger.error(
-            "ORDER_EXECUTION_FAILED",
-            extra={"symbol": symbol, "qty": qty, "side": side, "error": str(e)},
-        )
+        logger.error('ORDER_EXECUTION_FAILED', extra={'symbol': symbol, 'qty': qty, 'side': side, 'error': str(e)})
         raise
-
 
 def safe_submit_order(api: TradingClient, req) -> Order | None:
     config.reload_env()
     if not market_is_open():
-        logger.warning(
-            "MARKET_CLOSED_ORDER_SKIP", extra={"symbol": getattr(req, "symbol", "")}
-        )
+        logger.warning('MARKET_CLOSED_ORDER_SKIP', extra={'symbol': getattr(req, 'symbol', '')})
         return None
     for attempt in range(2):
         try:
@@ -5252,282 +3250,168 @@                 acct = api.get_account()
             except Exception:
                 acct = None
-            if acct and getattr(req, "side", "").lower() == "buy":
-                price = getattr(req, "limit_price", None)
+            if acct and getattr(req, 'side', '').lower() == 'buy':
+                price = getattr(req, 'limit_price', None)
                 if not price:
-                    price = getattr(req, "notional", 0)
-                need = float(price or 0) * float(getattr(req, "qty", 0))
-                if need > float(getattr(acct, "buying_power", 0)):
-                    logger.warning(
-                        "insufficient buying power for %s: requested %s, available %s",
-                        req.symbol,
-                        req.qty,
-                        acct.buying_power,
-                    )
+                    price = getattr(req, 'notional', 0)
+                need = float(price or 0) * float(getattr(req, 'qty', 0))
+                if need > float(getattr(acct, 'buying_power', 0)):
+                    logger.warning('insufficient buying power for %s: requested %s, available %s', req.symbol, req.qty, acct.buying_power)
                     return None
-            if getattr(req, "side", "").lower() == "sell":
+            if getattr(req, 'side', '').lower() == 'sell':
                 try:
                     positions = api.get_all_positions()
                 except Exception:
                     positions = []
-                avail = next(
-                    (float(p.qty) for p in positions if p.symbol == req.symbol), 0.0
-                )
-                if float(getattr(req, "qty", 0)) > avail:
-                    logger.warning(
-                        f"insufficient qty available for {req.symbol}: requested {req.qty}, available {avail}"
-                    )
+                avail = next((float(p.qty) for p in positions if p.symbol == req.symbol), 0.0)
+                if float(getattr(req, 'qty', 0)) > avail:
+                    logger.warning(f'insufficient qty available for {req.symbol}: requested {req.qty}, available {avail}')
                     return None
-
             try:
                 order = api.submit_order(order_data=req)
             except APIError as e:
-                if getattr(e, "code", None) == 40310000:
-                    available = int(
-                        getattr(e, "_raw_errors", [{}])[0].get("available", 0)
-                    )
+                if getattr(e, 'code', None) == 40310000:
+                    available = int(getattr(e, '_raw_errors', [{}])[0].get('available', 0))
                     if available > 0:
-                        logger.info(
-                            f"Adjusting order for {req.symbol} to available qty={available}"
-                        )
+                        logger.info(f'Adjusting order for {req.symbol} to available qty={available}')
                         if isinstance(req, dict):
-                            req["qty"] = available
+                            req['qty'] = available
                         else:
                             req.qty = available
                         order = api.submit_order(order_data=req)
                     else:
-                        logger.warning(f"Skipping {req.symbol}, no available qty")
+                        logger.warning(f'Skipping {req.symbol}, no available qty')
                         continue
                 else:
                     raise
-
             start_ts = time.monotonic()
-            while getattr(order, "status", None) == OrderStatus.PENDING_NEW:
+            while getattr(order, 'status', None) == OrderStatus.PENDING_NEW:
                 if time.monotonic() - start_ts > 1:
-                    logger.warning(
-                        f"Order stuck in PENDING_NEW: {req.symbol}, retrying or monitoring required."
-                    )
+                    logger.warning(f'Order stuck in PENDING_NEW: {req.symbol}, retrying or monitoring required.')
                     break
-                time.sleep(0.1)  # AI-AGENT-REF: avoid busy polling
+                time.sleep(0.1)
                 order = api.get_order_by_id(order.id)
-            logger.info(
-                f"Order status for {req.symbol}: {getattr(order, 'status', '')}"
-            )
-            status = getattr(order, "status", "")
-            filled_qty = getattr(order, "filled_qty", "0")
-            if status == "filled":
-                logger.info(
-                    "ORDER_ACK",
-                    extra={"symbol": req.symbol, "order_id": getattr(order, "id", "")},
-                )
-            elif status == "partially_filled":
-                logger.warning(
-                    f"Order partially filled for {req.symbol}: {filled_qty}/{getattr(req, 'qty', 0)}"
-                )
-            elif status in ("rejected", "canceled"):
-                logger.error(
-                    f"Order for {req.symbol} was {status}: {getattr(order, 'reject_reason', '')}"
-                )
-                raise OrderExecutionError(f"Buy failed for {req.symbol}: {status}")
+            logger.info(f"Order status for {req.symbol}: {getattr(order, 'status', '')}")
+            status = getattr(order, 'status', '')
+            filled_qty = getattr(order, 'filled_qty', '0')
+            if status == 'filled':
+                logger.info('ORDER_ACK', extra={'symbol': req.symbol, 'order_id': getattr(order, 'id', '')})
+            elif status == 'partially_filled':
+                logger.warning(f"Order partially filled for {req.symbol}: {filled_qty}/{getattr(req, 'qty', 0)}")
+            elif status in ('rejected', 'canceled'):
+                logger.error(f"Order for {req.symbol} was {status}: {getattr(order, 'reject_reason', '')}")
+                raise OrderExecutionError(f'Buy failed for {req.symbol}: {status}')
             elif status == OrderStatus.NEW:
-                logger.info(f"Order for {req.symbol} is NEW; awaiting fill")
+                logger.info(f'Order for {req.symbol} is NEW; awaiting fill')
             else:
-                logger.error(
-                    f"Order for {req.symbol} status={status}: {getattr(order, 'reject_reason', '')}"
-                )
+                logger.error(f"Order for {req.symbol} status={status}: {getattr(order, 'reject_reason', '')}")
             return order
         except APIError as e:
-            if "insufficient qty" in str(e).lower():
-                logger.warning(f"insufficient qty available for {req.symbol}: {e}")
+            if 'insufficient qty' in str(e).lower():
+                logger.warning(f'insufficient qty available for {req.symbol}: {e}')
                 return None
             time.sleep(1)
             if attempt == 1:
-                logger.warning(f"submit_order failed for {req.symbol}: {e}")
+                logger.warning(f'submit_order failed for {req.symbol}: {e}')
                 return None
         except Exception as e:
             time.sleep(1)
             if attempt == 1:
-                logger.warning(f"submit_order failed for {req.symbol}: {e}")
+                logger.warning(f'submit_order failed for {req.symbol}: {e}')
                 return None
     return None
 
-
-def poll_order_fill_status(ctx: BotContext, order_id: str, timeout: int = 120) -> None:
+def poll_order_fill_status(ctx: BotContext, order_id: str, timeout: int=120) -> None:
     """Poll Alpaca for order fill status until it is no longer open."""
     start = pytime.time()
     while pytime.time() - start < timeout:
         try:
             od = ctx.api.get_order_by_id(order_id)
-            status = getattr(od, "status", "")
-            filled = getattr(od, "filled_qty", "0")
-            if status not in {"new", "accepted", "partially_filled"}:
-                logger.info(
-                    "ORDER_FINAL_STATUS",
-                    extra={
-                        "order_id": order_id,
-                        "status": status,
-                        "filled_qty": filled,
-                    },
-                )
+            status = getattr(od, 'status', '')
+            filled = getattr(od, 'filled_qty', '0')
+            if status not in {'new', 'accepted', 'partially_filled'}:
+                logger.info('ORDER_FINAL_STATUS', extra={'order_id': order_id, 'status': status, 'filled_qty': filled})
                 return
         except Exception as e:
-            logger.warning(f"[poll_order_fill_status] failed for {order_id}: {e}")
+            logger.warning(f'[poll_order_fill_status] failed for {order_id}: {e}')
             return
         pytime.sleep(3)
 
-
-def send_exit_order(
-    ctx: BotContext,
-    symbol: str,
-    exit_qty: int,
-    price: float,
-    reason: str,
-    raw_positions: list | None = None,
-) -> None:
-    logger.info(
-        f"EXIT_SIGNAL | symbol={symbol}  reason={reason}  exit_qty={exit_qty}  price={price}"
-    )
-    if raw_positions is not None and not any(
-        getattr(p, "symbol", "") == symbol for p in raw_positions
-    ):
-        logger.info("SKIP_NO_POSITION", extra={"symbol": symbol})
+def send_exit_order(ctx: BotContext, symbol: str, exit_qty: int, price: float, reason: str, raw_positions: list | None=None) -> None:
+    logger.info(f'EXIT_SIGNAL | symbol={symbol}  reason={reason}  exit_qty={exit_qty}  price={price}')
+    if raw_positions is not None and (not any((getattr(p, 'symbol', '') == symbol for p in raw_positions))):
+        logger.info('SKIP_NO_POSITION', extra={'symbol': symbol})
         return
     try:
         pos = ctx.api.get_open_position(symbol)
         held_qty = int(pos.qty)
     except Exception:
         held_qty = 0
-
     if held_qty < exit_qty:
-        logger.warning(
-            f"No shares available to exit for {symbol} (requested {exit_qty}, have {held_qty})"
-        )
+        logger.warning(f'No shares available to exit for {symbol} (requested {exit_qty}, have {held_qty})')
         return
-
     if price <= 0.0:
-        req = MarketOrderRequest(
-            symbol=symbol,
-            qty=exit_qty,
-            side=OrderSide.SELL,
-            time_in_force=TimeInForce.DAY,
-        )
+        req = MarketOrderRequest(symbol=symbol, qty=exit_qty, side=OrderSide.SELL, time_in_force=TimeInForce.DAY)
         order = safe_submit_order(ctx.api, req)
         if order is not None:
             from ai_trading.strategies.base import StrategySignal as TradeSignal
-
             try:
                 acct = ctx.api.get_account()
-                eq = float(getattr(acct, "equity", 0) or 0)
-                wt = (exit_qty * price) / eq if eq > 0 else 0.0
-                ctx.risk_engine.register_fill(
-                    TradeSignal(
-                        symbol=symbol,
-                        side="sell",
-                        confidence=1.0,
-                        strategy="exit",
-                        weight=abs(wt),
-                        asset_class="equity",
-                    )
-                )
+                eq = float(getattr(acct, 'equity', 0) or 0)
+                wt = exit_qty * price / eq if eq > 0 else 0.0
+                ctx.risk_engine.register_fill(TradeSignal(symbol=symbol, side='sell', confidence=1.0, strategy='exit', weight=abs(wt), asset_class='equity'))
             except Exception:
-                logger.debug("register_fill exit failed", exc_info=True)
+                logger.debug('register_fill exit failed', exc_info=True)
         return
-
-    limit_order = safe_submit_order(
-        ctx.api,
-        LimitOrderRequest(
-            symbol=symbol,
-            qty=exit_qty,
-            side=OrderSide.SELL,
-            time_in_force=TimeInForce.DAY,
-            limit_price=price,
-        ),
-    )
+    limit_order = safe_submit_order(ctx.api, LimitOrderRequest(symbol=symbol, qty=exit_qty, side=OrderSide.SELL, time_in_force=TimeInForce.DAY, limit_price=price))
     if limit_order is not None:
         from ai_trading.strategies.base import StrategySignal as TradeSignal
-
         try:
             acct = ctx.api.get_account()
-            eq = float(getattr(acct, "equity", 0) or 0)
-            wt = (exit_qty * price) / eq if eq > 0 else 0.0
-            ctx.risk_engine.register_fill(
-                TradeSignal(
-                    symbol=symbol,
-                    side="sell",
-                    confidence=1.0,
-                    strategy="exit",
-                    weight=abs(wt),
-                    asset_class="equity",
-                )
-            )
+            eq = float(getattr(acct, 'equity', 0) or 0)
+            wt = exit_qty * price / eq if eq > 0 else 0.0
+            ctx.risk_engine.register_fill(TradeSignal(symbol=symbol, side='sell', confidence=1.0, strategy='exit', weight=abs(wt), asset_class='equity'))
         except Exception:
-            logger.debug("register_fill exit failed", exc_info=True)
+            logger.debug('register_fill exit failed', exc_info=True)
     pytime.sleep(5)
     try:
         o2 = ctx.api.get_order_by_id(limit_order.id)
-        if getattr(o2, "status", "") in {"new", "accepted", "partially_filled"}:
+        if getattr(o2, 'status', '') in {'new', 'accepted', 'partially_filled'}:
             ctx.api.cancel_order_by_id(limit_order.id)
-            safe_submit_order(
-                ctx.api,
-                MarketOrderRequest(
-                    symbol=symbol,
-                    qty=exit_qty,
-                    side=OrderSide.SELL,
-                    time_in_force=TimeInForce.DAY,
-                ),
-            )
+            safe_submit_order(ctx.api, MarketOrderRequest(symbol=symbol, qty=exit_qty, side=OrderSide.SELL, time_in_force=TimeInForce.DAY))
     except Exception as e:
-        logger.warning(
-            f"[send_exit_order] couldn\u2019t check/cancel order {getattr(limit_order, 'id', '')}: {e}"
-        )
-
-
-def twap_submit(
-    ctx: BotContext,
-    symbol: str,
-    total_qty: int,
-    side: str,
-    window_secs: int = 600,
-    n_slices: int = 10,
-) -> None:
+        logger.warning(f"[send_exit_order] couldn’t check/cancel order {getattr(limit_order, 'id', '')}: {e}")
+
+def twap_submit(ctx: BotContext, symbol: str, total_qty: int, side: str, window_secs: int=600, n_slices: int=10) -> None:
     slice_qty = total_qty // n_slices
     wait_secs = window_secs / n_slices
     for i in range(n_slices):
         try:
             submit_order(ctx, symbol, slice_qty, side)
         except Exception as e:
-            logger.exception(f"[TWAP] slice {i+1}/{n_slices} failed: {e}")
+            logger.exception(f'[TWAP] slice {i + 1}/{n_slices} failed: {e}')
             break
         pytime.sleep(wait_secs)
 
-
-def vwap_pegged_submit(
-    ctx: BotContext, symbol: str, total_qty: int, side: str, duration: int = 300
-) -> None:
+def vwap_pegged_submit(ctx: BotContext, symbol: str, total_qty: int, side: str, duration: int=300) -> None:
     start_time = pytime.time()
     placed = 0
     while placed < total_qty and pytime.time() - start_time < duration:
         try:
             df = fetch_minute_df_safe(symbol)
         except DataFetchError:
-            logger.error("[VWAP] no minute data for %s", symbol)
+            logger.error('[VWAP] no minute data for %s', symbol)
             break
         if df is None or df.empty:
-            logger.warning(
-                "[VWAP] missing bars, aborting VWAP slice", extra={"symbol": symbol}
-            )
+            logger.warning('[VWAP] missing bars, aborting VWAP slice', extra={'symbol': symbol})
             break
-        vwap_price = ta.vwap(df["high"], df["low"], df["close"], df["volume"]).iloc[-1]
+        vwap_price = ta.vwap(df['high'], df['low'], df['close'], df['volume']).iloc[-1]
         try:
             req = StockLatestQuoteRequest(symbol_or_symbols=[symbol])
             quote: Quote = ctx.data_client.get_stock_latest_quote(req)
-            spread = (
-                (quote.ask_price - quote.bid_price)
-                if quote.ask_price and quote.bid_price
-                else 0.0
-            )
+            spread = quote.ask_price - quote.bid_price if quote.ask_price and quote.bid_price else 0.0
         except APIError as e:
-            logger.warning(f"[vwap_slice] Alpaca quote failed for {symbol}: {e}")
+            logger.warning(f'[vwap_slice] Alpaca quote failed for {symbol}: {e}')
             spread = 0.0
         except Exception:
             spread = 0.0
@@ -5538,94 +3422,49 @@         order = None
         for attempt in range(3):
             try:
-                logger.info(
-                    "ORDER_SENT",
-                    extra={
-                        "timestamp": utc_now_iso(),
-                        "symbol": symbol,
-                        "side": side,
-                        "qty": slice_qty,
-                        "order_type": "limit",
-                    },
-                )
-                order = safe_submit_order(
-                    ctx.api,
-                    LimitOrderRequest(
-                        symbol=symbol,
-                        qty=slice_qty,
-                        side=OrderSide.BUY if side == "buy" else OrderSide.SELL,
-                        time_in_force=TimeInForce.IOC,
-                        limit_price=round(vwap_price, 2),
-                    ),
-                )
-                logger.info(
-                    "ORDER_ACK",
-                    extra={
-                        "symbol": symbol,
-                        "order_id": getattr(order, "id", ""),
-                        "status": getattr(order, "status", ""),
-                    },
-                )
-                Thread(
-                    target=poll_order_fill_status,
-                    args=(ctx, getattr(order, "id", "")),
-                    daemon=True,
-                ).start()
-                fill_price = float(getattr(order, "filled_avg_price", 0) or 0)
+                logger.info('ORDER_SENT', extra={'timestamp': utc_now_iso(), 'symbol': symbol, 'side': side, 'qty': slice_qty, 'order_type': 'limit'})
+                order = safe_submit_order(ctx.api, LimitOrderRequest(symbol=symbol, qty=slice_qty, side=OrderSide.BUY if side == 'buy' else OrderSide.SELL, time_in_force=TimeInForce.IOC, limit_price=round(vwap_price, 2)))
+                logger.info('ORDER_ACK', extra={'symbol': symbol, 'order_id': getattr(order, 'id', ''), 'status': getattr(order, 'status', '')})
+                Thread(target=poll_order_fill_status, args=(ctx, getattr(order, 'id', '')), daemon=True).start()
+                fill_price = float(getattr(order, 'filled_avg_price', 0) or 0)
                 if fill_price > 0:
                     slip = (fill_price - vwap_price) * 100
                     if slippage_total:
                         try:
                             slippage_total.inc(abs(slip))
                         except Exception as exc:
-                            logger.exception("bot.py unexpected", exc_info=exc)
+                            logger.exception('bot.py unexpected', exc_info=exc)
                             raise
                     if slippage_count:
                         try:
                             slippage_count.inc()
                         except Exception as exc:
-                            logger.exception("bot.py unexpected", exc_info=exc)
+                            logger.exception('bot.py unexpected', exc_info=exc)
                             raise
-                    _slippage_log.append(
-                        (
-                            symbol,
-                            vwap_price,
-                            fill_price,
-                            datetime.now(UTC),
-                        )
-                    )
+                    _slippage_log.append((symbol, vwap_price, fill_price, datetime.now(UTC)))
                     with slippage_lock:
                         try:
-                            with open(SLIPPAGE_LOG_FILE, "a", newline="") as sf:
-                                csv.writer(sf).writerow(
-                                    [
-                                        utc_now_iso(),
-                                        symbol,
-                                        vwap_price,
-                                        fill_price,
-                                        slip,
-                                    ]
-                                )
+                            with open(SLIPPAGE_LOG_FILE, 'a', newline='') as sf:
+                                csv.writer(sf).writerow([utc_now_iso(), symbol, vwap_price, fill_price, slip])
                         except Exception as e:
-                            logger.warning(f"Failed to append slippage log: {e}")
+                            logger.warning(f'Failed to append slippage log: {e}')
                 if orders_total:
                     try:
                         orders_total.inc()
                     except Exception as exc:
-                        logger.exception("bot.py unexpected", exc_info=exc)
+                        logger.exception('bot.py unexpected', exc_info=exc)
                         raise
                 break
             except APIError as e:
-                logger.warning(f"[VWAP] APIError attempt {attempt+1} for {symbol}: {e}")
+                logger.warning(f'[VWAP] APIError attempt {attempt + 1} for {symbol}: {e}')
                 pytime.sleep(attempt + 1)
             except Exception as e:
-                logger.exception(f"[VWAP] slice attempt {attempt+1} failed: {e}")
+                logger.exception(f'[VWAP] slice attempt {attempt + 1} failed: {e}')
                 pytime.sleep(attempt + 1)
         if order is None:
             break
         placed += slice_qty
         pytime.sleep(duration / 10)
-
 
 @dataclass(frozen=True)
 class SliceConfig:
@@ -5634,24 +3473,9 @@     max_retries: int = 3
     backoff_factor: float = 2.0
     max_backoff_interval: int = 300
-
-
-DEFAULT_SLICE_CFG = SliceConfig(
-    pct=POV_SLICE_PCT,
-    sleep_interval=60,
-    max_retries=3,
-    backoff_factor=2.0,
-    max_backoff_interval=300,
-)
-
-
-def pov_submit(
-    ctx: BotContext,
-    symbol: str,
-    total_qty: int,
-    side: str,
-    cfg: SliceConfig = DEFAULT_SLICE_CFG,
-) -> bool:
+DEFAULT_SLICE_CFG = SliceConfig(pct=POV_SLICE_PCT, sleep_interval=60, max_retries=3, backoff_factor=2.0, max_backoff_interval=300)
+
+def pov_submit(ctx: BotContext, symbol: str, total_qty: int, side: str, cfg: SliceConfig=DEFAULT_SLICE_CFG) -> bool:
     placed = 0
     retries = 0
     interval = cfg.sleep_interval
@@ -5661,15 +3485,9 @@         except DataFetchError:
             retries += 1
             if retries > cfg.max_retries:
-                logger.warning(
-                    f"[pov_submit] no minute data after {cfg.max_retries} retries, aborting",
-                    extra={"symbol": symbol},
-                )
+                logger.warning(f'[pov_submit] no minute data after {cfg.max_retries} retries, aborting', extra={'symbol': symbol})
                 return False
-            logger.warning(
-                f"[pov_submit] missing bars, retry {retries}/{cfg.max_retries} in {interval:.1f}s",
-                extra={"symbol": symbol},
-            )
+            logger.warning(f'[pov_submit] missing bars, retry {retries}/{cfg.max_retries} in {interval:.1f}s', extra={'symbol': symbol})
             sleep_time = interval * (0.8 + 0.4 * random.random())
             pytime.sleep(sleep_time)
             interval = min(interval * cfg.backoff_factor, cfg.max_backoff_interval)
@@ -5677,475 +3495,299 @@         if df is None or df.empty:
             retries += 1
             if retries > cfg.max_retries:
-                logger.warning(
-                    f"[pov_submit] no minute data after {cfg.max_retries} retries, aborting",
-                    extra={"symbol": symbol},
-                )
+                logger.warning(f'[pov_submit] no minute data after {cfg.max_retries} retries, aborting', extra={'symbol': symbol})
                 return False
-            logger.warning(
-                f"[pov_submit] missing bars, retry {retries}/{cfg.max_retries} in {interval:.1f}s",
-                extra={"symbol": symbol},
-            )
+            logger.warning(f'[pov_submit] missing bars, retry {retries}/{cfg.max_retries} in {interval:.1f}s', extra={'symbol': symbol})
             sleep_time = interval * (0.8 + 0.4 * random.random())
             pytime.sleep(sleep_time)
             interval = min(interval * cfg.backoff_factor, cfg.max_backoff_interval)
             continue
         retries = 0
         interval = cfg.sleep_interval
-
         try:
             req = StockLatestQuoteRequest(symbol_or_symbols=[symbol])
             quote: Quote = ctx.data_client.get_stock_latest_quote(req)
-            spread = (
-                (quote.ask_price - quote.bid_price)
-                if quote.ask_price and quote.bid_price
-                else 0.0
-            )
+            spread = quote.ask_price - quote.bid_price if quote.ask_price and quote.bid_price else 0.0
         except APIError as e:
-            logger.warning(f"[pov_submit] Alpaca quote failed for {symbol}: {e}")
+            logger.warning(f'[pov_submit] Alpaca quote failed for {symbol}: {e}')
             spread = 0.0
         except Exception:
             spread = 0.0
-
-        vol = df["volume"].iloc[-1]
-
-        # AI-AGENT-REF: Dynamic spread threshold based on market conditions
-        # Instead of fixed 0.05, use dynamic threshold based on symbol characteristics
-        dynamic_spread_threshold = min(0.10, max(0.02, vol / 1000000 * 0.05))
-
+        vol = df['volume'].iloc[-1]
+        dynamic_spread_threshold = min(0.1, max(0.02, vol / 1000000 * 0.05))
         if spread > dynamic_spread_threshold:
-            # Less aggressive reduction - only 25% instead of 50%
             slice_qty = min(int(vol * cfg.pct * 0.75), total_qty - placed)
-            logger.debug(
-                "[pov_submit] High spread detected, reducing slice by 25%",
-                extra={
-                    "symbol": symbol,
-                    "spread": spread,
-                    "threshold": dynamic_spread_threshold,
-                    "reduced_slice_qty": slice_qty,
-                },
-            )
+            logger.debug('[pov_submit] High spread detected, reducing slice by 25%', extra={'symbol': symbol, 'spread': spread, 'threshold': dynamic_spread_threshold, 'reduced_slice_qty': slice_qty})
         else:
             slice_qty = min(int(vol * cfg.pct), total_qty - placed)
-
         if slice_qty < 1:
-            logger.debug(
-                f"[pov_submit] slice_qty<1 (vol={vol}), waiting",
-                extra={"symbol": symbol},
-            )
+            logger.debug(f'[pov_submit] slice_qty<1 (vol={vol}), waiting', extra={'symbol': symbol})
             pytime.sleep(cfg.sleep_interval * (0.8 + 0.4 * random.random()))
             continue
         try:
-            # AI-AGENT-REF: Fix order slicing to track actual filled quantities
             order = submit_order(ctx, symbol, slice_qty, side)
             if order is None:
-                logger.warning(
-                    "[pov_submit] submit_order returned None for slice, skipping",
-                    extra={"symbol": symbol, "slice_qty": slice_qty},
-                )
+                logger.warning('[pov_submit] submit_order returned None for slice, skipping', extra={'symbol': symbol, 'slice_qty': slice_qty})
                 continue
-
-            # Track actual filled quantity, not intended quantity
-            actual_filled = int(getattr(order, "filled_qty", "0") or "0")
-
-            # For partially filled orders, the filled_qty might be less than slice_qty
+            actual_filled = int(getattr(order, 'filled_qty', '0') or '0')
             if actual_filled < slice_qty:
-                logger.warning(
-                    "POV_SLICE_PARTIAL_FILL",
-                    extra={
-                        "symbol": symbol,
-                        "intended_qty": slice_qty,
-                        "actual_filled": actual_filled,
-                        "order_id": getattr(order, "id", ""),
-                        "status": getattr(order, "status", ""),
-                    },
-                )
-
-            placed += actual_filled  # Use actual filled, not intended
-
+                logger.warning('POV_SLICE_PARTIAL_FILL', extra={'symbol': symbol, 'intended_qty': slice_qty, 'actual_filled': actual_filled, 'order_id': getattr(order, 'id', ''), 'status': getattr(order, 'status', '')})
+            placed += actual_filled
         except Exception as e:
-            logger.exception(
-                f"[pov_submit] submit_order failed on slice, aborting: {e}",
-                extra={"symbol": symbol},
-            )
+            logger.exception(f'[pov_submit] submit_order failed on slice, aborting: {e}', extra={'symbol': symbol})
             return False
-
-        logger.info(
-            "POV_SLICE_PLACED",
-            extra={
-                "symbol": symbol,
-                "slice_qty": slice_qty,
-                "actual_filled": (
-                    actual_filled if "actual_filled" in locals() else slice_qty
-                ),
-                "total_placed": placed,
-            },
-        )
+        logger.info('POV_SLICE_PLACED', extra={'symbol': symbol, 'slice_qty': slice_qty, 'actual_filled': actual_filled if 'actual_filled' in locals() else slice_qty, 'total_placed': placed})
         pytime.sleep(cfg.sleep_interval * (0.8 + 0.4 * random.random()))
-    logger.info("POV_SUBMIT_COMPLETE", extra={"symbol": symbol, "placed": placed})
+    logger.info('POV_SUBMIT_COMPLETE', extra={'symbol': symbol, 'placed': placed})
     return True
 
-
-def maybe_pyramid(
-    ctx: BotContext,
-    symbol: str,
-    entry_price: float,
-    current_price: float,
-    atr: float,
-    prob: float,
-):
+def maybe_pyramid(ctx: BotContext, symbol: str, entry_price: float, current_price: float, atr: float, prob: float):
     """Add to a winning position when probability remains high."""
-    profit = (current_price - entry_price) if entry_price else 0
+    profit = current_price - entry_price if entry_price else 0
     if profit > 2 * atr and prob >= 0.75:
         try:
             pos = ctx.api.get_open_position(symbol)
             qty = int(abs(int(pos.qty)) * 0.5)
             if qty > 0:
-                submit_order(ctx, symbol, qty, "buy")
-                logger.info("PYRAMIDED", extra={"symbol": symbol, "qty": qty})
+                submit_order(ctx, symbol, qty, 'buy')
+                logger.info('PYRAMIDED', extra={'symbol': symbol, 'qty': qty})
         except Exception as e:
-            logger.exception(f"[maybe_pyramid] failed for {symbol}: {e}")
-
-
-def update_trailing_stop(
-    ctx: BotContext,
-    ticker: str,
-    price: float,
-    qty: int,
-    atr: float,
-) -> str:
+            logger.exception(f'[maybe_pyramid] failed for {symbol}: {e}')
+
+def update_trailing_stop(ctx: BotContext, ticker: str, price: float, qty: int, atr: float) -> str:
     factor = 1.0 if is_high_vol_regime() else TRAILING_FACTOR
     te = ctx.trailing_extremes
     if qty > 0:
         with targets_lock:
             te[ticker] = max(te.get(ticker, price), price)
         if price < te[ticker] - factor * atr:
-            return "exit_long"
+            return 'exit_long'
     elif qty < 0:
         with targets_lock:
             te[ticker] = min(te.get(ticker, price), price)
         if price > te[ticker] + factor * atr:
-            return "exit_short"
-    return "hold"
-
-
-def calculate_entry_size(
-    ctx: BotContext, symbol: str, price: float, atr: float, win_prob: float
-) -> int:
+            return 'exit_short'
+    return 'hold'
+
+def calculate_entry_size(ctx: BotContext, symbol: str, price: float, atr: float, win_prob: float) -> int:
     """Calculate entry size based on account balance and risk parameters."""
-
     if ctx.api is None:
-        logger.warning("ctx.api is None - using default entry size")
+        logger.warning('ctx.api is None - using default entry size')
         return 1
-
     try:
         cash = float(ctx.api.get_account().cash)
     except Exception as exc:
-        logger.warning("Failed to get cash for entry size calculation: %s", exc)
+        logger.warning('Failed to get cash for entry size calculation: %s', exc)
         return 1
-
-    cap_pct = ctx.params.get("CAPITAL_CAP", CAPITAL_CAP)
-    cap_sz = int(round((cash * cap_pct) / price)) if price > 0 else 0
+    cap_pct = ctx.params.get('CAPITAL_CAP', CAPITAL_CAP)
+    cap_sz = int(round(cash * cap_pct / price)) if price > 0 else 0
     df = ctx.data_fetcher.get_daily_df(ctx, symbol)
-    rets = (
-        df["close"].pct_change(fill_method=None).dropna().values
-        if df is not None and not df.empty
-        else np.array([0.0])
-    )
+    rets = df['close'].pct_change(fill_method=None).dropna().values if df is not None and (not df.empty) else np.array([0.0])
     kelly_sz = fractional_kelly_size(ctx, cash, price, atr, win_prob)
     vol_sz = vol_target_position_size(cash, price, rets, target_vol=0.02)
     dollar_cap = ctx.max_position_dollars / price if price > 0 else kelly_sz
     base = int(round(min(kelly_sz, vol_sz, cap_sz, dollar_cap, MAX_POSITION_SIZE)))
     factor = max(0.5, min(1.5, 1 + (win_prob - 0.5)))
     liq = liquidity_factor(ctx, symbol)
-    # AI-AGENT-REF: Fix zero quantity from low liquidity - use minimum viable size
     if liq < 0.2:
-        # If we have significant cash, still allow minimum position
         if cash > 5000:
-            logger.info(
-                f"Low liquidity for {symbol} (factor={liq:.3f}), using minimum position size"
-            )
+            logger.info(f'Low liquidity for {symbol} (factor={liq:.3f}), using minimum position size')
             return max(1, int(1000 / price)) if price > 0 else 1
         return 0
     size = int(round(base * factor * liq))
     return max(size, 1)
 
-
 def execute_entry(ctx: BotContext, symbol: str, qty: int, side: str) -> None:
     """Execute entry order."""
-
     if ctx.api is None:
-        logger.warning("ctx.api is None - cannot execute entry")
+        logger.warning('ctx.api is None - cannot execute entry')
         return
-
     try:
         buying_pw = float(ctx.api.get_account().buying_power)
         if buying_pw <= 0:
-            logger.info("NO_BUYING_POWER", extra={"symbol": symbol})
+            logger.info('NO_BUYING_POWER', extra={'symbol': symbol})
             return
     except Exception as exc:
-        logger.warning("Failed to get buying power for %s: %s", symbol, exc)
+        logger.warning('Failed to get buying power for %s: %s', symbol, exc)
         return
-    if qty is None or qty <= 0 or not np.isfinite(qty):
-        logger.error(
-            f"Invalid order quantity for {symbol}: {qty}. Skipping order and logging input data."
-        )
-        # Optionally, log signal, price, and input features here for debug
+    if qty is None or qty <= 0 or (not np.isfinite(qty)):
+        logger.error(f'Invalid order quantity for {symbol}: {qty}. Skipping order and logging input data.')
         return
     if POV_SLICE_PCT > 0 and qty > SLICE_THRESHOLD:
-        logger.info("POV_SLICE_ENTRY", extra={"symbol": symbol, "qty": qty})
+        logger.info('POV_SLICE_ENTRY', extra={'symbol': symbol, 'qty': qty})
         pov_submit(ctx, symbol, qty, side)
     elif qty > SLICE_THRESHOLD:
-        logger.info("VWAP_SLICE_ENTRY", extra={"symbol": symbol, "qty": qty})
+        logger.info('VWAP_SLICE_ENTRY', extra={'symbol': symbol, 'qty': qty})
         vwap_pegged_submit(ctx, symbol, qty, side)
     else:
-        logger.info("MARKET_ENTRY", extra={"symbol": symbol, "qty": qty})
+        logger.info('MARKET_ENTRY', extra={'symbol': symbol, 'qty': qty})
         submit_order(ctx, symbol, qty, side)
-
     try:
         raw = fetch_minute_df_safe(symbol)
     except DataFetchError:
-        logger.warning("NO_MINUTE_BARS_POST_ENTRY", extra={"symbol": symbol})
+        logger.warning('NO_MINUTE_BARS_POST_ENTRY', extra={'symbol': symbol})
         return
     if raw is None or raw.empty:
-        logger.warning("NO_MINUTE_BARS_POST_ENTRY", extra={"symbol": symbol})
+        logger.warning('NO_MINUTE_BARS_POST_ENTRY', extra={'symbol': symbol})
         return
     try:
         df_ind = prepare_indicators(raw)
         if df_ind is None:
-            logger.warning(
-                "INSUFFICIENT_INDICATORS_POST_ENTRY", extra={"symbol": symbol}
-            )
+            logger.warning('INSUFFICIENT_INDICATORS_POST_ENTRY', extra={'symbol': symbol})
             return
     except ValueError as exc:
-        logger.warning(f"Indicator preparation failed for {symbol}: {exc}")
+        logger.warning(f'Indicator preparation failed for {symbol}: {exc}')
         return
     if df_ind.empty:
-        logger.warning("INSUFFICIENT_INDICATORS_POST_ENTRY", extra={"symbol": symbol})
+        logger.warning('INSUFFICIENT_INDICATORS_POST_ENTRY', extra={'symbol': symbol})
         return
     entry_price = get_latest_close(df_ind)
-    ctx.trade_logger.log_entry(symbol, entry_price, qty, side, "", "", confidence=0.5)
-
+    ctx.trade_logger.log_entry(symbol, entry_price, qty, side, '', '', confidence=0.5)
     now_pac = datetime.now(UTC).astimezone(PACIFIC)
     mo = datetime.combine(now_pac.date(), ctx.market_open, PACIFIC)
     mc = datetime.combine(now_pac.date(), ctx.market_close, PACIFIC)
     tp_factor = TAKE_PROFIT_FACTOR * 1.1 if is_high_vol_regime() else TAKE_PROFIT_FACTOR
-    stop, take = scaled_atr_stop(
-        entry_price,
-        df_ind["atr"].iloc[-1],
-        now_pac,
-        mo,
-        mc,
-        max_factor=tp_factor,
-        min_factor=0.5,
-    )
+    stop, take = scaled_atr_stop(entry_price, df_ind['atr'].iloc[-1], now_pac, mo, mc, max_factor=tp_factor, min_factor=0.5)
     with targets_lock:
         ctx.stop_targets[symbol] = stop
         ctx.take_profit_targets[symbol] = take
 
-
 def execute_exit(ctx: BotContext, state: BotState, symbol: str, qty: int) -> None:
     if qty is None or not np.isfinite(qty) or qty <= 0:
-        logger.warning(f"Skipping {symbol}: computed qty <= 0")
+        logger.warning(f'Skipping {symbol}: computed qty <= 0')
         return
     try:
         raw = fetch_minute_df_safe(symbol)
     except DataFetchError:
-        logger.warning("NO_MINUTE_BARS_POST_EXIT", extra={"symbol": symbol})
+        logger.warning('NO_MINUTE_BARS_POST_EXIT', extra={'symbol': symbol})
         raw = pd.DataFrame()
     exit_price = get_latest_close(raw) if raw is not None else 1.0
-    send_exit_order(ctx, symbol, qty, exit_price, "manual_exit")
+    send_exit_order(ctx, symbol, qty, exit_price, 'manual_exit')
     ctx.trade_logger.log_exit(state, symbol, exit_price)
     on_trade_exit_rebalance(ctx)
     with targets_lock:
         ctx.take_profit_targets.pop(symbol, None)
         ctx.stop_targets.pop(symbol, None)
 
-
 def exit_all_positions(ctx: BotContext) -> None:
     raw_positions = ctx.api.get_all_positions()
     for pos in raw_positions:
         qty = abs(int(pos.qty))
         if qty:
-            send_exit_order(
-                ctx, pos.symbol, qty, 0.0, "eod_exit", raw_positions=raw_positions
-            )
-            logger.info("EOD_EXIT", extra={"symbol": pos.symbol, "qty": qty})
-
+            send_exit_order(ctx, pos.symbol, qty, 0.0, 'eod_exit', raw_positions=raw_positions)
+            logger.info('EOD_EXIT', extra={'symbol': pos.symbol, 'qty': qty})
 
 def _liquidate_all_positions(ctx: BotContext) -> None:
     """Helper to liquidate every open position."""
-    # AI-AGENT-REF: existing exit_all_positions wrapper for emergency liquidation
     exit_all_positions(ctx)
-
 
 def liquidate_positions_if_needed(ctx: BotContext) -> None:
     """Liquidate all positions when certain risk conditions trigger."""
     if check_halt_flag(ctx):
-        # Modified: DO NOT liquidate positions on halt flag.
-        logger.info(
-            "TRADING_HALTED_VIA_FLAG is active: NOT liquidating positions, holding open positions."
-        )
+        logger.info('TRADING_HALTED_VIA_FLAG is active: NOT liquidating positions, holding open positions.')
         return
 
-    # normal liquidation logic would go here (placeholder)
-
-
-# ─── L. SIGNAL & TRADE LOGIC ───────────────────────────────────────────────────
-def signal_and_confirm(
-    ctx: BotContext, state: BotState, symbol: str, df: pd.DataFrame, model
-) -> tuple[int, float, str]:
+def signal_and_confirm(ctx: BotContext, state: BotState, symbol: str, df: pd.DataFrame, model) -> tuple[int, float, str]:
     """Wrapper that evaluates signals and checks confidence threshold."""
     sig, conf, strat = ctx.signal_manager.evaluate(ctx, state, df, symbol, model)
     if sig == -1 or conf < CONF_THRESHOLD:
-        logger.debug(
-            "SKIP_LOW_SIGNAL", extra={"symbol": symbol, "sig": sig, "conf": conf}
-        )
-        return -1, 0.0, ""
-    return sig, conf, strat
-
-
-def pre_trade_checks(
-    ctx: BotContext, state: BotState, symbol: str, balance: float, regime_ok: bool
-) -> bool:
-    if config.FORCE_TRADES:
-        logger.warning("FORCE_TRADES override active: ignoring all pre-trade halts.")
+        logger.debug('SKIP_LOW_SIGNAL', extra={'symbol': symbol, 'sig': sig, 'conf': conf})
+        return (-1, 0.0, '')
+    return (sig, conf, strat)
+
+def pre_trade_checks(ctx: BotContext, state: BotState, symbol: str, balance: float, regime_ok: bool) -> bool:
+    if S.force_trades:
+        logger.warning('FORCE_TRADES override active: ignoring all pre-trade halts.')
         return True
-    # Streak kill-switch check
-    if (
-        state.streak_halt_until
-        and datetime.now(UTC).astimezone(PACIFIC) < state.streak_halt_until
-    ):
-        logger.info(
-            "SKIP_STREAK_HALT",
-            extra={"symbol": symbol, "until": state.streak_halt_until},
-        )
-        _log_health_diagnostics(ctx, "streak")
+    if state.streak_halt_until and datetime.now(UTC).astimezone(PACIFIC) < state.streak_halt_until:
+        logger.info('SKIP_STREAK_HALT', extra={'symbol': symbol, 'until': state.streak_halt_until})
+        _log_health_diagnostics(ctx, 'streak')
         return False
-    if getattr(state, "pdt_blocked", False):
-        logger.info("SKIP_PDT_RULE", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "pdt")
+    if getattr(state, 'pdt_blocked', False):
+        logger.info('SKIP_PDT_RULE', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'pdt')
         return False
     if check_halt_flag(ctx):
-        logger.info("SKIP_HALT_FLAG", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "halt_flag")
+        logger.info('SKIP_HALT_FLAG', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'halt_flag')
         return False
     if check_daily_loss(ctx, state):
-        logger.info("SKIP_DAILY_LOSS", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "daily_loss")
+        logger.info('SKIP_DAILY_LOSS', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'daily_loss')
         return False
     if check_weekly_loss(ctx, state):
-        logger.info("SKIP_WEEKLY_LOSS", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "weekly_loss")
+        logger.info('SKIP_WEEKLY_LOSS', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'weekly_loss')
         return False
     if too_many_positions(ctx, symbol):
-        logger.info("SKIP_TOO_MANY_POSITIONS", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "positions")
+        logger.info('SKIP_TOO_MANY_POSITIONS', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'positions')
         return False
     if too_correlated(ctx, symbol):
-        logger.info("SKIP_HIGH_CORRELATION", extra={"symbol": symbol})
-        _log_health_diagnostics(ctx, "correlation")
+        logger.info('SKIP_HIGH_CORRELATION', extra={'symbol': symbol})
+        _log_health_diagnostics(ctx, 'correlation')
         return False
     return ctx.data_fetcher.get_daily_df(ctx, symbol) is not None
 
-
-def should_enter(
-    ctx: BotContext, state: BotState, symbol: str, balance: float, regime_ok: bool
-) -> bool:
+def should_enter(ctx: BotContext, state: BotState, symbol: str, balance: float, regime_ok: bool) -> bool:
     return pre_trade_checks(ctx, state, symbol, balance, regime_ok)
 
-
-def should_exit(
-    ctx: BotContext, symbol: str, price: float, atr: float
-) -> tuple[bool, int, str]:
+def should_exit(ctx: BotContext, symbol: str, price: float, atr: float) -> tuple[bool, int, str]:
     try:
         pos = ctx.api.get_open_position(symbol)
         current_qty = int(pos.qty)
     except Exception:
         current_qty = 0
-
-    # AI-AGENT-REF: remove time-based rebalance hold logic
     if symbol in ctx.rebalance_buys:
         ctx.rebalance_buys.pop(symbol, None)
-
     stop = ctx.stop_targets.get(symbol)
     if stop is not None:
         if current_qty > 0 and price <= stop:
-            return True, abs(current_qty), "stop_loss"
+            return (True, abs(current_qty), 'stop_loss')
         if current_qty < 0 and price >= stop:
-            return True, abs(current_qty), "stop_loss"
-
+            return (True, abs(current_qty), 'stop_loss')
     tp = ctx.take_profit_targets.get(symbol)
-    if current_qty > 0 and tp and price >= tp:
+    if current_qty > 0 and tp and (price >= tp):
         exit_qty = max(int(abs(current_qty) * SCALING_FACTOR), 1)
-        return True, exit_qty, "take_profit"
-    if current_qty < 0 and tp and price <= tp:
+        return (True, exit_qty, 'take_profit')
+    if current_qty < 0 and tp and (price <= tp):
         exit_qty = max(int(abs(current_qty) * SCALING_FACTOR), 1)
-        return True, exit_qty, "take_profit"
-
+        return (True, exit_qty, 'take_profit')
     action = update_trailing_stop(ctx, symbol, price, current_qty, atr)
-    if (action == "exit_long" and current_qty > 0) or (
-        action == "exit_short" and current_qty < 0
-    ):
-        return True, abs(current_qty), "trailing_stop"
-
-    return False, 0, ""
-
-
-def _safe_trade(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    balance: float,
-    model: RandomForestClassifier,
-    regime_ok: bool,
-    side: OrderSide | None = None,
-) -> bool:
-    try:
-        # Real-time position check to prevent buy/sell flip-flops
+    if action == 'exit_long' and current_qty > 0 or (action == 'exit_short' and current_qty < 0):
+        return (True, abs(current_qty), 'trailing_stop')
+    return (False, 0, '')
+
+def _safe_trade(ctx: BotContext, state: BotState, symbol: str, balance: float, model: RandomForestClassifier, regime_ok: bool, side: OrderSide | None=None) -> bool:
+    try:
         if side is not None:
             try:
-                live_positions = {
-                    p.symbol: int(p.qty) for p in ctx.api.get_all_positions()
-                }
+                live_positions = {p.symbol: int(p.qty) for p in ctx.api.get_all_positions()}
                 if side == OrderSide.BUY and symbol in live_positions:
-                    logger.info(f"REALTIME_SKIP | {symbol} already held. Skipping BUY.")
+                    logger.info(f'REALTIME_SKIP | {symbol} already held. Skipping BUY.')
                     return False
                 elif side == OrderSide.SELL and symbol not in live_positions:
-                    logger.info(f"REALTIME_SKIP | {symbol} not held. Skipping SELL.")
+                    logger.info(f'REALTIME_SKIP | {symbol} not held. Skipping SELL.')
                     return False
             except Exception as e:
-                logger.warning(
-                    f"REALTIME_CHECK_FAIL | Could not check live positions for {symbol}: {e}"
-                )
+                logger.warning(f'REALTIME_CHECK_FAIL | Could not check live positions for {symbol}: {e}')
         return trade_logic(ctx, state, symbol, balance, model, regime_ok)
     except RetryError as e:
-        logger.warning(
-            f"[trade_logic] retries exhausted for {symbol}: {e}",
-            extra={"symbol": symbol},
-        )
+        logger.warning(f'[trade_logic] retries exhausted for {symbol}: {e}', extra={'symbol': symbol})
         return False
     except APIError as e:
         msg = str(e).lower()
-        if "insufficient buying power" in msg or "potential wash trade" in msg:
-            logger.warning(
-                f"[trade_logic] skipping {symbol} due to APIError: {e}",
-                extra={"symbol": symbol},
-            )
+        if 'insufficient buying power' in msg or 'potential wash trade' in msg:
+            logger.warning(f'[trade_logic] skipping {symbol} due to APIError: {e}', extra={'symbol': symbol})
             return False
         else:
-            logger.exception(f"[trade_logic] APIError for {symbol}: {e}")
+            logger.exception(f'[trade_logic] APIError for {symbol}: {e}')
             return False
     except Exception:
-        logger.exception(f"[trade_logic] unhandled exception for {symbol}")
+        logger.exception(f'[trade_logic] unhandled exception for {symbol}')
         return False
 
-
-def _fetch_feature_data(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-) -> tuple[pd.DataFrame | None, pd.DataFrame | None, bool | None]:
+def _fetch_feature_data(ctx: BotContext, state: BotState, symbol: str) -> tuple[pd.DataFrame | None, pd.DataFrame | None, bool | None]:
     """Fetch raw price data and compute indicators.
 
     Returns ``(raw_df, feat_df, skip_flag)``. When data is missing returns
@@ -6155,93 +3797,69 @@     try:
         raw_df = fetch_minute_df_safe(symbol)
     except DataFetchError:
-        logger.info(f"SKIP_NO_PRICE_DATA | {symbol}")
-        return None, None, False
+        logger.info(f'SKIP_NO_PRICE_DATA | {symbol}')
+        return (None, None, False)
     except APIError as e:
         msg = str(e).lower()
-        if "subscription does not permit querying recent sip data" in msg:
-            logger.debug(f"{symbol}: minute fetch failed, falling back to daily.")
+        if 'subscription does not permit querying recent sip data' in msg:
+            logger.debug(f'{symbol}: minute fetch failed, falling back to daily.')
             raw_df = ctx.data_fetcher.get_daily_df(ctx, symbol)
             if raw_df is None or raw_df.empty:
-                logger.debug(f"{symbol}: no daily data either; skipping.")
-                logger.info(f"SKIP_NO_PRICE_DATA | {symbol}")
-                return None, None, False
+                logger.debug(f'{symbol}: no daily data either; skipping.')
+                logger.info(f'SKIP_NO_PRICE_DATA | {symbol}')
+                return (None, None, False)
         else:
             raise
     if raw_df is None or raw_df.empty:
-        logger.info(f"SKIP_NO_PRICE_DATA | {symbol}")
-        return None, None, False
-
-    # Guard: validate OHLCV shape before feature engineering
+        logger.info(f'SKIP_NO_PRICE_DATA | {symbol}')
+        return (None, None, False)
     try:
         validate_ohlcv(raw_df)
     except Exception as e:
-        logger.warning("OHLCV validation failed for %s: %s; skipping symbol", symbol, e)
-        return raw_df, pd.DataFrame(), True
-
+        logger.warning('OHLCV validation failed for %s: %s; skipping symbol', symbol, e)
+        return (raw_df, pd.DataFrame(), True)
     df = raw_df.copy()
-    # AI-AGENT-REF: log initial dataframe and monitor row drops
-    logger.debug(f"Initial tail data for {symbol}:\n{df.tail(5)}")
+    logger.debug(f'Initial tail data for {symbol}:\n{df.tail(5)}')
     initial_len = len(df)
-
     df = compute_macd(df)
-    assert_row_integrity(initial_len, len(df), "compute_macd", symbol)
+    assert_row_integrity(initial_len, len(df), 'compute_macd', symbol)
     logger.debug(f"[{symbol}] Post MACD: last closes:\n{df[['close']].tail(5)}")
-
     df = compute_atr(df)
-    assert_row_integrity(initial_len, len(df), "compute_atr", symbol)
+    assert_row_integrity(initial_len, len(df), 'compute_atr', symbol)
     logger.debug(f"[{symbol}] Post ATR: last closes:\n{df[['close']].tail(5)}")
-
     df = compute_vwap(df)
-    assert_row_integrity(initial_len, len(df), "compute_vwap", symbol)
+    assert_row_integrity(initial_len, len(df), 'compute_vwap', symbol)
     logger.debug(f"[{symbol}] Post VWAP: last closes:\n{df[['close']].tail(5)}")
-
     df = compute_macds(df)
-    logger.debug(f"{symbol} dataframe columns after indicators: {df.columns.tolist()}")
-    df = ensure_columns(df, ["macd", "atr", "vwap", "macds"], symbol)
+    logger.debug(f'{symbol} dataframe columns after indicators: {df.columns.tolist()}')
+    df = ensure_columns(df, ['macd', 'atr', 'vwap', 'macds'], symbol)
     if df.empty and raw_df is not None:
         df = raw_df.copy()
-
     try:
         feat_df = prepare_indicators(df)
         if feat_df is None:
-            return raw_df, None, True
-        # AI-AGENT-REF: fallback to raw data when feature engineering drops all rows
+            return (raw_df, None, True)
         if feat_df.empty:
-            logger.warning(
-                "Parsed feature DataFrame is empty; falling back to raw data"
-            )
+            logger.warning('Parsed feature DataFrame is empty; falling back to raw data')
             feat_df = raw_df.copy()
     except ValueError as exc:
-        logger.warning(f"Indicator preparation failed for {symbol}: {exc}")
-        return raw_df, None, True
+        logger.warning(f'Indicator preparation failed for {symbol}: {exc}')
+        return (raw_df, None, True)
     if feat_df.empty:
-        logger.debug(f"SKIP_INSUFFICIENT_FEATURES | symbol={symbol}")
-        return raw_df, None, True
-    return raw_df, feat_df, None
-
+        logger.debug(f'SKIP_INSUFFICIENT_FEATURES | symbol={symbol}')
+        return (raw_df, None, True)
+    return (raw_df, feat_df, None)
 
 def _model_feature_names(model) -> list[str]:
-    if hasattr(model, "feature_names_in_"):
+    if hasattr(model, 'feature_names_in_'):
         return list(model.feature_names_in_)
-    return [
-        "rsi",
-        "macd",
-        "atr",
-        "vwap",
-        "macds",
-        "ichimoku_conv",
-        "ichimoku_base",
-        "stochrsi",
-    ]
-
+    return ['rsi', 'macd', 'atr', 'vwap', 'macds', 'ichimoku_conv', 'ichimoku_base', 'stochrsi']
 
 def _should_hold_position(df: pd.DataFrame) -> bool:
     """Return True if trend indicators favor staying in the trade."""
-    from ai_trading.indicators import rsi  # type: ignore
-
-    try:
-        close = df["close"].astype(float)
+    from ai_trading.indicators import rsi
+    try:
+        close = df['close'].astype(float)
         ema_fast = close.ewm(span=20, adjust=False).mean().iloc[-1]
         ema_slow = close.ewm(span=50, adjust=False).mean().iloc[-1]
         rsi_val = rsi(tuple(close), 14).iloc[-1]
@@ -6249,37 +3867,23 @@     except Exception:
         return False
 
-
-def _exit_positions_if_needed(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    feat_df: pd.DataFrame,
-    final_score: float,
-    conf: float,
-    current_qty: int,
-) -> bool:
-    if final_score < 0 and current_qty > 0 and abs(conf) >= CONF_THRESHOLD:
+def _exit_positions_if_needed(ctx: BotContext, state: BotState, symbol: str, feat_df: pd.DataFrame, final_score: float, conf: float, current_qty: int) -> bool:
+    if final_score < 0 and current_qty > 0 and (abs(conf) >= CONF_THRESHOLD):
         if _should_hold_position(feat_df):
-            logger.info("HOLD_SIGNAL_ACTIVE", extra={"symbol": symbol})
+            logger.info('HOLD_SIGNAL_ACTIVE', extra={'symbol': symbol})
         else:
             price = get_latest_close(feat_df)
-            logger.info(
-                f"SIGNAL_REVERSAL_EXIT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}"
-            )
-            send_exit_order(ctx, symbol, current_qty, price, "reversal")
+            logger.info(f'SIGNAL_REVERSAL_EXIT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}')
+            send_exit_order(ctx, symbol, current_qty, price, 'reversal')
             ctx.trade_logger.log_exit(state, symbol, price)
             with targets_lock:
                 ctx.stop_targets.pop(symbol, None)
                 ctx.take_profit_targets.pop(symbol, None)
             return True
-
-    if final_score > 0 and current_qty < 0 and abs(conf) >= CONF_THRESHOLD:
+    if final_score > 0 and current_qty < 0 and (abs(conf) >= CONF_THRESHOLD):
         price = get_latest_close(feat_df)
-        logger.info(
-            f"SIGNAL_BULLISH_EXIT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}"
-        )
-        send_exit_order(ctx, symbol, abs(current_qty), price, "reversal")
+        logger.info(f'SIGNAL_BULLISH_EXIT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}')
+        send_exit_order(ctx, symbol, abs(current_qty), price, 'reversal')
         ctx.trade_logger.log_exit(state, symbol, price)
         with targets_lock:
             ctx.stop_targets.pop(symbol, None)
@@ -6287,245 +3891,121 @@         return True
     return False
 
-
-def _enter_long(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    balance: float,
-    feat_df: pd.DataFrame,
-    final_score: float,
-    conf: float,
-    strat: str,
-) -> bool:
+def _enter_long(ctx: BotContext, state: BotState, symbol: str, balance: float, feat_df: pd.DataFrame, final_score: float, conf: float, strat: str) -> bool:
     current_price = get_latest_close(feat_df)
-    logger.debug(f"Latest 5 rows for {symbol}:\n{feat_df.tail(5)}")
-    logger.debug(f"Computed price for {symbol}: {current_price}")
+    logger.debug(f'Latest 5 rows for {symbol}:\n{feat_df.tail(5)}')
+    logger.debug(f'Computed price for {symbol}: {current_price}')
     if current_price <= 0 or pd.isna(current_price):
-        logger.critical(f"Invalid price computed for {symbol}: {current_price}")
+        logger.critical(f'Invalid price computed for {symbol}: {current_price}')
         return True
-
-    # AI-AGENT-REF: Get target weight with sensible fallback for signal-based trading
     target_weight = ctx.portfolio_weights.get(symbol, 0.0)
     if target_weight == 0.0:
-        # If no portfolio weight exists (e.g., new signal), calculate a reasonable default
-        # Based on confidence and ensuring we don't exceed exposure limits
-        confidence_weight = conf * 0.15  # Max 15% for high confidence signals
-        exposure_cap = (
-            getattr(ctx.config, "exposure_cap_aggressive", 0.88)
-            if hasattr(ctx, "config")
-            else 0.88
-        )
-
-        # Get current total exposure to avoid exceeding cap
+        confidence_weight = conf * 0.15
+        exposure_cap = getattr(ctx.config, 'exposure_cap_aggressive', 0.88) if hasattr(ctx, 'config') else 0.88
         try:
             positions = ctx.api.get_all_positions()
-            current_exposure = sum(
-                abs(float(p.market_value)) for p in positions
-            ) / float(ctx.api.get_account().equity)
+            current_exposure = sum((abs(float(p.market_value)) for p in positions)) / float(ctx.api.get_account().equity)
             available_exposure = max(0, exposure_cap - current_exposure)
-            target_weight = min(
-                confidence_weight, available_exposure, 0.15
-            )  # Cap at 15%
-            logger.info(
-                f"Computed weight for {symbol}: {target_weight:.3f} (confidence={conf:.3f}, available_exposure={available_exposure:.3f})"
-            )
+            target_weight = min(confidence_weight, available_exposure, 0.15)
+            logger.info(f'Computed weight for {symbol}: {target_weight:.3f} (confidence={conf:.3f}, available_exposure={available_exposure:.3f})')
         except Exception as e:
-            logger.warning(
-                f"Could not compute dynamic weight for {symbol}: {e}, using confidence-based weight"
-            )
-            target_weight = min(confidence_weight, 0.10)  # Conservative 10% fallback
-
+            logger.warning(f'Could not compute dynamic weight for {symbol}: {e}, using confidence-based weight')
+            target_weight = min(confidence_weight, 0.1)
     raw_qty = int(balance * target_weight / current_price) if current_price > 0 else 0
-
-    # AI-AGENT-REF: Fix zero quantity calculations - ensure minimum position size when cash available
     if raw_qty is None or not np.isfinite(raw_qty) or raw_qty <= 0:
-        # If we have significant cash available and a valid signal, use minimum position size
-        if balance > 1000 and target_weight > 0.001 and current_price > 0:
-            raw_qty = max(1, int(1000 / current_price))  # Minimum $1000 position
-            logger.info(
-                f"Using minimum position size for {symbol}: {raw_qty} shares (balance=${balance:.0f})"
-            )
+        if balance > 1000 and target_weight > 0.001 and (current_price > 0):
+            raw_qty = max(1, int(1000 / current_price))
+            logger.info(f'Using minimum position size for {symbol}: {raw_qty} shares (balance=${balance:.0f})')
         else:
-            logger.warning(
-                f"Skipping {symbol}: computed qty <= 0 (balance=${balance:.0f}, weight={target_weight:.4f})"
-            )
+            logger.warning(f'Skipping {symbol}: computed qty <= 0 (balance=${balance:.0f}, weight={target_weight:.4f})')
             return True
-    logger.info(
-        f"SIGNAL_BUY | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}  qty={raw_qty}"
-    )
+    logger.info(f'SIGNAL_BUY | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}  qty={raw_qty}')
     if not sector_exposure_ok(ctx, symbol, raw_qty, current_price):
-        logger.info(
-            "SKIP_SECTOR_CAP | Buy order skipped due to sector exposure limits",
-            extra={
-                "symbol": symbol,
-                "side": "buy",
-                "qty": raw_qty,
-                "price": current_price,
-            },
-        )
+        logger.info('SKIP_SECTOR_CAP | Buy order skipped due to sector exposure limits', extra={'symbol': symbol, 'side': 'buy', 'qty': raw_qty, 'price': current_price})
         return True
-    order = submit_order(ctx, symbol, raw_qty, "buy")
+    order = submit_order(ctx, symbol, raw_qty, 'buy')
     if order is None:
-        logger.debug(f"TRADE_LOGIC_NO_ORDER | symbol={symbol}")
+        logger.debug(f'TRADE_LOGIC_NO_ORDER | symbol={symbol}')
     else:
-        logger.debug(f"TRADE_LOGIC_ORDER_PLACED | symbol={symbol}  order_id={order.id}")
-        ctx.trade_logger.log_entry(
-            symbol,
-            current_price,
-            raw_qty,
-            "buy",
-            strat,
-            signal_tags=strat,
-            confidence=conf,
-        )
+        logger.debug(f'TRADE_LOGIC_ORDER_PLACED | symbol={symbol}  order_id={order.id}')
+        ctx.trade_logger.log_entry(symbol, current_price, raw_qty, 'buy', strat, signal_tags=strat, confidence=conf)
         now_pac = datetime.now(UTC).astimezone(PACIFIC)
         mo = datetime.combine(now_pac.date(), ctx.market_open, PACIFIC)
         mc = datetime.combine(now_pac.date(), ctx.market_close, PACIFIC)
-        tp_factor = (
-            TAKE_PROFIT_FACTOR * 1.1 if is_high_vol_regime() else TAKE_PROFIT_FACTOR
-        )
-        stop, take = scaled_atr_stop(
-            entry_price=current_price,
-            atr=feat_df["atr"].iloc[-1],
-            now=now_pac,
-            market_open=mo,
-            market_close=mc,
-            max_factor=tp_factor,
-            min_factor=0.5,
-        )
+        tp_factor = TAKE_PROFIT_FACTOR * 1.1 if is_high_vol_regime() else TAKE_PROFIT_FACTOR
+        stop, take = scaled_atr_stop(entry_price=current_price, atr=feat_df['atr'].iloc[-1], now=now_pac, market_open=mo, market_close=mc, max_factor=tp_factor, min_factor=0.5)
         with targets_lock:
             ctx.stop_targets[symbol] = stop
             ctx.take_profit_targets[symbol] = take
-        # AI-AGENT-REF: Add thread-safe locking for trade cooldown modifications
         with trade_cooldowns_lock:
             state.trade_cooldowns[symbol] = datetime.now(UTC)
-        state.last_trade_direction[symbol] = "buy"
-
-        # AI-AGENT-REF: Record trade in frequency tracker for overtrading prevention
+        state.last_trade_direction[symbol] = 'buy'
         _record_trade_in_frequency_tracker(state, symbol, datetime.now(UTC))
     return True
 
-
-def _enter_short(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    feat_df: pd.DataFrame,
-    final_score: float,
-    conf: float,
-    strat: str,
-) -> bool:
+def _enter_short(ctx: BotContext, state: BotState, symbol: str, feat_df: pd.DataFrame, final_score: float, conf: float, strat: str) -> bool:
     current_price = get_latest_close(feat_df)
-    logger.debug(f"Latest 5 rows for {symbol}:\n{feat_df.tail(5)}")
-    logger.debug(f"Computed price for {symbol}: {current_price}")
+    logger.debug(f'Latest 5 rows for {symbol}:\n{feat_df.tail(5)}')
+    logger.debug(f'Computed price for {symbol}: {current_price}')
     if current_price <= 0 or pd.isna(current_price):
-        logger.critical(f"Invalid price computed for {symbol}: {current_price}")
+        logger.critical(f'Invalid price computed for {symbol}: {current_price}')
         return True
-    atr = feat_df["atr"].iloc[-1]
+    atr = feat_df['atr'].iloc[-1]
     qty = calculate_entry_size(ctx, symbol, current_price, atr, conf)
     try:
         asset = ctx.api.get_asset(symbol)
-        if hasattr(asset, "shortable") and not asset.shortable:
-            logger.info(f"SKIP_NOT_SHORTABLE | symbol={symbol}")
+        if hasattr(asset, 'shortable') and (not asset.shortable):
+            logger.info(f'SKIP_NOT_SHORTABLE | symbol={symbol}')
             return True
-        avail = getattr(asset, "shortable_shares", None)
+        avail = getattr(asset, 'shortable_shares', None)
         if avail is not None:
             qty = min(qty, int(avail))
     except Exception as exc:
-        logger.exception("bot.py unexpected", exc_info=exc)
+        logger.exception('bot.py unexpected', exc_info=exc)
         raise
     if qty is None or not np.isfinite(qty) or qty <= 0:
-        logger.warning(f"Skipping {symbol}: computed qty <= 0")
+        logger.warning(f'Skipping {symbol}: computed qty <= 0')
         return True
-    logger.info(
-        f"SIGNAL_SHORT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}  qty={qty}"
-    )
+    logger.info(f'SIGNAL_SHORT | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}  qty={qty}')
     if not sector_exposure_ok(ctx, symbol, qty, current_price):
-        logger.info(
-            "SKIP_SECTOR_CAP | Short order skipped due to sector exposure limits",
-            extra={
-                "symbol": symbol,
-                "side": "sell_short",
-                "qty": qty,
-                "price": current_price,
-            },
-        )
+        logger.info('SKIP_SECTOR_CAP | Short order skipped due to sector exposure limits', extra={'symbol': symbol, 'side': 'sell_short', 'qty': qty, 'price': current_price})
         return True
-    order = submit_order(
-        ctx, symbol, qty, "sell_short"
-    )  # AI-AGENT-REF: Use sell_short for short signals
+    order = submit_order(ctx, symbol, qty, 'sell_short')
     if order is None:
-        logger.debug(f"TRADE_LOGIC_NO_ORDER | symbol={symbol}")
+        logger.debug(f'TRADE_LOGIC_NO_ORDER | symbol={symbol}')
     else:
-        logger.debug(f"TRADE_LOGIC_ORDER_PLACED | symbol={symbol}  order_id={order.id}")
-        ctx.trade_logger.log_entry(
-            symbol,
-            current_price,
-            qty,
-            "sell",
-            strat,
-            signal_tags=strat,
-            confidence=conf,
-        )
+        logger.debug(f'TRADE_LOGIC_ORDER_PLACED | symbol={symbol}  order_id={order.id}')
+        ctx.trade_logger.log_entry(symbol, current_price, qty, 'sell', strat, signal_tags=strat, confidence=conf)
         now_pac = datetime.now(UTC).astimezone(PACIFIC)
         mo = datetime.combine(now_pac.date(), ctx.market_open, PACIFIC)
         mc = datetime.combine(now_pac.date(), ctx.market_close, PACIFIC)
-        tp_factor = (
-            TAKE_PROFIT_FACTOR * 1.1 if is_high_vol_regime() else TAKE_PROFIT_FACTOR
-        )
-        long_stop, long_take = scaled_atr_stop(
-            entry_price=current_price,
-            atr=atr,
-            now=now_pac,
-            market_open=mo,
-            market_close=mc,
-            max_factor=tp_factor,
-            min_factor=0.5,
-        )
-        stop, take = long_take, long_stop
+        tp_factor = TAKE_PROFIT_FACTOR * 1.1 if is_high_vol_regime() else TAKE_PROFIT_FACTOR
+        long_stop, long_take = scaled_atr_stop(entry_price=current_price, atr=atr, now=now_pac, market_open=mo, market_close=mc, max_factor=tp_factor, min_factor=0.5)
+        stop, take = (long_take, long_stop)
         with targets_lock:
             ctx.stop_targets[symbol] = stop
             ctx.take_profit_targets[symbol] = take
-        # AI-AGENT-REF: Add thread-safe locking for trade cooldown modifications
         with trade_cooldowns_lock:
             state.trade_cooldowns[symbol] = datetime.now(UTC)
-        state.last_trade_direction[symbol] = "sell"
-
-        # AI-AGENT-REF: Record trade in frequency tracker for overtrading prevention
+        state.last_trade_direction[symbol] = 'sell'
         _record_trade_in_frequency_tracker(state, symbol, datetime.now(UTC))
     return True
 
-
-def _manage_existing_position(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    feat_df: pd.DataFrame,
-    conf: float,
-    atr: float,
-    current_qty: int,
-) -> bool:
+def _manage_existing_position(ctx: BotContext, state: BotState, symbol: str, feat_df: pd.DataFrame, conf: float, atr: float, current_qty: int) -> bool:
     price = get_latest_close(feat_df)
-    logger.debug(f"Latest 5 rows for {symbol}:\n{feat_df.tail(5)}")
-    logger.debug(f"Computed price for {symbol}: {price}")
+    logger.debug(f'Latest 5 rows for {symbol}:\n{feat_df.tail(5)}')
+    logger.debug(f'Computed price for {symbol}: {price}')
     if price <= 0 or pd.isna(price):
-        logger.critical(f"Invalid price computed for {symbol}: {price}")
+        logger.critical(f'Invalid price computed for {symbol}: {price}')
         return False
-    # AI-AGENT-REF: always rely on indicator-driven exits
     should_exit_flag, exit_qty, reason = should_exit(ctx, symbol, price, atr)
     if should_exit_flag and exit_qty > 0:
-        logger.info(
-            f"EXIT_SIGNAL | symbol={symbol}  reason={reason}  exit_qty={exit_qty}  price={price:.4f}"
-        )
+        logger.info(f'EXIT_SIGNAL | symbol={symbol}  reason={reason}  exit_qty={exit_qty}  price={price:.4f}')
         send_exit_order(ctx, symbol, exit_qty, price, reason)
-        if reason == "stop_loss":
-            # AI-AGENT-REF: Add thread-safe locking for trade cooldown modifications
+        if reason == 'stop_loss':
             with trade_cooldowns_lock:
                 state.trade_cooldowns[symbol] = datetime.now(UTC)
-            state.last_trade_direction[symbol] = "sell"
-
-            # AI-AGENT-REF: Record trade in frequency tracker for overtrading prevention
+            state.last_trade_direction[symbol] = 'sell'
             _record_trade_in_frequency_tracker(state, symbol, datetime.now(UTC))
         ctx.trade_logger.log_exit(state, symbol, price)
         try:
@@ -6535,7 +4015,7 @@                     ctx.stop_targets.pop(symbol, None)
                     ctx.take_profit_targets.pop(symbol, None)
         except Exception as exc:
-            logger.exception("bot.py unexpected", exc_info=exc)
+            logger.exception('bot.py unexpected', exc_info=exc)
             raise
     else:
         try:
@@ -6543,33 +4023,20 @@             entry_price = float(pos.avg_entry_price)
             maybe_pyramid(ctx, symbol, entry_price, price, atr, conf)
         except Exception as exc:
-            logger.exception("bot.py unexpected", exc_info=exc)
+            logger.exception('bot.py unexpected', exc_info=exc)
             raise
     return True
 
-
-def _evaluate_trade_signal(
-    ctx: BotContext, state: BotState, feat_df: pd.DataFrame, symbol: str, model: Any
-) -> tuple[float, float, str]:
+def _evaluate_trade_signal(ctx: BotContext, state: BotState, feat_df: pd.DataFrame, symbol: str, model: Any) -> tuple[float, float, str]:
     """Return ``(final_score, confidence, strategy)`` for ``symbol``."""
-
     sig, conf, strat = ctx.signal_manager.evaluate(ctx, state, feat_df, symbol, model)
-    comp_list = [
-        {"signal": lab, "flag": s, "weight": w}
-        for s, w, lab in ctx.signal_manager.last_components
-    ]
-    logger.debug("COMPONENTS | symbol=%s  components=%r", symbol, comp_list)
-    final_score = sum(s * w for s, w, _ in ctx.signal_manager.last_components)
-    logger.info(
-        "SIGNAL_RESULT | symbol=%s  final_score=%.4f  confidence=%.4f",
-        symbol,
-        final_score,
-        conf,
-    )
+    comp_list = [{'signal': lab, 'flag': s, 'weight': w} for s, w, lab in ctx.signal_manager.last_components]
+    logger.debug('COMPONENTS | symbol=%s  components=%r', symbol, comp_list)
+    final_score = sum((s * w for s, w, _ in ctx.signal_manager.last_components))
+    logger.info('SIGNAL_RESULT | symbol=%s  final_score=%.4f  confidence=%.4f', symbol, final_score, conf)
     if final_score is None or not np.isfinite(final_score) or final_score == 0:
-        raise ValueError("Invalid or empty signal")
-    return final_score, conf, strat
-
+        raise ValueError('Invalid or empty signal')
+    return (final_score, conf, strat)
 
 def _current_position_qty(ctx: BotContext, symbol: str) -> int:
     try:
@@ -6577,7 +4044,6 @@         return int(pos.qty)
     except Exception:
         return 0
-
 
 def _recent_rebalance_flag(ctx: BotContext, symbol: str) -> bool:
     """Return ``False`` and clear any rebalance timestamp."""
@@ -6585,127 +4051,77 @@         ctx.rebalance_buys.pop(symbol, None)
     return False
 
-
-def trade_logic(
-    ctx: BotContext,
-    state: BotState,
-    symbol: str,
-    balance: float,
-    model: Any,
-    regime_ok: bool,
-) -> bool:
+def trade_logic(ctx: BotContext, state: BotState, symbol: str, balance: float, model: Any, regime_ok: bool) -> bool:
     """
     Core per-symbol logic: fetch data, compute features, evaluate signals, enter/exit orders.
     """
-    logger.info(f"PROCESSING_SYMBOL | symbol={symbol}")
-
+    logger.info(f'PROCESSING_SYMBOL | symbol={symbol}')
     if not pre_trade_checks(ctx, state, symbol, balance, regime_ok):
-        logger.debug("SKIP_PRE_TRADE_CHECKS", extra={"symbol": symbol})
+        logger.debug('SKIP_PRE_TRADE_CHECKS', extra={'symbol': symbol})
         return False
-
     raw_df, feat_df, skip_flag = _fetch_feature_data(ctx, state, symbol)
     if feat_df is None:
         return skip_flag if skip_flag is not None else False
-
-    for col in ["macd", "atr", "vwap", "macds"]:
+    for col in ['macd', 'atr', 'vwap', 'macds']:
         if col not in feat_df.columns:
             feat_df[col] = 0.0
-
     feature_names = _model_feature_names(model)
     missing = [f for f in feature_names if f not in feat_df.columns]
     if missing:
-        logger.debug(
-            f"Feature snapshot for {symbol}: macd={feat_df['macd'].iloc[-1]}, atr={feat_df['atr'].iloc[-1]}, vwap={feat_df['vwap'].iloc[-1]}, macds={feat_df['macds'].iloc[-1]}"
-        )
-        logger.info("SKIP_MISSING_FEATURES | symbol=%s  missing=%s", symbol, missing)
+        logger.debug(f"Feature snapshot for {symbol}: macd={feat_df['macd'].iloc[-1]}, atr={feat_df['atr'].iloc[-1]}, vwap={feat_df['vwap'].iloc[-1]}, macds={feat_df['macds'].iloc[-1]}")
+        logger.info('SKIP_MISSING_FEATURES | symbol=%s  missing=%s', symbol, missing)
         return True
-
-    try:
-        final_score, conf, strat = _evaluate_trade_signal(
-            ctx, state, feat_df, symbol, model
-        )
+    try:
+        final_score, conf, strat = _evaluate_trade_signal(ctx, state, feat_df, symbol, model)
     except ValueError as exc:
-        logger.error("%s", exc)
+        logger.error('%s', exc)
         return True
     if pd.isna(final_score) or pd.isna(conf):
-        logger.warning(f"Skipping {symbol}: model returned NaN prediction")
+        logger.warning(f'Skipping {symbol}: model returned NaN prediction')
         return True
-
     current_qty = _current_position_qty(ctx, symbol)
-
     now = datetime.now(UTC)
-
-    signal = "buy" if final_score > 0 else "sell" if final_score < 0 else "hold"
-
-    if _exit_positions_if_needed(
-        ctx, state, symbol, feat_df, final_score, conf, current_qty
-    ):
+    signal = 'buy' if final_score > 0 else 'sell' if final_score < 0 else 'hold'
+    if _exit_positions_if_needed(ctx, state, symbol, feat_df, final_score, conf, current_qty):
         return True
-
-    # AI-AGENT-REF: Add thread-safe locking for trade cooldown access
     with trade_cooldowns_lock:
         cd_ts = state.trade_cooldowns.get(symbol)
     if cd_ts and (now - cd_ts).total_seconds() < TRADE_COOLDOWN_MIN * 60:
         prev = state.last_trade_direction.get(symbol)
-        if prev and (
-            (prev == "buy" and signal == "sell") or (prev == "sell" and signal == "buy")
-        ):
-            logger.info("SKIP_REVERSED_SIGNAL", extra={"symbol": symbol})
+        if prev and (prev == 'buy' and signal == 'sell' or (prev == 'sell' and signal == 'buy')):
+            logger.info('SKIP_REVERSED_SIGNAL', extra={'symbol': symbol})
             return True
-        logger.debug("SKIP_COOLDOWN", extra={"symbol": symbol})
+        logger.debug('SKIP_COOLDOWN', extra={'symbol': symbol})
         return True
-
-    # AI-AGENT-REF: Enhanced overtrading prevention - check frequency limits
     if _check_trade_frequency_limits(state, symbol, now):
-        logger.info("SKIP_FREQUENCY_LIMIT", extra={"symbol": symbol})
+        logger.info('SKIP_FREQUENCY_LIMIT', extra={'symbol': symbol})
         return True
-
-    if final_score > 0 and conf >= BUY_THRESHOLD and current_qty == 0:
+    if final_score > 0 and conf >= BUY_THRESHOLD and (current_qty == 0):
         if symbol in state.long_positions:
             held = state.position_cache.get(symbol, 0)
-            logger.info(
-                f"Skipping BUY for {symbol} — position already LONG {held} shares"
-            )
+            logger.info(f'Skipping BUY for {symbol} — position already LONG {held} shares')
             return True
-        return _enter_long(
-            ctx, state, symbol, balance, feat_df, final_score, conf, strat
-        )
-
-    if final_score < 0 and conf >= BUY_THRESHOLD and current_qty == 0:
+        return _enter_long(ctx, state, symbol, balance, feat_df, final_score, conf, strat)
+    if final_score < 0 and conf >= BUY_THRESHOLD and (current_qty == 0):
         if symbol in state.short_positions:
             held = abs(state.position_cache.get(symbol, 0))
-            logger.info(
-                f"Skipping SELL for {symbol} — position already SHORT {held} shares"
-            )
+            logger.info(f'Skipping SELL for {symbol} — position already SHORT {held} shares')
             return True
         return _enter_short(ctx, state, symbol, feat_df, final_score, conf, strat)
-
-    # If holding, check for stops/take/trailing
     if current_qty != 0:
-        atr = feat_df["atr"].iloc[-1]
-        return _manage_existing_position(
-            ctx, state, symbol, feat_df, conf, atr, current_qty
-        )
-
-    # Else hold / no action
-    logger.info(
-        f"SKIP_LOW_OR_NO_SIGNAL | symbol={symbol}  "
-        f"final_score={final_score:.4f}  confidence={conf:.4f}"
-    )
+        atr = feat_df['atr'].iloc[-1]
+        return _manage_existing_position(ctx, state, symbol, feat_df, conf, atr, current_qty)
+    logger.info(f'SKIP_LOW_OR_NO_SIGNAL | symbol={symbol}  final_score={final_score:.4f}  confidence={conf:.4f}')
     return True
-
 
 def compute_portfolio_weights(ctx: BotContext, symbols: list[str]) -> dict[str, float]:
     """Delegate to ai_trading.portfolio.compute_portfolio_weights with correct ctx."""
     from ai_trading.portfolio import compute_portfolio_weights as _cpw
-
     return _cpw(ctx, symbols)
-
 
 def on_trade_exit_rebalance(ctx: BotContext) -> None:
     from ai_trading import portfolio
     from ai_trading.utils import portfolio_lock
-
     try:
         positions = ctx.api.get_all_positions()
         symbols = [p.symbol for p in positions]
@@ -6713,10 +4129,10 @@         symbols = []
     current = portfolio.compute_portfolio_weights(ctx, symbols)
     old = ctx.portfolio_weights
-    drift = max(abs(current[s] - old.get(s, 0)) for s in current) if current else 0
+    drift = max((abs(current[s] - old.get(s, 0)) for s in current)) if current else 0
     if drift <= 0.1:
         return True
-    with portfolio_lock:  # FIXED: protect shared portfolio state
+    with portfolio_lock:
         ctx.portfolio_weights = current
     total_value = float(ctx.api.get_account().portfolio_value)
     for sym, w in current.items():
@@ -6724,40 +4140,29 @@         try:
             raw = fetch_minute_df_safe(sym)
         except DataFetchError:
-            logger.warning("REBALANCE_NO_DATA | %s", sym)
+            logger.warning('REBALANCE_NO_DATA | %s', sym)
             continue
         price = get_latest_close(raw) if raw is not None else 1.0
         if price <= 0:
             continue
         target_shares = int(round(target_dollar / price))
         try:
-            submit_order(
-                ctx,
-                sym,
-                abs(target_shares),
-                "buy" if target_shares > 0 else "sell",
-            )
+            submit_order(ctx, sym, abs(target_shares), 'buy' if target_shares > 0 else 'sell')
         except Exception:
-            logger.exception(f"Rebalance failed for {sym}")
-    logger.info("PORTFOLIO_REBALANCED")
-
+            logger.exception(f'Rebalance failed for {sym}')
+    logger.info('PORTFOLIO_REBALANCED')
 
 def pair_trade_signal(sym1: str, sym2: str) -> tuple[str, int]:
     from statsmodels.tsa.stattools import coint
-
     df1 = ctx.data_fetcher.get_daily_df(ctx, sym1)
     df2 = ctx.data_fetcher.get_daily_df(ctx, sym2)
-    if not hasattr(df1, "loc") or "close" not in df1.columns:
-        raise ValueError(
-            f"pair_trade_signal: df1 for {sym1} is invalid or missing 'close'"
-        )
-    if not hasattr(df2, "loc") or "close" not in df2.columns:
-        raise ValueError(
-            f"pair_trade_signal: df2 for {sym2} is invalid or missing 'close'"
-        )
-    df = pd.concat([df1["close"], df2["close"]], axis=1).dropna()
+    if not hasattr(df1, 'loc') or 'close' not in df1.columns:
+        raise ValueError(f"pair_trade_signal: df1 for {sym1} is invalid or missing 'close'")
+    if not hasattr(df2, 'loc') or 'close' not in df2.columns:
+        raise ValueError(f"pair_trade_signal: df2 for {sym2} is invalid or missing 'close'")
+    df = pd.concat([df1['close'], df2['close']], axis=1).dropna()
     if df.empty:
-        return ("no_signal", 0)
+        return ('no_signal', 0)
     t_stat, p_value, _ = coint(df.iloc[:, 0], df.iloc[:, 1])
     if p_value < 0.05:
         beta = np.polyfit(df.iloc[:, 1], df.iloc[:, 0], 1)[0]
@@ -6765,66 +4170,44 @@         z = (spread - spread.mean()) / spread.std()
         z0 = z.iloc[-1]
         if z0 > 2:
-            return ("short_spread", 1)
+            return ('short_spread', 1)
         elif z0 < -2:
-            return ("long_spread", 1)
-    return ("no_signal", 0)
-
-
-# ─── M. UTILITIES ─────────────────────────────────────────────────────────────
-def fetch_data(
-    ctx: BotContext, symbols: list[str], period: str, interval: str
-) -> pd.DataFrame | None:
+            return ('long_spread', 1)
+    return ('no_signal', 0)
+
+def fetch_data(ctx: BotContext, symbols: list[str], period: str, interval: str) -> pd.DataFrame | None:
     frames: list[pd.DataFrame] = []
     now = datetime.now(UTC)
-    if period.endswith("d"):
+    if period.endswith('d'):
         delta = timedelta(days=int(period[:-1]))
-    elif period.endswith("mo"):
+    elif period.endswith('mo'):
         delta = timedelta(days=30 * int(period[:-2]))
-    elif period.endswith("y"):
+    elif period.endswith('y'):
         delta = timedelta(days=365 * int(period[:-1]))
     else:
         delta = timedelta(days=7)
     unix_to = int(now.timestamp())
     unix_from = int((now - delta).timestamp())
-
     for batch in chunked(symbols, 3):
         for sym in batch:
             try:
-                ohlc = finnhub_client.stock_candle(
-                    sym, resolution=interval, _from=unix_from, to=unix_to
-                )
+                ohlc = finnhub_client.stock_candle(sym, resolution=interval, _from=unix_from, to=unix_to)
             except FinnhubAPIException as e:
-                logger.warning(f"[fetch_data] {sym} error: {e}")
+                logger.warning(f'[fetch_data] {sym} error: {e}')
                 continue
-
-            if not ohlc or ohlc.get("s") != "ok":
+            if not ohlc or ohlc.get('s') != 'ok':
                 continue
-
-            idx = safe_to_datetime(ohlc.get("t", []), context=f"prefetch {sym}")
-            df_sym = pd.DataFrame(
-                {
-                    "open": ohlc.get("o", []),
-                    "high": ohlc.get("h", []),
-                    "low": ohlc.get("l", []),
-                    "close": ohlc.get("c", []),
-                    "volume": ohlc.get("v", []),
-                },
-                index=idx,
-            )
-
+            idx = safe_to_datetime(ohlc.get('t', []), context=f'prefetch {sym}')
+            df_sym = pd.DataFrame({'open': ohlc.get('o', []), 'high': ohlc.get('h', []), 'low': ohlc.get('l', []), 'close': ohlc.get('c', []), 'volume': ohlc.get('v', [])}, index=idx)
             df_sym.columns = pd.MultiIndex.from_product([[sym], df_sym.columns])
             frames.append(df_sym)
-
         pytime.sleep(random.uniform(2, 5))
-
     if not frames:
         return None
-
     return pd.concat(frames, axis=1)
 
-
 class EnsembleModel:
+
     def __init__(self, models):
         self.models = models
 
@@ -6836,21 +4219,15 @@         proba = self.predict_proba(X)
         return np.argmax(proba, axis=1)
 
-
-def load_model(path: str = MODEL_PATH) -> dict | EnsembleModel | None:
+def load_model(path: str=MODEL_PATH) -> dict | EnsembleModel | None:
     """Load a model from ``path`` supporting both single and ensemble files."""
     import joblib
-
     if not os.path.exists(path):
         return None
-
     loaded = joblib.load(path)
-    # if this is a plain dict, return it directly
     if isinstance(loaded, dict):
-        logger.info("MODEL_LOADED")
+        logger.info('MODEL_LOADED')
         return loaded
-
-    # AI-AGENT-REF: use isfile checks for optional ensemble components
     rf_exists = os.path.isfile(MODEL_RF_PATH)
     xgb_exists = os.path.isfile(MODEL_XGB_PATH)
     lgb_exists = os.path.isfile(MODEL_LGB_PATH)
@@ -6860,25 +4237,20 @@             try:
                 models.append(joblib.load(p))
             except Exception as e:
-                logger.exception("MODEL_LOAD_FAILED: %s", e)
+                logger.exception('MODEL_LOAD_FAILED: %s', e)
                 return None
-        logger.info(
-            "MODEL_LOADED",
-            extra={"path": f"{MODEL_RF_PATH}, {MODEL_XGB_PATH}, {MODEL_LGB_PATH}"},
-        )
+        logger.info('MODEL_LOADED', extra={'path': f'{MODEL_RF_PATH}, {MODEL_XGB_PATH}, {MODEL_LGB_PATH}'})
         return EnsembleModel(models)
-
     try:
         if isinstance(loaded, list):
             model = EnsembleModel(loaded)
-            logger.info("MODEL_LOADED")
+            logger.info('MODEL_LOADED')
             return model
-        logger.info("MODEL_LOADED")
+        logger.info('MODEL_LOADED')
         return loaded
     except Exception as e:
-        logger.exception("MODEL_LOAD_FAILED: %s", e)
+        logger.exception('MODEL_LOAD_FAILED: %s', e)
         return None
-
 
 def online_update(state: BotState, symbol: str, X_new, y_new) -> None:
     y_new = np.clip(y_new, -0.05, 0.05)
@@ -6888,616 +4260,399 @@         try:
             model_pipeline.partial_fit(X_new, y_new)
         except Exception as e:
-            logger.error(f"Online update failed for {symbol}: {e}")
+            logger.error(f'Online update failed for {symbol}: {e}')
             return
     pred = model_pipeline.predict(X_new)
     online_error = float(np.mean((pred - y_new) ** 2))
-    log_metrics(
-        {
-            "timestamp": utc_now_iso(),
-            "type": "online_update",
-            "symbol": symbol,
-            "error": online_error,
-        }
-    )
+    log_metrics({'timestamp': utc_now_iso(), 'type': 'online_update', 'symbol': symbol, 'error': online_error})
     state.rolling_losses.append(online_error)
     if len(state.rolling_losses) >= 20 and sum(state.rolling_losses[-20:]) > 0.02:
         state.updates_halted = True
-        logger.warning("Halting online updates due to 20-trade rolling loss >2%")
-
+        logger.warning('Halting online updates due to 20-trade rolling loss >2%')
 
 def update_signal_weights() -> None:
     try:
         if not os.path.exists(TRADE_LOG_FILE):
-            logger.warning("No trades log found; skipping weight update.")
+            logger.warning('No trades log found; skipping weight update.')
             return
-        df = pd.read_csv(
-            TRADE_LOG_FILE,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=[
-                "entry_price",
-                "exit_price",
-                "signal_tags",
-                "side",
-                "confidence",
-                "exit_time",
-            ],
-        ).dropna(subset=["entry_price", "exit_price", "signal_tags"])
+        df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['entry_price', 'exit_price', 'signal_tags', 'side', 'confidence', 'exit_time']).dropna(subset=['entry_price', 'exit_price', 'signal_tags'])
         if df.empty:
-            logger.warning("Loaded DataFrame is empty after parsing/fallback")
-        direction = np.where(df["side"] == "buy", 1, -1)
-        df["pnl"] = (df["exit_price"] - df["entry_price"]) * direction
-        df["confidence"] = df.get("confidence", 0.5)
-        df["reward"] = df["pnl"] * df["confidence"]
+            logger.warning('Loaded DataFrame is empty after parsing/fallback')
+        direction = np.where(df['side'] == 'buy', 1, -1)
+        df['pnl'] = (df['exit_price'] - df['entry_price']) * direction
+        df['confidence'] = df.get('confidence', 0.5)
+        df['reward'] = df['pnl'] * df['confidence']
         optimize_signals(df, config)
-        recent_cut = pd.to_datetime(df["exit_time"], errors="coerce")
-        recent_mask = recent_cut >= (datetime.now(UTC) - timedelta(days=30))
+        recent_cut = pd.to_datetime(df['exit_time'], errors='coerce')
+        recent_mask = recent_cut >= datetime.now(UTC) - timedelta(days=30)
         df_recent = df[recent_mask]
-
-        df_tags = df.assign(tag=df["signal_tags"].str.split("+")).explode("tag")
-        df_recent_tags = df_recent.assign(
-            tag=df_recent["signal_tags"].str.split("+")
-        ).explode("tag")
-        stats_all = df_tags.groupby("tag")["reward"].agg(list).to_dict()
-        stats_recent = df_recent_tags.groupby("tag")["reward"].agg(list).to_dict()
-
+        df_tags = df.assign(tag=df['signal_tags'].str.split('+')).explode('tag')
+        df_recent_tags = df_recent.assign(tag=df_recent['signal_tags'].str.split('+')).explode('tag')
+        stats_all = df_tags.groupby('tag')['reward'].agg(list).to_dict()
+        stats_recent = df_recent_tags.groupby('tag')['reward'].agg(list).to_dict()
         new_weights = {}
         for tag, pnls in stats_all.items():
             overall_wr = np.mean([1 if p > 0 else 0 for p in pnls]) if pnls else 0.0
-            recent_wr = (
-                np.mean([1 if p > 0 else 0 for p in stats_recent.get(tag, [])])
-                if stats_recent.get(tag)
-                else overall_wr
-            )
+            recent_wr = np.mean([1 if p > 0 else 0 for p in stats_recent.get(tag, [])]) if stats_recent.get(tag) else overall_wr
             weight = 0.7 * recent_wr + 0.3 * overall_wr
             if recent_wr < 0.4:
                 weight *= 0.5
             new_weights[tag] = round(weight, 3)
-
         ALPHA = 0.2
         if os.path.exists(SIGNAL_WEIGHTS_FILE):
             try:
-                old_df = pd.read_csv(
-                    SIGNAL_WEIGHTS_FILE,
-                    on_bad_lines="skip",
-                    engine="python",
-                    usecols=["signal_name", "weight"],
-                )
+                old_df = pd.read_csv(SIGNAL_WEIGHTS_FILE, on_bad_lines='skip', engine='python', usecols=['signal_name', 'weight'])
                 if old_df.empty:
-                    logger.warning("Loaded DataFrame is empty after parsing/fallback")
+                    logger.warning('Loaded DataFrame is empty after parsing/fallback')
                     old = {}
                 else:
-                    old = old_df.set_index("signal_name")["weight"].to_dict()
+                    old = old_df.set_index('signal_name')['weight'].to_dict()
             except ValueError as e:
-                if "usecols" in str(e).lower():
-                    logger.warning(
-                        "Signal weights CSV missing expected columns, trying fallback read"
-                    )
+                if 'usecols' in str(e).lower():
+                    logger.warning('Signal weights CSV missing expected columns, trying fallback read')
                     try:
-                        # Fallback: read all columns and try to map
-                        old_df = pd.read_csv(
-                            SIGNAL_WEIGHTS_FILE, on_bad_lines="skip", engine="python"
-                        )
-                        if "signal" in old_df.columns:
-                            # Old format with 'signal' column
-                            old = old_df.set_index("signal")["weight"].to_dict()
-                        elif "signal_name" in old_df.columns:
-                            # New format with 'signal_name' column
-                            old = old_df.set_index("signal_name")["weight"].to_dict()
+                        old_df = pd.read_csv(SIGNAL_WEIGHTS_FILE, on_bad_lines='skip', engine='python')
+                        if 'signal' in old_df.columns:
+                            old = old_df.set_index('signal')['weight'].to_dict()
+                        elif 'signal_name' in old_df.columns:
+                            old = old_df.set_index('signal_name')['weight'].to_dict()
                         else:
-                            logger.error(
-                                "Signal weights CSV has unexpected format: %s",
-                                old_df.columns.tolist(),
-                            )
+                            logger.error('Signal weights CSV has unexpected format: %s', old_df.columns.tolist())
                             old = {}
                     except Exception as fallback_e:
-                        logger.error(
-                            "Failed to load signal weights with fallback: %s",
-                            fallback_e,
-                        )
+                        logger.error('Failed to load signal weights with fallback: %s', fallback_e)
                         old = {}
                 else:
-                    logger.error("Failed to load signal weights: %s", e)
+                    logger.error('Failed to load signal weights: %s', e)
                     old = {}
         else:
             old = {}
-        merged = {
-            tag: round(ALPHA * w + (1 - ALPHA) * old.get(tag, w), 3)
-            for tag, w in new_weights.items()
-        }
-        out_df = pd.DataFrame.from_dict(
-            merged, orient="index", columns=["weight"]
-        ).reset_index()
-        out_df.columns = ["signal_name", "weight"]
+        merged = {tag: round(ALPHA * w + (1 - ALPHA) * old.get(tag, w), 3) for tag, w in new_weights.items()}
+        out_df = pd.DataFrame.from_dict(merged, orient='index', columns=['weight']).reset_index()
+        out_df.columns = ['signal_name', 'weight']
         out_df.to_csv(SIGNAL_WEIGHTS_FILE, index=False)
-        logger.info("SIGNAL_WEIGHTS_UPDATED", extra={"count": len(merged)})
+        logger.info('SIGNAL_WEIGHTS_UPDATED', extra={'count': len(merged)})
     except Exception as e:
-        logger.exception(f"update_signal_weights failed: {e}")
-
-
-def run_meta_learning_weight_optimizer(
-    trade_log_path: str = TRADE_LOG_FILE,
-    output_path: str = SIGNAL_WEIGHTS_FILE,
-    alpha: float = 1.0,
-):
+        logger.exception(f'update_signal_weights failed: {e}')
+
+def run_meta_learning_weight_optimizer(trade_log_path: str=TRADE_LOG_FILE, output_path: str=SIGNAL_WEIGHTS_FILE, alpha: float=1.0):
     if not meta_lock.acquire(blocking=False):
-        logger.warning("METALEARN_SKIPPED_LOCKED")
+        logger.warning('METALEARN_SKIPPED_LOCKED')
         return
     try:
         if not os.path.exists(trade_log_path):
-            logger.warning("METALEARN_NO_TRADES")
+            logger.warning('METALEARN_NO_TRADES')
             return
-
-        df = pd.read_csv(
-            trade_log_path,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=["entry_price", "exit_price", "signal_tags", "side", "confidence"],
-        ).dropna(subset=["entry_price", "exit_price", "signal_tags"])
+        df = pd.read_csv(trade_log_path, on_bad_lines='skip', engine='python', usecols=['entry_price', 'exit_price', 'signal_tags', 'side', 'confidence']).dropna(subset=['entry_price', 'exit_price', 'signal_tags'])
         if df.empty:
-            logger.warning("Loaded DataFrame is empty after parsing/fallback")
-            logger.warning("METALEARN_NO_VALID_ROWS")
+            logger.warning('Loaded DataFrame is empty after parsing/fallback')
+            logger.warning('METALEARN_NO_VALID_ROWS')
             return
-
-        direction = np.where(df["side"] == "buy", 1, -1)
-        df["pnl"] = (df["exit_price"] - df["entry_price"]) * direction
-        df["confidence"] = df.get("confidence", 0.5)
-        df["reward"] = df["pnl"] * df["confidence"]
-        df["outcome"] = (df["pnl"] > 0).astype(int)
-
-        tags = sorted({tag for row in df["signal_tags"] for tag in row.split("+")})
-        X = np.array(
-            [[int(tag in row.split("+")) for tag in tags] for row in df["signal_tags"]]
-        )
-        y = df["outcome"].values
-
+        direction = np.where(df['side'] == 'buy', 1, -1)
+        df['pnl'] = (df['exit_price'] - df['entry_price']) * direction
+        df['confidence'] = df.get('confidence', 0.5)
+        df['reward'] = df['pnl'] * df['confidence']
+        df['outcome'] = (df['pnl'] > 0).astype(int)
+        tags = sorted({tag for row in df['signal_tags'] for tag in row.split('+')})
+        X = np.array([[int(tag in row.split('+')) for tag in tags] for row in df['signal_tags']])
+        y = df['outcome'].values
         if len(y) < len(tags):
-            logger.warning("METALEARN_TOO_FEW_SAMPLES")
+            logger.warning('METALEARN_TOO_FEW_SAMPLES')
             return
-
-        sample_w = df["reward"].abs() + 1e-3
+        sample_w = df['reward'].abs() + 0.001
         model = Ridge(alpha=alpha, fit_intercept=True)
         if X.empty:
-            logger.warning("META_MODEL_TRAIN_SKIPPED_EMPTY")
+            logger.warning('META_MODEL_TRAIN_SKIPPED_EMPTY')
             return
         model.fit(X, y, sample_weight=sample_w)
         atomic_joblib_dump(model, META_MODEL_PATH)
-        logger.info("META_MODEL_TRAINED", extra={"samples": len(y)})
-        log_metrics(
-            {
-                "timestamp": utc_now_iso(),
-                "type": "meta_model_train",
-                "samples": len(y),
-                "hyperparams": json.dumps({"alpha": alpha}),
-                "seed": SEED,
-                "model": "Ridge",
-                "git_hash": get_git_hash(),
-            }
-        )
-
-        weights = {
-            tag: round(max(0, min(1, w)), 3)
-            for tag, w in zip(tags, model.coef_, strict=False)
-        }
-        out_df = pd.DataFrame(list(weights.items()), columns=["signal_name", "weight"])
+        logger.info('META_MODEL_TRAINED', extra={'samples': len(y)})
+        log_metrics({'timestamp': utc_now_iso(), 'type': 'meta_model_train', 'samples': len(y), 'hyperparams': json.dumps({'alpha': alpha}), 'seed': SEED, 'model': 'Ridge', 'git_hash': get_git_hash()})
+        weights = {tag: round(max(0, min(1, w)), 3) for tag, w in zip(tags, model.coef_, strict=False)}
+        out_df = pd.DataFrame(list(weights.items()), columns=['signal_name', 'weight'])
         out_df.to_csv(output_path, index=False)
-        logger.info("META_WEIGHTS_UPDATED", extra={"weights": weights})
+        logger.info('META_WEIGHTS_UPDATED', extra={'weights': weights})
     finally:
         meta_lock.release()
 
-
-def run_bayesian_meta_learning_optimizer(
-    trade_log_path: str = TRADE_LOG_FILE, output_path: str = SIGNAL_WEIGHTS_FILE
-):
+def run_bayesian_meta_learning_optimizer(trade_log_path: str=TRADE_LOG_FILE, output_path: str=SIGNAL_WEIGHTS_FILE):
     if not meta_lock.acquire(blocking=False):
-        logger.warning("METALEARN_SKIPPED_LOCKED")
+        logger.warning('METALEARN_SKIPPED_LOCKED')
         return
     try:
         if not os.path.exists(trade_log_path):
-            logger.warning("METALEARN_NO_TRADES")
+            logger.warning('METALEARN_NO_TRADES')
             return
-
-        df = pd.read_csv(
-            trade_log_path,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=["entry_price", "exit_price", "signal_tags", "side"],
-        ).dropna(subset=["entry_price", "exit_price", "signal_tags"])
+        df = pd.read_csv(trade_log_path, on_bad_lines='skip', engine='python', usecols=['entry_price', 'exit_price', 'signal_tags', 'side']).dropna(subset=['entry_price', 'exit_price', 'signal_tags'])
         if df.empty:
-            logger.warning("Loaded DataFrame is empty after parsing/fallback")
-            logger.warning("METALEARN_NO_VALID_ROWS")
+            logger.warning('Loaded DataFrame is empty after parsing/fallback')
+            logger.warning('METALEARN_NO_VALID_ROWS')
             return
-
-        direction = np.where(df["side"] == "buy", 1, -1)
-        df["pnl"] = (df["exit_price"] - df["entry_price"]) * direction
-        df["outcome"] = (df["pnl"] > 0).astype(int)
-
-        tags = sorted({tag for row in df["signal_tags"] for tag in row.split("+")})
-        X = np.array(
-            [[int(tag in row.split("+")) for tag in tags] for row in df["signal_tags"]]
-        )
-        y = df["outcome"].values
-
+        direction = np.where(df['side'] == 'buy', 1, -1)
+        df['pnl'] = (df['exit_price'] - df['entry_price']) * direction
+        df['outcome'] = (df['pnl'] > 0).astype(int)
+        tags = sorted({tag for row in df['signal_tags'] for tag in row.split('+')})
+        X = np.array([[int(tag in row.split('+')) for tag in tags] for row in df['signal_tags']])
+        y = df['outcome'].values
         if len(y) < len(tags):
-            logger.warning("METALEARN_TOO_FEW_SAMPLES")
+            logger.warning('METALEARN_TOO_FEW_SAMPLES')
             return
-
         model = BayesianRidge(fit_intercept=True, normalize=True)
         if X.size == 0:
-            logger.warning("BAYES_MODEL_TRAIN_SKIPPED_EMPTY")
+            logger.warning('BAYES_MODEL_TRAIN_SKIPPED_EMPTY')
             return
         model.fit(X, y)
-        atomic_joblib_dump(model, abspath("meta_model_bayes.pkl"))
-        logger.info("META_MODEL_BAYESIAN_TRAINED", extra={"samples": len(y)})
-        log_metrics(
-            {
-                "timestamp": utc_now_iso(),
-                "type": "meta_model_bayes_train",
-                "samples": len(y),
-                "seed": SEED,
-                "model": "BayesianRidge",
-                "git_hash": get_git_hash(),
-            }
-        )
-
-        weights = {
-            tag: round(max(0, min(1, w)), 3)
-            for tag, w in zip(tags, model.coef_, strict=False)
-        }
-        out_df = pd.DataFrame(list(weights.items()), columns=["signal_name", "weight"])
+        atomic_joblib_dump(model, abspath('meta_model_bayes.pkl'))
+        logger.info('META_MODEL_BAYESIAN_TRAINED', extra={'samples': len(y)})
+        log_metrics({'timestamp': utc_now_iso(), 'type': 'meta_model_bayes_train', 'samples': len(y), 'seed': SEED, 'model': 'BayesianRidge', 'git_hash': get_git_hash()})
+        weights = {tag: round(max(0, min(1, w)), 3) for tag, w in zip(tags, model.coef_, strict=False)}
+        out_df = pd.DataFrame(list(weights.items()), columns=['signal_name', 'weight'])
         out_df.to_csv(output_path, index=False)
-        logger.info("META_WEIGHTS_UPDATED", extra={"weights": weights})
+        logger.info('META_WEIGHTS_UPDATED', extra={'weights': weights})
     finally:
         meta_lock.release()
 
-
-def load_global_signal_performance(
-    min_trades: int | None = None, threshold: float | None = None
-) -> dict[str, float] | None:
+def load_global_signal_performance(min_trades: int | None=None, threshold: float | None=None) -> dict[str, float] | None:
     """Load global signal performance with enhanced error handling and configurable thresholds."""
-    # AI-AGENT-REF: Use configurable meta-learning parameters from environment
-    # Reduced requirements to allow meta-learning to activate more easily
     if min_trades is None:
-        min_trades = int(os.getenv("METALEARN_MIN_TRADES", "2"))  # Reduced from 3 to 2
+        min_trades = int(os.getenv('METALEARN_MIN_TRADES', '2'))
     if threshold is None:
-        threshold = float(
-            os.getenv("METALEARN_PERFORMANCE_THRESHOLD", "0.3")
-        )  # Reduced from 0.4 to 0.3
-
+        threshold = float(os.getenv('METALEARN_PERFORMANCE_THRESHOLD', '0.3'))
     if not os.path.exists(TRADE_LOG_FILE):
-        logger.info("METALEARN_NO_HISTORY | Using defaults for new deployment")
+        logger.info('METALEARN_NO_HISTORY | Using defaults for new deployment')
         return None
-
-    try:
-        df = pd.read_csv(
-            TRADE_LOG_FILE,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=["exit_price", "entry_price", "signal_tags", "side"],
-        ).dropna(subset=["exit_price", "entry_price", "signal_tags"])
-
+    try:
+        df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['exit_price', 'entry_price', 'signal_tags', 'side']).dropna(subset=['exit_price', 'entry_price', 'signal_tags'])
         if df.empty:
-            logger.warning("METALEARN_EMPTY_TRADE_LOG - No valid trades found")
+            logger.warning('METALEARN_EMPTY_TRADE_LOG - No valid trades found')
             return {}
-
-        # Enhanced data validation and cleaning
-        df["exit_price"] = pd.to_numeric(df["exit_price"], errors="coerce")
-        df["entry_price"] = pd.to_numeric(df["entry_price"], errors="coerce")
-        df["signal_tags"] = df["signal_tags"].astype(str)
-
-        # Remove rows with invalid price data
-        df = df.dropna(subset=["exit_price", "entry_price"])
+        df['exit_price'] = pd.to_numeric(df['exit_price'], errors='coerce')
+        df['entry_price'] = pd.to_numeric(df['entry_price'], errors='coerce')
+        df['signal_tags'] = df['signal_tags'].astype(str)
+        df = df.dropna(subset=['exit_price', 'entry_price'])
         if df.empty:
-            logger.warning(
-                "METALEARN_INVALID_PRICES - No trades with valid prices. "
-                "This suggests price data corruption or insufficient trading history. "
-                "Using default signal weights.",
-                extra={
-                    "trade_log": TRADE_LOG_FILE,
-                    "suggestion": "Check price data format and trade logging",
-                },
-            )
+            logger.warning('METALEARN_INVALID_PRICES - No trades with valid prices. This suggests price data corruption or insufficient trading history. Using default signal weights.', extra={'trade_log': TRADE_LOG_FILE, 'suggestion': 'Check price data format and trade logging'})
             return {}
-
-        # Calculate PnL with validation
-        direction = np.where(df.side == "buy", 1, -1)
-        df["pnl"] = (df.exit_price - df.entry_price) * direction
-
-        # Enhanced signal tag processing
-        df_tags = df.assign(tag=df.signal_tags.str.split("+")).explode("tag")
-        df_tags = df_tags[
-            df_tags["tag"].notna() & (df_tags["tag"] != "")
-        ]  # Remove empty tags
-
+        direction = np.where(df.side == 'buy', 1, -1)
+        df['pnl'] = (df.exit_price - df.entry_price) * direction
+        df_tags = df.assign(tag=df.signal_tags.str.split('+')).explode('tag')
+        df_tags = df_tags[df_tags['tag'].notna() & (df_tags['tag'] != '')]
         if df_tags.empty:
-            logger.warning("METALEARN_NO_SIGNAL_TAGS - No valid signal tags found")
+            logger.warning('METALEARN_NO_SIGNAL_TAGS - No valid signal tags found')
             return {}
-
-        # Calculate win rates with minimum trade validation
         win_rates = {}
-        tag_groups = df_tags.groupby("tag")
-
+        tag_groups = df_tags.groupby('tag')
         for tag, group in tag_groups:
             if len(group) >= min_trades:
-                win_rate = (group["pnl"] > 0).mean()
+                win_rate = (group['pnl'] > 0).mean()
                 win_rates[tag] = round(win_rate, 3)
-
         if not win_rates:
-            logger.warning(
-                "METALEARN_INSUFFICIENT_TRADES - No signals meet minimum trade requirement (%d)",
-                min_trades,
-            )
+            logger.warning('METALEARN_INSUFFICIENT_TRADES - No signals meet minimum trade requirement (%d)', min_trades)
             return {}
-
-        # Filter by performance threshold
         filtered = {tag: wr for tag, wr in win_rates.items() if wr >= threshold}
-
-        # Enhanced logging with more details
-        logger.info(
-            "METALEARN_FILTERED_SIGNALS",
-            extra={
-                "signals": list(filtered.keys()) or [],
-                "total_signals_analyzed": len(win_rates),
-                "signals_above_threshold": len(filtered),
-                "threshold": threshold,
-                "min_trades": min_trades,
-                "total_trades": len(df),
-            },
-        )
-
+        logger.info('METALEARN_FILTERED_SIGNALS', extra={'signals': list(filtered.keys()) or [], 'total_signals_analyzed': len(win_rates), 'signals_above_threshold': len(filtered), 'threshold': threshold, 'min_trades': min_trades, 'total_trades': len(df)})
         if not filtered:
-            logger.warning(
-                "METALEARN_NO_SIGNALS_ABOVE_THRESHOLD - No signals above threshold %.3f",
-                threshold,
-            )
-            # Return best performing signals even if below threshold, with reduced weight
+            logger.warning('METALEARN_NO_SIGNALS_ABOVE_THRESHOLD - No signals above threshold %.3f', threshold)
             if win_rates:
                 best_signal = max(win_rates.items(), key=lambda x: x[1])
-                logger.info(
-                    "METALEARN_FALLBACK_SIGNAL - Using best signal: %s (%.3f)",
-                    best_signal[0],
-                    best_signal[1],
-                )
-                return {best_signal[0]: best_signal[1] * 0.5}  # Reduced confidence
-
+                logger.info('METALEARN_FALLBACK_SIGNAL - Using best signal: %s (%.3f)', best_signal[0], best_signal[1])
+                return {best_signal[0]: best_signal[1] * 0.5}
         return filtered
-
     except Exception as e:
-        logger.error(
-            "METALEARN_PROCESSING_ERROR - Failed to process signal performance: %s",
-            e,
-            exc_info=True,
-        )
+        logger.error('METALEARN_PROCESSING_ERROR - Failed to process signal performance: %s', e, exc_info=True)
         return {}
-
 
 def _normalize_index(data: pd.DataFrame) -> pd.DataFrame:
     """Return ``data`` with a clean UTC index named ``date``."""
     if data.index.name:
-        data = data.reset_index().rename(columns={data.index.name: "date"})
+        data = data.reset_index().rename(columns={data.index.name: 'date'})
     else:
-        data = data.reset_index().rename(columns={"index": "date"})
-    data["date"] = pd.to_datetime(data["date"], utc=True)
-    data = data.sort_values("date").set_index("date")
+        data = data.reset_index().rename(columns={'index': 'date'})
+    data['date'] = pd.to_datetime(data['date'], utc=True)
+    data = data.sort_values('date').set_index('date')
     if data.index.tz is not None:
-        data.index = data.index.tz_convert("UTC").tz_localize(None)
+        data.index = data.index.tz_convert('UTC').tz_localize(None)
     return data
 
-
-def _add_basic_indicators(
-    df: pd.DataFrame, symbol: str, state: BotState | None
-) -> None:
+def _add_basic_indicators(df: pd.DataFrame, symbol: str, state: BotState | None) -> None:
     """Add VWAP, RSI, ATR and simple moving averages."""
     try:
-        df["vwap"] = ta.vwap(df["high"], df["low"], df["close"], df["volume"])
+        df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])
     except Exception as exc:
-        log_warning("INDICATOR_VWAP_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_VWAP_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["vwap"] = np.nan
-    try:
-        df["rsi"] = ta.rsi(df["close"], length=14)
+        df['vwap'] = np.nan
+    try:
+        df['rsi'] = ta.rsi(df['close'], length=14)
     except Exception as exc:
-        log_warning("INDICATOR_RSI_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_RSI_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["rsi"] = np.nan
-    try:
-        df["atr"] = ta.atr(df["high"], df["low"], df["close"], length=14)
+        df['rsi'] = np.nan
+    try:
+        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)
     except Exception as exc:
-        log_warning("INDICATOR_ATR_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_ATR_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["atr"] = np.nan
-    if "close" not in df.columns:
+        df['atr'] = np.nan
+    if 'close' not in df.columns:
         raise KeyError("'close' column missing for SMA calculations")
-    close = df["close"].dropna()
+    close = df['close'].dropna()
     if close.empty:
-        raise ValueError("No close price data available for SMA calculations")
-    df["sma_50"] = close.astype(float).rolling(window=50).mean()
-    df["sma_200"] = close.astype(float).rolling(window=200).mean()
-
+        raise ValueError('No close price data available for SMA calculations')
+    df['sma_50'] = close.astype(float).rolling(window=50).mean()
+    df['sma_200'] = close.astype(float).rolling(window=200).mean()
 
 def _add_macd(df: pd.DataFrame, symbol: str, state: BotState | None) -> None:
     """Add MACD indicators using the defensive helper."""
-    # ai_trading/core/bot_engine.py:7337 - Convert import guard to hard import (internal module)
-    from ai_trading.signals import (
-        calculate_macd as signals_calculate_macd,  # type: ignore
-    )
-
-    try:
-        if "close" not in df.columns:
+    from ai_trading.signals import calculate_macd as signals_calculate_macd
+    try:
+        if 'close' not in df.columns:
             raise KeyError("'close' column missing for MACD calculation")
-        close_series = df["close"].dropna()
+        close_series = df['close'].dropna()
         if close_series.empty:
-            raise ValueError("No close price data available for MACD")
+            raise ValueError('No close price data available for MACD')
         macd_df = signals_calculate_macd(close_series)
         if macd_df is None:
-            logger.warning("MACD returned None for %s", symbol)
-            raise ValueError("MACD calculation returned None")
-        macd_col = macd_df.get("macd")
-        signal_col = macd_df.get("signal")
+            logger.warning('MACD returned None for %s', symbol)
+            raise ValueError('MACD calculation returned None')
+        macd_col = macd_df.get('macd')
+        signal_col = macd_df.get('signal')
         if macd_col is None or signal_col is None:
-            raise KeyError("MACD dataframe missing required columns")
-        df["macd"] = macd_col.astype(float)
-        df["macds"] = signal_col.astype(float)
+            raise KeyError('MACD dataframe missing required columns')
+        df['macd'] = macd_col.astype(float)
+        df['macds'] = signal_col.astype(float)
     except Exception as exc:
-        log_warning(
-            "INDICATOR_MACD_FAIL",
-            exc=exc,
-            extra={"symbol": symbol, "snapshot": df["close"].tail(5).to_dict()},
-        )
+        log_warning('INDICATOR_MACD_FAIL', exc=exc, extra={'symbol': symbol, 'snapshot': df['close'].tail(5).to_dict()})
         if state:
             state.indicator_failures += 1
-        df["macd"] = np.nan
-        df["macds"] = np.nan
-
-
-def _add_additional_indicators(
-    df: pd.DataFrame, symbol: str, state: BotState | None
-) -> None:
+        df['macd'] = np.nan
+        df['macds'] = np.nan
+
+def _add_additional_indicators(df: pd.DataFrame, symbol: str, state: BotState | None) -> None:
     """Add a suite of secondary technical indicators."""
-    # dedupe any duplicate timestamps
-    df = df[~df.index.duplicated(keep="first")]
-    try:
-        kc = ta.kc(df["high"], df["low"], df["close"], length=20)
-        df["kc_lower"] = kc.iloc[:, 0]
-        df["kc_mid"] = kc.iloc[:, 1]
-        df["kc_upper"] = kc.iloc[:, 2]
+    df = df[~df.index.duplicated(keep='first')]
+    try:
+        kc = ta.kc(df['high'], df['low'], df['close'], length=20)
+        df['kc_lower'] = kc.iloc[:, 0]
+        df['kc_mid'] = kc.iloc[:, 1]
+        df['kc_upper'] = kc.iloc[:, 2]
     except Exception as exc:
-        log_warning("INDICATOR_KC_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_KC_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["kc_lower"] = np.nan
-        df["kc_mid"] = np.nan
-        df["kc_upper"] = np.nan
-
-    df["atr_band_upper"] = df["close"] + 1.5 * df["atr"]
-    df["atr_band_lower"] = df["close"] - 1.5 * df["atr"]
-    df["avg_vol_20"] = df["volume"].rolling(20).mean()
-    df["dow"] = df.index.dayofweek
-
-    try:
-        bb = ta.bbands(df["close"], length=20)
-        df["bb_upper"] = bb["BBU_20_2.0"]
-        df["bb_lower"] = bb["BBL_20_2.0"]
-        df["bb_percent"] = bb["BBP_20_2.0"]
+        df['kc_lower'] = np.nan
+        df['kc_mid'] = np.nan
+        df['kc_upper'] = np.nan
+    df['atr_band_upper'] = df['close'] + 1.5 * df['atr']
+    df['atr_band_lower'] = df['close'] - 1.5 * df['atr']
+    df['avg_vol_20'] = df['volume'].rolling(20).mean()
+    df['dow'] = df.index.dayofweek
+    try:
+        bb = ta.bbands(df['close'], length=20)
+        df['bb_upper'] = bb['BBU_20_2.0']
+        df['bb_lower'] = bb['BBL_20_2.0']
+        df['bb_percent'] = bb['BBP_20_2.0']
     except Exception as exc:
-        log_warning("INDICATOR_BBANDS_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_BBANDS_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["bb_upper"] = np.nan
-        df["bb_lower"] = np.nan
-        df["bb_percent"] = np.nan
-
-    try:
-        adx = ta.adx(df["high"], df["low"], df["close"], length=14)
-        df["adx"] = adx["ADX_14"]
-        df["dmp"] = adx["DMP_14"]
-        df["dmn"] = adx["DMN_14"]
+        df['bb_upper'] = np.nan
+        df['bb_lower'] = np.nan
+        df['bb_percent'] = np.nan
+    try:
+        adx = ta.adx(df['high'], df['low'], df['close'], length=14)
+        df['adx'] = adx['ADX_14']
+        df['dmp'] = adx['DMP_14']
+        df['dmn'] = adx['DMN_14']
     except Exception as exc:
-        log_warning("INDICATOR_ADX_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_ADX_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["adx"] = np.nan
-        df["dmp"] = np.nan
-        df["dmn"] = np.nan
-
-    try:
-        df["cci"] = ta.cci(df["high"], df["low"], df["close"], length=20)
+        df['adx'] = np.nan
+        df['dmp'] = np.nan
+        df['dmn'] = np.nan
+    try:
+        df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20)
     except Exception as exc:
-        log_warning("INDICATOR_CCI_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_CCI_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["cci"] = np.nan
-
-    df[["high", "low", "close", "volume"]] = df[
-        ["high", "low", "close", "volume"]
-    ].astype(float)
+        df['cci'] = np.nan
+    df[['high', 'low', 'close', 'volume']] = df[['high', 'low', 'close', 'volume']].astype(float)
     try:
         mfi_vals = ta.mfi(df.high, df.low, df.close, df.volume, length=14)
-        df["+mfi"] = mfi_vals
+        df['+mfi'] = mfi_vals
     except ValueError:
-        logger.warning("Skipping MFI: insufficient or duplicate data")
-
-    try:
-        df["tema"] = ta.tema(df["close"], length=10)
+        logger.warning('Skipping MFI: insufficient or duplicate data')
+    try:
+        df['tema'] = ta.tema(df['close'], length=10)
     except Exception as exc:
-        log_warning("INDICATOR_TEMA_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_TEMA_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["tema"] = np.nan
-
-    try:
-        df["willr"] = ta.willr(df["high"], df["low"], df["close"], length=14)
+        df['tema'] = np.nan
+    try:
+        df['willr'] = ta.willr(df['high'], df['low'], df['close'], length=14)
     except Exception as exc:
-        log_warning("INDICATOR_WILLR_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_WILLR_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["willr"] = np.nan
-
-    try:
-        psar = ta.psar(df["high"], df["low"], df["close"])
-        df["psar_long"] = psar["PSARl_0.02_0.2"]
-        df["psar_short"] = psar["PSARs_0.02_0.2"]
+        df['willr'] = np.nan
+    try:
+        psar = ta.psar(df['high'], df['low'], df['close'])
+        df['psar_long'] = psar['PSARl_0.02_0.2']
+        df['psar_short'] = psar['PSARs_0.02_0.2']
     except Exception as exc:
-        log_warning("INDICATOR_PSAR_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_PSAR_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["psar_long"] = np.nan
-        df["psar_short"] = np.nan
-
-    try:
-        # compute_ichimoku returns the indicator dataframe and the signal dataframe
-        ich_df, ich_signal_df = compute_ichimoku(df["high"], df["low"], df["close"])
+        df['psar_long'] = np.nan
+        df['psar_short'] = np.nan
+    try:
+        ich_df, ich_signal_df = compute_ichimoku(df['high'], df['low'], df['close'])
         for col in ich_df.columns:
-            df[f"ich_{col}"] = ich_df[col]
+            df[f'ich_{col}'] = ich_df[col]
         for col in ich_signal_df.columns:
-            df[f"ichi_signal_{col}"] = ich_signal_df[col]
+            df[f'ichi_signal_{col}'] = ich_signal_df[col]
     except (KeyError, IndexError):
-        logger.warning("Skipping Ichimoku: empty or irregular index")
-
-    try:
-        st = ta.stochrsi(df["close"])
-        df["stochrsi"] = st["STOCHRSIk_14_14_3_3"]
+        logger.warning('Skipping Ichimoku: empty or irregular index')
+    try:
+        st = ta.stochrsi(df['close'])
+        df['stochrsi'] = st['STOCHRSIk_14_14_3_3']
     except Exception as exc:
-        log_warning("INDICATOR_STOCHRSI_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_STOCHRSI_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["stochrsi"] = np.nan
-
-
-def _add_multi_timeframe_features(
-    df: pd.DataFrame, symbol: str, state: BotState | None
-) -> None:
+        df['stochrsi'] = np.nan
+
+def _add_multi_timeframe_features(df: pd.DataFrame, symbol: str, state: BotState | None) -> None:
     """Add multi-timeframe and lag-based features."""
     try:
-        df["ret_5m"] = df["close"].pct_change(5, fill_method=None)
-        df["ret_1h"] = df["close"].pct_change(60, fill_method=None)
-        df["ret_d"] = df["close"].pct_change(390, fill_method=None)
-        df["ret_w"] = df["close"].pct_change(1950, fill_method=None)
-        df["vol_norm"] = (
-            df["volume"].rolling(60).mean() / df["volume"].rolling(5).mean()
-        )
-        df["5m_vs_1h"] = df["ret_5m"] - df["ret_1h"]
-        df["vol_5m"] = df["close"].pct_change(fill_method=None).rolling(5).std()
-        df["vol_1h"] = df["close"].pct_change(fill_method=None).rolling(60).std()
-        df["vol_d"] = df["close"].pct_change(fill_method=None).rolling(390).std()
-        df["vol_w"] = df["close"].pct_change(fill_method=None).rolling(1950).std()
-        df["vol_ratio"] = df["vol_5m"] / df["vol_1h"]
-        df["mom_agg"] = df["ret_5m"] + df["ret_1h"] + df["ret_d"]
-        df["lag_close_1"] = df["close"].shift(1)
-        df["lag_close_3"] = df["close"].shift(3)
+        df['ret_5m'] = df['close'].pct_change(5, fill_method=None)
+        df['ret_1h'] = df['close'].pct_change(60, fill_method=None)
+        df['ret_d'] = df['close'].pct_change(390, fill_method=None)
+        df['ret_w'] = df['close'].pct_change(1950, fill_method=None)
+        df['vol_norm'] = df['volume'].rolling(60).mean() / df['volume'].rolling(5).mean()
+        df['5m_vs_1h'] = df['ret_5m'] - df['ret_1h']
+        df['vol_5m'] = df['close'].pct_change(fill_method=None).rolling(5).std()
+        df['vol_1h'] = df['close'].pct_change(fill_method=None).rolling(60).std()
+        df['vol_d'] = df['close'].pct_change(fill_method=None).rolling(390).std()
+        df['vol_w'] = df['close'].pct_change(fill_method=None).rolling(1950).std()
+        df['vol_ratio'] = df['vol_5m'] / df['vol_1h']
+        df['mom_agg'] = df['ret_5m'] + df['ret_1h'] + df['ret_d']
+        df['lag_close_1'] = df['close'].shift(1)
+        df['lag_close_3'] = df['close'].shift(3)
     except Exception as exc:
-        log_warning("INDICATOR_MULTITF_FAIL", exc=exc, extra={"symbol": symbol})
+        log_warning('INDICATOR_MULTITF_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        df["ret_5m"] = df["ret_1h"] = df["ret_d"] = df["ret_w"] = np.nan
-        df["vol_norm"] = df["5m_vs_1h"] = np.nan
-        df["vol_5m"] = df["vol_1h"] = df["vol_d"] = df["vol_w"] = np.nan
-        df["vol_ratio"] = df["mom_agg"] = df["lag_close_1"] = df["lag_close_3"] = np.nan
-
+        df['ret_5m'] = df['ret_1h'] = df['ret_d'] = df['ret_w'] = np.nan
+        df['vol_norm'] = df['5m_vs_1h'] = np.nan
+        df['vol_5m'] = df['vol_1h'] = df['vol_d'] = df['vol_w'] = np.nan
+        df['vol_ratio'] = df['mom_agg'] = df['lag_close_1'] = df['lag_close_3'] = np.nan
 
 def _drop_inactive_features(df: pd.DataFrame) -> None:
     """Remove features listed in ``INACTIVE_FEATURES_FILE`` if present."""
@@ -7505,50 +4660,27 @@         try:
             with open(INACTIVE_FEATURES_FILE) as f:
                 inactive = set(json.load(f))
-            df.drop(
-                columns=[c for c in inactive if c in df.columns],
-                inplace=True,
-                errors="ignore",
-            )
-        except Exception as exc:  # pragma: no cover - unexpected I/O
-            logger.exception("bot.py unexpected", exc_info=exc)
+            df.drop(columns=[c for c in inactive if c in df.columns], inplace=True, errors='ignore')
+        except Exception as exc:
+            logger.exception('bot.py unexpected', exc_info=exc)
             raise
-
 
 @profile
 def prepare_indicators(frame: pd.DataFrame) -> pd.DataFrame:
-    # Calculate RSI and assign to both rsi and rsi_14
-    frame["rsi"] = ta.rsi(frame["close"], length=14)
-    frame["rsi_14"] = frame["rsi"]
-
-    # Ichimoku conversion and base lines
-    frame["ichimoku_conv"] = (
-        frame["high"].rolling(window=9).max() + frame["low"].rolling(window=9).min()
-    ) / 2
-    frame["ichimoku_base"] = (
-        frame["high"].rolling(window=26).max() + frame["low"].rolling(window=26).min()
-    ) / 2
-
-    # Stochastic RSI calculation
-    rsi_min = frame["rsi_14"].rolling(window=14).min()
-    rsi_max = frame["rsi_14"].rolling(window=14).max()
-    frame["stochrsi"] = (frame["rsi_14"] - rsi_min) / (rsi_max - rsi_min)
-
-    # Guarantee all required columns exist
-    required = ["ichimoku_conv", "ichimoku_base", "stochrsi"]
+    frame['rsi'] = ta.rsi(frame['close'], length=14)
+    frame['rsi_14'] = frame['rsi']
+    frame['ichimoku_conv'] = (frame['high'].rolling(window=9).max() + frame['low'].rolling(window=9).min()) / 2
+    frame['ichimoku_base'] = (frame['high'].rolling(window=26).max() + frame['low'].rolling(window=26).min()) / 2
+    rsi_min = frame['rsi_14'].rolling(window=14).min()
+    rsi_max = frame['rsi_14'].rolling(window=14).max()
+    frame['stochrsi'] = (frame['rsi_14'] - rsi_min) / (rsi_max - rsi_min)
+    required = ['ichimoku_conv', 'ichimoku_base', 'stochrsi']
     for col in required:
         if col not in frame.columns:
             frame[col] = np.nan
-
-    # Only drop rows where all are missing
-    frame.dropna(subset=required, how="all", inplace=True)
-
+    frame.dropna(subset=required, how='all', inplace=True)
     return frame
 
-
-# --- Back-compat shim for tests that expect this symbol at module scope ---
-# This is a thin wrapper to ensure AST-based tests can find prepare_indicators
-# at module scope even if the implementation changes.
 def prepare_indicators_compat(*args, **kwargs):
     """
     Back-compat wrapper. Delegates to the current implementation.
@@ -7556,192 +4688,124 @@     """
     return prepare_indicators(*args, **kwargs)
 
-
 def _compute_regime_features(df: pd.DataFrame) -> pd.DataFrame:
     """Compute regime features; tolerate proxy bars that only include 'close'."""
-    # 1) Canonicalize columns (o/h/l/c/v mapping, lowercase)
     try:
         from ai_trading.utils.ohlcv import standardize_ohlcv
-
         df = standardize_ohlcv(df)
     except Exception as e:
-        # OHLCV standardization failed - log warning but continue with raw data
-        logger.warning("Failed to standardize OHLCV data: %s", e)
-
-    # 2) Synthesize missing OHLC from 'close' when needed (proxy baskets)
-    if "close" in df.columns:
-        if "high" not in df.columns:
-            df["high"] = df["close"].rolling(3, min_periods=1).max()
-        if "low" not in df.columns:
-            df["low"] = df["close"].rolling(3, min_periods=1).min()
-        if "open" not in df.columns:
-            df["open"] = df["close"].shift(1).fillna(df["close"])
-        if "volume" not in df.columns:
-            df["volume"] = 0.0
-
-    # 3) Optional MACD import (keep failures soft)
-    try:
-        from ai_trading.signals import (
-            calculate_macd as signals_calculate_macd,  # type: ignore
-        )
+        logger.warning('Failed to standardize OHLCV data: %s', e)
+    if 'close' in df.columns:
+        if 'high' not in df.columns:
+            df['high'] = df['close'].rolling(3, min_periods=1).max()
+        if 'low' not in df.columns:
+            df['low'] = df['close'].rolling(3, min_periods=1).min()
+        if 'open' not in df.columns:
+            df['open'] = df['close'].shift(1).fillna(df['close'])
+        if 'volume' not in df.columns:
+            df['volume'] = 0.0
+    try:
+        from ai_trading.signals import calculate_macd as signals_calculate_macd
     except Exception:
-        logger.warning("signals module not available for regime features")
+        logger.warning('signals module not available for regime features')
         signals_calculate_macd = None
-
-    # 4) Build features with fallbacks
     feat = pd.DataFrame(index=df.index)
     try:
-        feat["atr"] = ta.atr(df["high"], df["low"], df["close"], length=14)
+        feat['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)
     except Exception:
-        # Fallback ATR proxy from close-to-close movement
-        tr = (df["close"].diff().abs()).fillna(0.0)
-        feat["atr"] = tr.rolling(14, min_periods=1).mean()
-    feat["rsi"] = ta.rsi(df["close"], length=14)
+        tr = df['close'].diff().abs().fillna(0.0)
+        feat['atr'] = tr.rolling(14, min_periods=1).mean()
+    feat['rsi'] = ta.rsi(df['close'], length=14)
     if signals_calculate_macd:
         try:
-            macd_df = signals_calculate_macd(df["close"])
-            feat["macd"] = (
-                macd_df["macd"] if macd_df is not None and "macd" in macd_df else np.nan
-            )
+            macd_df = signals_calculate_macd(df['close'])
+            feat['macd'] = macd_df['macd'] if macd_df is not None and 'macd' in macd_df else np.nan
         except Exception as e:
-            logger.warning("Regime MACD calculation failed: %s", e)
-            feat["macd"] = np.nan
+            logger.warning('Regime MACD calculation failed: %s', e)
+            feat['macd'] = np.nan
     else:
-        feat["macd"] = np.nan
-    feat["vol"] = (
-        df["close"].pct_change(fill_method=None).rolling(14, min_periods=1).std()
-    )
-    return feat.dropna(how="all")
-
+        feat['macd'] = np.nan
+    feat['vol'] = df['close'].pct_change(fill_method=None).rolling(14, min_periods=1).std()
+    return feat.dropna(how='all')
 
 def detect_regime(df: pd.DataFrame) -> str:
     """Simple SMA-based market regime detection."""
-    if df is None or df.empty or "close" not in df:
-        return "chop"
-    close = df["close"].astype(float)
+    if df is None or df.empty or 'close' not in df:
+        return 'chop'
+    close = df['close'].astype(float)
     sma50 = close.rolling(50).mean()
     sma200 = close.rolling(200).mean()
     if sma50.iloc[-1] > sma200.iloc[-1]:
-        return "bull"
+        return 'bull'
     if sma50.iloc[-1] < sma200.iloc[-1]:
-        return "bear"
-    return "chop"
-
+        return 'bear'
+    return 'chop'
 
 def _initialize_regime_model(ctx=None):
     """Initialize regime model - load existing or train new one."""
-    # Train or load regime model - skip in test environment
-    if os.getenv("TESTING") == "1" or os.getenv("PYTEST_RUNNING"):
-        logger.info("Skipping regime model training in test environment")
-        return RandomForestClassifier(
-            n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-        )
+    if os.getenv('TESTING') == '1' or os.getenv('PYTEST_RUNNING'):
+        logger.info('Skipping regime model training in test environment')
+        return RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
     elif os.path.exists(REGIME_MODEL_PATH):
         try:
-            with open(REGIME_MODEL_PATH, "rb") as f:
+            with open(REGIME_MODEL_PATH, 'rb') as f:
                 return pickle.load(f)
         except Exception as e:
-            logger.warning(f"Failed to load regime model: {e}")
-            return RandomForestClassifier(
-                n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-            )
+            logger.warning(f'Failed to load regime model: {e}')
+            return RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
     else:
         if ctx is None:
-            logger.warning(
-                "No context provided for regime model training; using fallback"
-            )
-            return RandomForestClassifier(
-                n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-            )
-
-        # --- Regime training uses basket-based proxy now ---
+            logger.warning('No context provided for regime model training; using fallback')
+            return RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
         wide = _build_regime_dataset(ctx)
-        if wide is None or getattr(wide, "empty", False):
-            logger.warning("Regime basket is empty; skipping model train")
+        if wide is None or getattr(wide, 'empty', False):
+            logger.warning('Regime basket is empty; skipping model train')
             bars = pd.DataFrame()
         else:
             bars = _regime_basket_to_proxy_bars(wide)
-
-        # Normalize to a DatetimeIndex robustly (proxy has 'timestamp' column)
         try:
-            if "timestamp" in getattr(bars, "columns", []):
-                idx = safe_to_datetime(bars["timestamp"], context="regime data")
-                bars = bars.drop(columns=["timestamp"])
+            if 'timestamp' in getattr(bars, 'columns', []):
+                idx = safe_to_datetime(bars['timestamp'], context='regime data')
+                bars = bars.drop(columns=['timestamp'])
                 bars.index = idx
-            # Final conversion (idempotent for Timestamps)
-            bars.index = safe_to_datetime(bars.index, context="regime data")
+            bars.index = safe_to_datetime(bars.index, context='regime data')
         except Exception as e:
-            logger.warning("REGIME index normalization failed: %s", e)
+            logger.warning('REGIME index normalization failed: %s', e)
             bars = pd.DataFrame()
         bars = bars.rename(columns=lambda c: c.lower())
         feats = _compute_regime_features(bars)
-        labels = (
-            (bars["close"] > bars["close"].rolling(200).mean())
-            .loc[feats.index]
-            .astype(int)
-            .rename("label")
-        )
-        training = feats.join(labels, how="inner").dropna()
-
-        # Add validation for training data quality
+        labels = (bars['close'] > bars['close'].rolling(200).mean()).loc[feats.index].astype(int).rename('label')
+        training = feats.join(labels, how='inner').dropna()
         if training.empty:
-            logger.warning(
-                "Regime training dataset is empty after joining features and labels"
-            )
-            if not _REGIME_INSUFFICIENT_DATA_WARNED["done"]:
-                logger.warning(
-                    "No valid training data for regime model; using fallback"
-                )
-                _REGIME_INSUFFICIENT_DATA_WARNED["done"] = True
-            return RandomForestClassifier(
-                n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-            )
-
-        # Import settings for regime configuration
+            logger.warning('Regime training dataset is empty after joining features and labels')
+            if not _REGIME_INSUFFICIENT_DATA_WARNED['done']:
+                logger.warning('No valid training data for regime model; using fallback')
+                _REGIME_INSUFFICIENT_DATA_WARNED['done'] = True
+            return RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
         from ai_trading.config.settings import get_settings
-
         settings = get_settings()
-
-        logger.debug(
-            "Regime training data validation: %d rows available, minimum required: %d",
-            len(training),
-            settings.REGIME_MIN_ROWS,
-        )
-
+        logger.debug('Regime training data validation: %d rows available, minimum required: %d', len(training), settings.REGIME_MIN_ROWS)
         if len(training) >= settings.REGIME_MIN_ROWS:
-            X = training[["atr", "rsi", "macd", "vol"]]
-            y = training["label"]
-            regime_model = RandomForestClassifier(
-                n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-            )
+            X = training[['atr', 'rsi', 'macd', 'vol']]
+            y = training['label']
+            regime_model = RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
             if X.empty:
-                logger.warning("REGIME_MODEL_TRAIN_SKIPPED_EMPTY")
+                logger.warning('REGIME_MODEL_TRAIN_SKIPPED_EMPTY')
             else:
                 regime_model.fit(X, y)
             try:
                 atomic_pickle_dump(regime_model, REGIME_MODEL_PATH)
             except Exception as e:
-                logger.warning(f"Failed to save regime model: {e}")
+                logger.warning(f'Failed to save regime model: {e}')
             else:
-                logger.info("REGIME_MODEL_TRAINED", extra={"rows": len(training)})
+                logger.info('REGIME_MODEL_TRAINED', extra={'rows': len(training)})
             return regime_model
         else:
-            # Log once at WARNING level; avoid noisy ERROR during closed market.
-            if not _REGIME_INSUFFICIENT_DATA_WARNED["done"]:
-                logger.warning(
-                    "Insufficient rows (%d < %d) for regime model; using fallback",
-                    len(training),
-                    settings.REGIME_MIN_ROWS,
-                )
-                _REGIME_INSUFFICIENT_DATA_WARNED["done"] = True
-            return RandomForestClassifier(
-                n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH
-            )
-
-
-# Initialize regime model lazily
+            if not _REGIME_INSUFFICIENT_DATA_WARNED['done']:
+                logger.warning('Insufficient rows (%d < %d) for regime model; using fallback', len(training), settings.REGIME_MIN_ROWS)
+                _REGIME_INSUFFICIENT_DATA_WARNED['done'] = True
+            return RandomForestClassifier(n_estimators=RF_ESTIMATORS, max_depth=RF_MAX_DEPTH)
 regime_model = None
-
 
 def _market_breadth(ctx: BotContext) -> float:
     syms = load_tickers(TICKERS_FILE)[:20]
@@ -7752,40 +4816,35 @@         if df is None or len(df) < 2:
             continue
         total += 1
-        if df["close"].iloc[-1] > df["close"].iloc[-2]:
+        if df['close'].iloc[-1] > df['close'].iloc[-2]:
             up += 1
     return up / total if total else 0.5
-
 
 def detect_regime_state(ctx: BotContext) -> str:
     df = ctx.data_fetcher.get_daily_df(ctx, REGIME_SYMBOLS[0])
     if df is None or len(df) < 200:
-        return "sideways"
-    atr14 = ta.atr(df["high"], df["low"], df["close"], length=14).iloc[-1]
-    atr50 = ta.atr(df["high"], df["low"], df["close"], length=50).iloc[-1]
+        return 'sideways'
+    atr14 = ta.atr(df['high'], df['low'], df['close'], length=14).iloc[-1]
+    atr50 = ta.atr(df['high'], df['low'], df['close'], length=50).iloc[-1]
     high_vol = atr50 > 0 and atr14 / atr50 > 1.5
-    sma50 = df["close"].rolling(50).mean().iloc[-1]
-    sma200 = df["close"].rolling(200).mean().iloc[-1]
+    sma50 = df['close'].rolling(50).mean().iloc[-1]
+    sma200 = df['close'].rolling(200).mean().iloc[-1]
     trend = sma50 - sma200
     breadth = _market_breadth(ctx)
     if high_vol:
-        return "high_volatility"
+        return 'high_volatility'
     if abs(trend) / sma200 < 0.005:
-        return "sideways"
+        return 'sideways'
     if trend > 0 and breadth > 0.55:
-        return "trending"
+        return 'trending'
     if trend < 0 and breadth < 0.45:
-        return "mean_reversion"
-    return "sideways"
-
+        return 'mean_reversion'
+    return 'sideways'
 
 def check_market_regime(state: BotState) -> bool:
     state.current_regime = detect_regime_state(ctx)
     return True
-
-
 _SCREEN_CACHE: dict[str, float] = {}
-
 
 def _validate_market_data_quality(df: pd.DataFrame, symbol: str) -> dict:
     """
@@ -7798,261 +4857,104 @@         dict: Validation result with valid flag, reason, and detailed message
     """
     try:
-        # Basic existence check
         if df is None:
-            return {
-                "valid": False,
-                "reason": "no_data",
-                "message": "No data available",
-                "details": {"symbol": symbol, "data_source": "missing"},
-            }
-
-        # Minimum rows requirement
-        min_rows_required = max(
-            ATR_LENGTH, 20
-        )  # Ensure enough for technical indicators
+            return {'valid': False, 'reason': 'no_data', 'message': 'No data available', 'details': {'symbol': symbol, 'data_source': 'missing'}}
+        min_rows_required = max(ATR_LENGTH, 20)
         if len(df) < min_rows_required:
-            return {
-                "valid": False,
-                "reason": f"insufficient_data_{len(df)}_rows",
-                "message": f"Insufficient data ({len(df)} rows, need {min_rows_required})",
-                "details": {
-                    "symbol": symbol,
-                    "rows_available": len(df),
-                    "rows_required": min_rows_required,
-                },
-            }
-
-        # Data completeness checks
-        required_columns = ["open", "high", "low", "close", "volume"]
+            return {'valid': False, 'reason': f'insufficient_data_{len(df)}_rows', 'message': f'Insufficient data ({len(df)} rows, need {min_rows_required})', 'details': {'symbol': symbol, 'rows_available': len(df), 'rows_required': min_rows_required}}
+        required_columns = ['open', 'high', 'low', 'close', 'volume']
         missing_columns = [col for col in required_columns if col not in df.columns]
         if missing_columns:
-            return {
-                "valid": False,
-                "reason": "missing_columns",
-                "message": f"Missing required columns: {missing_columns}",
-                "details": {
-                    "symbol": symbol,
-                    "missing_columns": missing_columns,
-                    "available_columns": list(df.columns),
-                },
-            }
-
-        # Data quality checks
-        recent_data = df.tail(min(50, len(df)))  # Check last 50 rows or all available
-
-        # Check for excessive NaN values
+            return {'valid': False, 'reason': 'missing_columns', 'message': f'Missing required columns: {missing_columns}', 'details': {'symbol': symbol, 'missing_columns': missing_columns, 'available_columns': list(df.columns)}}
+        recent_data = df.tail(min(50, len(df)))
         for col in required_columns:
             nan_count = recent_data[col].isna().sum()
-            nan_percentage = (nan_count / len(recent_data)) * 100
-            if nan_percentage > 10:  # More than 10% NaN values
-                return {
-                    "valid": False,
-                    "reason": f"excessive_nan_{col}",
-                    "message": f"Excessive NaN values in {col} column ({nan_percentage:.1f}%)",
-                    "details": {
-                        "symbol": symbol,
-                        "column": col,
-                        "nan_percentage": nan_percentage,
-                    },
-                }
-
-        # Check for price data anomalies
-        close_prices = recent_data["close"].dropna()
+            nan_percentage = nan_count / len(recent_data) * 100
+            if nan_percentage > 10:
+                return {'valid': False, 'reason': f'excessive_nan_{col}', 'message': f'Excessive NaN values in {col} column ({nan_percentage:.1f}%)', 'details': {'symbol': symbol, 'column': col, 'nan_percentage': nan_percentage}}
+        close_prices = recent_data['close'].dropna()
         if len(close_prices) < 5:
-            return {
-                "valid": False,
-                "reason": "insufficient_price_data",
-                "message": "Less than 5 valid close prices in recent data",
-                "details": {"symbol": symbol, "valid_close_prices": len(close_prices)},
-            }
-
-        # Check for zero or negative prices
+            return {'valid': False, 'reason': 'insufficient_price_data', 'message': 'Less than 5 valid close prices in recent data', 'details': {'symbol': symbol, 'valid_close_prices': len(close_prices)}}
         if (close_prices <= 0).any():
-            return {
-                "valid": False,
-                "reason": "invalid_prices",
-                "message": "Found zero or negative prices",
-                "details": {"symbol": symbol, "min_price": float(close_prices.min())},
-            }
-
-        # Check for unrealistic price volatility (circuit breaker)
+            return {'valid': False, 'reason': 'invalid_prices', 'message': 'Found zero or negative prices', 'details': {'symbol': symbol, 'min_price': float(close_prices.min())}}
         price_changes = close_prices.pct_change().dropna()
         if len(price_changes) > 0:
-            extreme_moves = (abs(price_changes) > 0.5).sum()  # 50%+ single-day moves
-            if (
-                extreme_moves > len(price_changes) * 0.1
-            ):  # More than 10% of days have extreme moves
-                logger.warning(
-                    "DATA_QUALITY_EXTREME_VOLATILITY",
-                    extra={
-                        "symbol": symbol,
-                        "extreme_moves": extreme_moves,
-                        "total_days": len(price_changes),
-                        "percentage": round(
-                            (extreme_moves / len(price_changes)) * 100, 1
-                        ),
-                        "note": "Potential data quality issue - consider excluding from trading",
-                    },
-                )
-
-        # Check volume data quality
-        volume_data = recent_data["volume"].dropna()
+            extreme_moves = (abs(price_changes) > 0.5).sum()
+            if extreme_moves > len(price_changes) * 0.1:
+                logger.warning('DATA_QUALITY_EXTREME_VOLATILITY', extra={'symbol': symbol, 'extreme_moves': extreme_moves, 'total_days': len(price_changes), 'percentage': round(extreme_moves / len(price_changes) * 100, 1), 'note': 'Potential data quality issue - consider excluding from trading'})
+        volume_data = recent_data['volume'].dropna()
         if len(volume_data) > 0:
-            # Check for suspiciously low volume
             median_volume = volume_data.median()
-            if median_volume < 10000:  # Very low liquidity threshold
-                return {
-                    "valid": False,
-                    "reason": "low_liquidity",
-                    "message": f"Median volume too low ({median_volume:,.0f})",
-                    "details": {
-                        "symbol": symbol,
-                        "median_volume": median_volume,
-                        "threshold": 10000,
-                    },
-                }
-
-            # Check for zero volume days
+            if median_volume < 10000:
+                return {'valid': False, 'reason': 'low_liquidity', 'message': f'Median volume too low ({median_volume:,.0f})', 'details': {'symbol': symbol, 'median_volume': median_volume, 'threshold': 10000}}
             zero_volume_days = (volume_data == 0).sum()
-            if (
-                zero_volume_days > len(volume_data) * 0.2
-            ):  # More than 20% zero volume days
-                return {
-                    "valid": False,
-                    "reason": "excessive_zero_volume",
-                    "message": f"Too many zero volume days ({zero_volume_days}/{len(volume_data)})",
-                    "details": {
-                        "symbol": symbol,
-                        "zero_volume_days": zero_volume_days,
-                        "total_days": len(volume_data),
-                    },
-                }
-
-        # All checks passed
-        return {
-            "valid": True,
-            "reason": "passed_validation",
-            "message": "Data quality validation passed",
-            "details": {
-                "symbol": symbol,
-                "rows_validated": len(df),
-                "recent_rows_checked": len(recent_data),
-                "validation_checks": [
-                    "existence",
-                    "completeness",
-                    "quality",
-                    "anomalies",
-                    "volume",
-                ],
-            },
-        }
-
+            if zero_volume_days > len(volume_data) * 0.2:
+                return {'valid': False, 'reason': 'excessive_zero_volume', 'message': f'Too many zero volume days ({zero_volume_days}/{len(volume_data)})', 'details': {'symbol': symbol, 'zero_volume_days': zero_volume_days, 'total_days': len(volume_data)}}
+        return {'valid': True, 'reason': 'passed_validation', 'message': 'Data quality validation passed', 'details': {'symbol': symbol, 'rows_validated': len(df), 'recent_rows_checked': len(recent_data), 'validation_checks': ['existence', 'completeness', 'quality', 'anomalies', 'volume']}}
     except Exception as e:
-        logger.error(
-            "DATA_VALIDATION_ERROR",
-            extra={"symbol": symbol, "error": str(e), "error_type": type(e).__name__},
-        )
-        return {
-            "valid": False,
-            "reason": "validation_error",
-            "message": f"Data validation failed with error: {e}",
-            "details": {"symbol": symbol, "error": str(e)},
-        }
-
-
-def screen_universe(
-    candidates: Sequence[str],
-    ctx: BotContext,
-    lookback: str = "1mo",
-    interval: str = "1d",
-    top_n: int = 20,
-) -> list[str]:
+        logger.error('DATA_VALIDATION_ERROR', extra={'symbol': symbol, 'error': str(e), 'error_type': type(e).__name__})
+        return {'valid': False, 'reason': 'validation_error', 'message': f'Data validation failed with error: {e}', 'details': {'symbol': symbol, 'error': str(e)}}
+
+def screen_universe(candidates: Sequence[str], ctx: BotContext, lookback: str='1mo', interval: str='1d', top_n: int=20) -> list[str]:
     cand_set = set(candidates)
-    logger.info(
-        f"[SCREEN_UNIVERSE] Starting screening of {len(cand_set)} candidates: {sorted(cand_set)}"
-    )
-
+    logger.info(f'[SCREEN_UNIVERSE] Starting screening of {len(cand_set)} candidates: {sorted(cand_set)}')
     for sym in list(_SCREEN_CACHE):
         if sym not in cand_set:
             _SCREEN_CACHE.pop(sym, None)
-
     new_syms = cand_set - _SCREEN_CACHE.keys()
-    filtered_out = {}  # Track reasons for filtering
-
+    filtered_out = {}
     for sym in new_syms:
         df = ctx.data_fetcher.get_daily_df(ctx, sym)
-
-        # AI-AGENT-REF: Enhanced market data validation for critical trading decisions
         validation_result = _validate_market_data_quality(df, sym)
-        if not validation_result["valid"]:
-            filtered_out[sym] = validation_result["reason"]
+        if not validation_result['valid']:
+            filtered_out[sym] = validation_result['reason']
             logger.debug(f"[SCREEN_UNIVERSE] {sym}: {validation_result['message']}")
             continue
-
         original_len = len(df)
-        df = df[df["volume"] > 100_000]
+        df = df[df['volume'] > 100000]
         if df.empty:
-            filtered_out[sym] = "low_volume"
-            logger.debug(
-                f"[SCREEN_UNIVERSE] {sym}: Filtered out due to low volume (original: {original_len} rows)"
-            )
+            filtered_out[sym] = 'low_volume'
+            logger.debug(f'[SCREEN_UNIVERSE] {sym}: Filtered out due to low volume (original: {original_len} rows)')
             continue
-
-        series = ta.atr(df["high"], df["low"], df["close"], length=ATR_LENGTH)
-        if series is None or not hasattr(series, "empty") or series.empty:
-            filtered_out[sym] = "atr_calculation_failed"
-            logger.warning(f"[SCREEN_UNIVERSE] {sym}: ATR calculation failed")
+        series = ta.atr(df['high'], df['low'], df['close'], length=ATR_LENGTH)
+        if series is None or not hasattr(series, 'empty') or series.empty:
+            filtered_out[sym] = 'atr_calculation_failed'
+            logger.warning(f'[SCREEN_UNIVERSE] {sym}: ATR calculation failed')
             continue
         atr_val = series.iloc[-1]
         if not pd.isna(atr_val):
             _SCREEN_CACHE[sym] = float(atr_val)
-            logger.debug(f"[SCREEN_UNIVERSE] {sym}: ATR = {atr_val:.4f}")
+            logger.debug(f'[SCREEN_UNIVERSE] {sym}: ATR = {atr_val:.4f}')
         else:
-            filtered_out[sym] = "atr_nan"
-            logger.debug(f"[SCREEN_UNIVERSE] {sym}: ATR value is NaN")
-
+            filtered_out[sym] = 'atr_nan'
+            logger.debug(f'[SCREEN_UNIVERSE] {sym}: ATR value is NaN')
     atrs = {sym: _SCREEN_CACHE[sym] for sym in cand_set if sym in _SCREEN_CACHE}
     ranked = sorted(atrs.items(), key=lambda kv: kv[1], reverse=True)
     selected = [sym for sym, _ in ranked[:top_n]]
-
-    logger.info(
-        f"[SCREEN_UNIVERSE] Selected {len(selected)} of {len(cand_set)} candidates. "
-        f"Selected: {selected}. "
-        f"Filtered out: {len(filtered_out)} symbols: {filtered_out}"
-    )
-
+    logger.info(f'[SCREEN_UNIVERSE] Selected {len(selected)} of {len(cand_set)} candidates. Selected: {selected}. Filtered out: {len(filtered_out)} symbols: {filtered_out}')
     return selected
-
 
 def screen_candidates() -> list[str]:
     """Load tickers and apply universe screening."""
     candidates = load_tickers(TICKERS_FILE)
     return screen_universe(candidates, ctx)
 
-
-# Fix for handling missing tickers.csv file
 def get_stock_bars_safe(api, symbol, timeframe):
     """Safely get stock bars with proper error handling."""
     try:
-        return api.get_stock_bars(symbol, timeframe)  # Ensure correct API method
+        return api.get_stock_bars(symbol, timeframe)
     except AttributeError as e:
-        logger.error(f"Alpaca API Error: {e}")
+        logger.error(f'Alpaca API Error: {e}')
         return None
 
-
-def load_tickers(path: str = TICKERS_FILE) -> list[str]:
+def load_tickers(path: str=TICKERS_FILE) -> list[str]:
     """Load tickers from file with fallback to default tickers."""
     tickers: list[str] = []
-
-    # Check if file exists and handle gracefully
     if not os.path.exists(path):
-        logger.warning(f"Tickers file {path} not found. Using default tickers.")
-        # Fallback: define default tickers
-        return ["AAPL", "GOOG", "AMZN"]
-
-    try:
-        with open(path, newline="") as f:
+        logger.warning(f'Tickers file {path} not found. Using default tickers.')
+        return ['AAPL', 'GOOG', 'AMZN']
+    try:
+        with open(path, newline='') as f:
             reader = csv.reader(f)
             next(reader, None)
             for row in reader:
@@ -8060,51 +4962,30 @@                 if t and t not in tickers:
                     tickers.append(t)
     except Exception as e:
-        logger.exception(f"[load_tickers] Failed to read {path}: {e}")
+        logger.exception(f'[load_tickers] Failed to read {path}: {e}')
     return tickers
 
-
 def daily_summary() -> None:
     try:
         if not os.path.exists(TRADE_LOG_FILE):
-            logger.info("DAILY_SUMMARY_NO_TRADES")
+            logger.info('DAILY_SUMMARY_NO_TRADES')
             return
-        df = pd.read_csv(
-            TRADE_LOG_FILE,
-            on_bad_lines="skip",
-            engine="python",
-            usecols=["entry_price", "exit_price", "side"],
-        ).dropna(subset=["entry_price", "exit_price"])
+        df = pd.read_csv(TRADE_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['entry_price', 'exit_price', 'side']).dropna(subset=['entry_price', 'exit_price'])
         if df.empty:
-            logger.warning("Loaded DataFrame is empty after parsing/fallback")
-        direction = np.where(df["side"] == "buy", 1, -1)
-        df["pnl"] = (df.exit_price - df.entry_price) * direction
+            logger.warning('Loaded DataFrame is empty after parsing/fallback')
+        direction = np.where(df['side'] == 'buy', 1, -1)
+        df['pnl'] = (df.exit_price - df.entry_price) * direction
         total_trades = len(df)
         win_rate = (df.pnl > 0).mean() if total_trades else 0
         total_pnl = df.pnl.sum()
         max_dd = (df.pnl.cumsum().cummax() - df.pnl.cumsum()).max()
-        logger.info(
-            "DAILY_SUMMARY",
-            extra={
-                "trades": total_trades,
-                "win_rate": f"{win_rate:.2%}",
-                "pnl": total_pnl,
-                "max_drawdown": max_dd,
-            },
-        )
+        logger.info('DAILY_SUMMARY', extra={'trades': total_trades, 'win_rate': f'{win_rate:.2%}', 'pnl': total_pnl, 'max_drawdown': max_dd})
     except Exception as e:
-        logger.exception(f"daily_summary failed: {e}")
-
-
-# ─── PCA-BASED PORTFOLIO ADJUSTMENT ─────────────────────────────────────────────
+        logger.exception(f'daily_summary failed: {e}')
+
 def run_daily_pca_adjustment(ctx: BotContext) -> None:
     from ai_trading.utils import portfolio_lock
-
-    """
-    Once per day, run PCA on last 90-day returns of current universe.
-    If top PC explains >40% variance and portfolio loads heavily,
-    reduce those weights by 20%.
-    """
+    '\n    Once per day, run PCA on last 90-day returns of current universe.\n    If top PC explains >40% variance and portfolio loads heavily,\n    reduce those weights by 20%.\n    '
     universe = list(ctx.portfolio_weights.keys())
     if not universe:
         return
@@ -8113,42 +4994,33 @@         df = ctx.data_fetcher.get_daily_df(ctx, sym)
         if df is None or len(df) < 90:
             continue
-        rts = df["close"].pct_change(fill_method=None).tail(90).reset_index(drop=True)
+        rts = df['close'].pct_change(fill_method=None).tail(90).reset_index(drop=True)
         returns_df[sym] = rts
-    returns_df = returns_df.dropna(axis=1, how="any")
+    returns_df = returns_df.dropna(axis=1, how='any')
     if returns_df.shape[1] < 2:
         return
     pca = PCA(n_components=3)
     if returns_df.empty:
-        logger.warning("PCA_SKIPPED_EMPTY_RETURNS")
+        logger.warning('PCA_SKIPPED_EMPTY_RETURNS')
         return
     pca.fit(returns_df.values)
     var_explained = pca.explained_variance_ratio_[0]
     if var_explained < 0.4:
         return
     top_loadings = pd.Series(pca.components_[0], index=returns_df.columns).abs()
-    # Identify symbols loading > median loading
     median_load = top_loadings.median()
     high_load_syms = top_loadings[top_loadings > median_load].index.tolist()
     if not high_load_syms:
         return
-    # Reduce those weights by 20%
-    with portfolio_lock:  # FIXED: protect shared portfolio state
+    with portfolio_lock:
         for sym in high_load_syms:
             old = ctx.portfolio_weights.get(sym, 0.0)
             ctx.portfolio_weights[sym] = round(old * 0.8, 4)
-        # Re-normalize to sum to 1
         total = sum(ctx.portfolio_weights.values())
         if total > 0:
             for sym in ctx.portfolio_weights:
-                ctx.portfolio_weights[sym] = round(
-                    ctx.portfolio_weights[sym] / total, 4
-                )
-    logger.info(
-        "PCA_ADJUSTMENT_APPLIED",
-        extra={"var_explained": round(var_explained, 3), "adjusted": high_load_syms},
-    )
-
+                ctx.portfolio_weights[sym] = round(ctx.portfolio_weights[sym] / total, 4)
+    logger.info('PCA_ADJUSTMENT_APPLIED', extra={'var_explained': round(var_explained, 3), 'adjusted': high_load_syms})
 
 def daily_reset(state: BotState) -> None:
     """Reset daily counters and in-memory slippage logs."""
@@ -8156,26 +5028,19 @@         config.reload_env()
         _slippage_log.clear()
         state.loss_streak = 0
-        logger.info("DAILY_STATE_RESET")
+        logger.info('DAILY_STATE_RESET')
     except Exception as e:
-        logger.exception(f"daily_reset failed: {e}")
-
-
-def _average_reward(n: int = 20) -> float:
+        logger.exception(f'daily_reset failed: {e}')
+
+def _average_reward(n: int=20) -> float:
     if not os.path.exists(REWARD_LOG_FILE):
         return 0.0
-    df = pd.read_csv(
-        REWARD_LOG_FILE,
-        on_bad_lines="skip",
-        engine="python",
-        usecols=["reward"],
-    ).tail(n)
+    df = pd.read_csv(REWARD_LOG_FILE, on_bad_lines='skip', engine='python', usecols=['reward']).tail(n)
     if df.empty:
-        logger.warning("Loaded DataFrame is empty after parsing/fallback")
-    if df.empty or "reward" not in df.columns:
+        logger.warning('Loaded DataFrame is empty after parsing/fallback')
+    if df.empty or 'reward' not in df.columns:
         return 0.0
-    return float(df["reward"].mean())
-
+    return float(df['reward'].mean())
 
 def _current_drawdown() -> float:
     try:
@@ -8189,40 +5054,30 @@         return 0.0
     return max(0.0, (peak - eq) / peak)
 
-
 def update_bot_mode(state: BotState) -> None:
     try:
         avg_r = _average_reward()
         dd = _current_drawdown()
         regime = state.current_regime
         if dd > 0.05 or avg_r < -0.01:
-            new_mode = "conservative"
-        elif avg_r > 0.05 and regime == "trending":
-            new_mode = "aggressive"
+            new_mode = 'conservative'
+        elif avg_r > 0.05 and regime == 'trending':
+            new_mode = 'aggressive'
         else:
-            new_mode = "balanced"
+            new_mode = 'balanced'
         if new_mode != state.mode_obj.mode:
             state.mode_obj = BotMode(new_mode)
             params.update(state.mode_obj.get_config())
-            ctx.kelly_fraction = params.get("KELLY_FRACTION", 0.6)
-            logger.info(
-                "MODE_SWITCH",
-                extra={
-                    "new_mode": new_mode,
-                    "avg_reward": avg_r,
-                    "drawdown": dd,
-                    "regime": regime,
-                },
-            )
+            ctx.kelly_fraction = params.get('KELLY_FRACTION', 0.6)
+            logger.info('MODE_SWITCH', extra={'new_mode': new_mode, 'avg_reward': avg_r, 'drawdown': dd, 'regime': regime})
     except Exception as e:
-        logger.exception(f"update_bot_mode failed: {e}")
-
+        logger.exception(f'update_bot_mode failed: {e}')
 
 def adaptive_risk_scaling(ctx: BotContext) -> None:
     """Adjust risk parameters based on volatility, rewards and drawdown."""
     try:
-        vol = _VOL_STATS.get("mean", 0)
-        spy_atr = _VOL_STATS.get("last", 0)
+        vol = _VOL_STATS.get('mean', 0)
+        spy_atr = _VOL_STATS.get('last', 0)
         avg_r = _average_reward(30)
         dd = _current_drawdown()
         try:
@@ -8230,43 +5085,28 @@         except Exception:
             equity = 0.0
         ctx.capital_scaler.update(ctx, equity)
-        params["CAPITAL_CAP"] = ctx.params["CAPITAL_CAP"]
-        frac = params.get("KELLY_FRACTION", 0.6)
-        if spy_atr and vol and spy_atr > vol * 1.5:
+        params['CAPITAL_CAP'] = ctx.params['CAPITAL_CAP']
+        frac = params.get('KELLY_FRACTION', 0.6)
+        if spy_atr and vol and (spy_atr > vol * 1.5):
             frac *= 0.5
         if avg_r < -0.02:
             frac *= 0.7
         if dd > 0.1:
             frac *= 0.5
         ctx.kelly_fraction = round(max(0.2, min(frac, 1.0)), 2)
-        params["CAPITAL_CAP"] = round(
-            max(0.02, min(0.1, params.get("CAPITAL_CAP", 0.25) * (1 - dd))), 3
-        )
-        logger.info(
-            "RISK_SCALED",
-            extra={
-                "kelly_fraction": ctx.kelly_fraction,
-                "dd": dd,
-                "atr": spy_atr,
-                "avg_reward": avg_r,
-            },
-        )
+        params['CAPITAL_CAP'] = round(max(0.02, min(0.1, params.get('CAPITAL_CAP', 0.25) * (1 - dd))), 3)
+        logger.info('RISK_SCALED', extra={'kelly_fraction': ctx.kelly_fraction, 'dd': dd, 'atr': spy_atr, 'avg_reward': avg_r})
     except Exception as e:
-        logger.exception(f"adaptive_risk_scaling failed: {e}")
-
+        logger.exception(f'adaptive_risk_scaling failed: {e}')
 
 def check_disaster_halt() -> None:
     try:
         dd = _current_drawdown()
         if dd >= DISASTER_DD_LIMIT:
-            set_halt_flag(f"DISASTER_DRAW_DOWN_{dd:.2%}")
-            logger.error("DISASTER_HALT_TRIGGERED", extra={"drawdown": dd})
+            set_halt_flag(f'DISASTER_DRAW_DOWN_{dd:.2%}')
+            logger.error('DISASTER_HALT_TRIGGERED', extra={'drawdown': dd})
     except Exception as e:
-        logger.exception(f"check_disaster_halt failed: {e}")
-
-
-# retrain_meta_learner is imported above if available
-
+        logger.exception(f'check_disaster_halt failed: {e}')
 
 def load_or_retrain_daily(ctx: BotContext) -> Any:
     """
@@ -8274,235 +5114,155 @@     2. If missing or older than today, call retrain_meta_learner(ctx, symbols) and update marker.
     3. Then load the (new) model from MODEL_PATH.
     """
-    today_str = (
-        datetime.now(UTC).astimezone(ZoneInfo("America/New_York")).strftime("%Y-%m-%d")
-    )
+    today_str = datetime.now(UTC).astimezone(ZoneInfo('America/New_York')).strftime('%Y-%m-%d')
     marker = RETRAIN_MARKER_FILE
-
     need_to_retrain = True
-    if config.DISABLE_DAILY_RETRAIN:
-        logger.info("Daily retraining disabled via DISABLE_DAILY_RETRAIN")
+    if S.disable_daily_retrain:
+        logger.info('Daily retraining disabled via DISABLE_DAILY_RETRAIN')
         need_to_retrain = False
     if os.path.isfile(marker):
         with open(marker) as f:
             last_date = f.read().strip()
         if last_date == today_str:
             need_to_retrain = False
-
     if not os.path.exists(MODEL_PATH):
-        logger.warning(
-            "MODEL_PATH missing; forcing initial retrain.",
-            extra={"path": MODEL_PATH},
-        )
+        logger.warning('MODEL_PATH missing; forcing initial retrain.', extra={'path': MODEL_PATH})
         need_to_retrain = True
-
     if need_to_retrain:
-        if not callable(globals().get("retrain_meta_learner")):
-            logger.warning(
-                "Daily retraining requested, but retrain_meta_learner is unavailable."
-            )
+        if not callable(globals().get('retrain_meta_learner')):
+            logger.warning('Daily retraining requested, but retrain_meta_learner is unavailable.')
+        elif not meta_lock.acquire(blocking=False):
+            logger.warning('METALEARN_SKIPPED_LOCKED')
         else:
-            if not meta_lock.acquire(blocking=False):
-                logger.warning("METALEARN_SKIPPED_LOCKED")
-            else:
-                try:
-                    symbols = load_tickers(TICKERS_FILE)
-                    logger.info(
-                        f"RETRAINING START for {today_str} on {len(symbols)} tickers..."
-                    )
-                    valid_symbols = []
-                    for symbol in symbols:
+            try:
+                symbols = load_tickers(TICKERS_FILE)
+                logger.info(f'RETRAINING START for {today_str} on {len(symbols)} tickers...')
+                valid_symbols = []
+                for symbol in symbols:
+                    try:
+                        df_min = fetch_minute_df_safe(symbol)
+                    except DataFetchError:
+                        logger.info(f'{symbol} returned no minute data; skipping symbol.')
+                        continue
+                    if df_min is None or df_min.empty:
+                        logger.info(f'{symbol} returned no minute data; skipping symbol.')
+                        continue
+                    valid_symbols.append(symbol)
+                if not valid_symbols:
+                    logger.warning('No symbols returned valid minute data; skipping retraining entirely.')
+                else:
+                    force_train = not os.path.exists(MODEL_PATH)
+                    if is_market_open():
+                        success = retrain_meta_learner(ctx, valid_symbols, force=force_train)
+                    else:
+                        logger.info('[retrain_meta_learner] Outside market hours; skipping')
+                        success = False
+                    if success:
                         try:
-                            df_min = fetch_minute_df_safe(symbol)
-                        except DataFetchError:
-                            logger.info(
-                                f"{symbol} returned no minute data; skipping symbol."
-                            )
-                            continue
-                        if df_min is None or df_min.empty:
-                            logger.info(
-                                f"{symbol} returned no minute data; skipping symbol."
-                            )
-                            continue
-                        valid_symbols.append(symbol)
-                    if not valid_symbols:
-                        logger.warning(
-                            "No symbols returned valid minute data; skipping retraining entirely."
-                        )
+                            with open(marker, 'w') as f:
+                                f.write(today_str)
+                        except Exception as e:
+                            logger.warning(f'Failed to write retrain marker file: {e}')
                     else:
-                        force_train = not os.path.exists(MODEL_PATH)
-                        if is_market_open():
-                            success = retrain_meta_learner(
-                                ctx, valid_symbols, force=force_train
-                            )
-                        else:
-                            logger.info(
-                                "[retrain_meta_learner] Outside market hours; skipping"
-                            )
-                            success = False
-                        if success:
-                            try:
-                                with open(marker, "w") as f:
-                                    f.write(today_str)
-                            except Exception as e:
-                                logger.warning(
-                                    f"Failed to write retrain marker file: {e}"
-                                )
-                        else:
-                            logger.warning(
-                                "Retraining failed; continuing with existing model."
-                            )
-                finally:
-                    meta_lock.release()
-
+                        logger.warning('Retraining failed; continuing with existing model.')
+            finally:
+                meta_lock.release()
     df_train = ctx.data_fetcher.get_daily_df(ctx, REGIME_SYMBOLS[0])
-    if df_train is not None and not df_train.empty:
-        X_train = (
-            df_train[["open", "high", "low", "close", "volume"]]
-            .astype(float)
-            .iloc[:-1]
-            .values
-        )
-        y_train = (
-            df_train["close"]
-            .pct_change(fill_method=None)
-            .shift(-1)
-            .fillna(0)
-            .values[:-1]
-        )
+    if df_train is not None and (not df_train.empty):
+        X_train = df_train[['open', 'high', 'low', 'close', 'volume']].astype(float).iloc[:-1].values
+        y_train = df_train['close'].pct_change(fill_method=None).shift(-1).fillna(0).values[:-1]
         with model_lock:
             try:
                 if len(X_train) == 0:
-                    logger.warning("DAILY_MODEL_TRAIN_SKIPPED_EMPTY")
+                    logger.warning('DAILY_MODEL_TRAIN_SKIPPED_EMPTY')
                 else:
                     model_pipeline.fit(X_train, y_train)
-                    mse = float(
-                        np.mean((model_pipeline.predict(X_train) - y_train) ** 2)
-                    )
-                    logger.info("TRAIN_METRIC", extra={"mse": mse})
+                    mse = float(np.mean((model_pipeline.predict(X_train) - y_train) ** 2))
+                    logger.info('TRAIN_METRIC', extra={'mse': mse})
             except Exception as e:
-                logger.error(f"Daily retrain failed: {e}")
-
-        date_str = datetime.now(UTC).strftime("%Y%m%d_%H%M")
-        os.makedirs("models", exist_ok=True)
-        path = f"models/sgd_{date_str}.pkl"
+                logger.error(f'Daily retrain failed: {e}')
+        date_str = datetime.now(UTC).strftime('%Y%m%d_%H%M')
+        os.makedirs('models', exist_ok=True)
+        path = f'models/sgd_{date_str}.pkl'
         atomic_joblib_dump(model_pipeline, path)
-        logger.info(f"Model checkpoint saved: {path}")
-
-        for f in os.listdir("models"):
-            if f.endswith(".pkl"):
-                dt = datetime.strptime(f.split("_")[1].split(".")[0], "%Y%m%d").replace(
-                    tzinfo=UTC
-                )
+        logger.info(f'Model checkpoint saved: {path}')
+        for f in os.listdir('models'):
+            if f.endswith('.pkl'):
+                dt = datetime.strptime(f.split('_')[1].split('.')[0], '%Y%m%d').replace(tzinfo=UTC)
                 if datetime.now(UTC) - dt > timedelta(days=30):
-                    os.remove(os.path.join("models", f))
-
+                    os.remove(os.path.join('models', f))
         batch_mse = float(np.mean((model_pipeline.predict(X_train) - y_train) ** 2))
-        log_metrics(
-            {
-                "timestamp": utc_now_iso(),
-                "type": "daily_retrain",
-                "batch_mse": batch_mse,
-                "hyperparams": json.dumps(utils.to_serializable(config.SGD_PARAMS)),
-                "seed": SEED,
-                "model": "SGDRegressor",
-                "git_hash": get_git_hash(),
-            }
-        )
+        log_metrics({'timestamp': utc_now_iso(), 'type': 'daily_retrain', 'batch_mse': batch_mse, 'hyperparams': json.dumps(utils.to_serializable(S.sgd_params)), 'seed': SEED, 'model': 'SGDRegressor', 'git_hash': get_git_hash()})
         state.updates_halted = False
         state.rolling_losses.clear()
-
     return model_pipeline
-
 
 def on_market_close() -> None:
     """Trigger daily retraining after the market closes."""
-    now_est = dt_.now(UTC).astimezone(ZoneInfo("America/New_York"))
+    now_est = dt_.now(UTC).astimezone(ZoneInfo('America/New_York'))
     if market_is_open(now_est):
-        logger.info("RETRAIN_SKIP_MARKET_OPEN")
+        logger.info('RETRAIN_SKIP_MARKET_OPEN')
         return
     if now_est.time() < dt_time(16, 0):
-        logger.info("RETRAIN_SKIP_EARLY", extra={"time": now_est.isoformat()})
+        logger.info('RETRAIN_SKIP_EARLY', extra={'time': now_est.isoformat()})
         return
     try:
         load_or_retrain_daily(ctx)
     except Exception as exc:
-        logger.exception(f"on_market_close failed: {exc}")
-
-
-# ─── M. MAIN LOOP & SCHEDULER ─────────────────────────────────────────────────
+        logger.exception(f'on_market_close failed: {exc}')
 app = Flask(__name__)
 
-
-@app.route("/health", methods=["GET"])
-@app.route("/health_check", methods=["GET"])
+@app.route('/health', methods=['GET'])
+@app.route('/health_check', methods=['GET'])
 def health() -> str:
     """Health endpoint exposing basic system metrics."""
     try:
         pre_trade_health_check(ctx, ctx.tickers or REGIME_SYMBOLS)
-        status = "ok"
+        status = 'ok'
     except Exception as exc:
-        status = f"degraded: {exc}"
-    summary = {
-        "status": status,
-        "no_signal_events": state.no_signal_events,
-        "indicator_failures": state.indicator_failures,
-    }
+        status = f'degraded: {exc}'
+    summary = {'status': status, 'no_signal_events': state.no_signal_events, 'indicator_failures': state.indicator_failures}
     from flask import jsonify
-
-    return jsonify(summary), 200
-
+    return (jsonify(summary), 200)
 
 def start_healthcheck() -> None:
     port = S.healthcheck_port
     try:
-        app.run(host="0.0.0.0", port=port)
+        app.run(host='0.0.0.0', port=port)
     except OSError as e:
-        logger.warning(
-            f"Healthcheck port {port} in use: {e}. Skipping health-endpoint."
-        )
+        logger.warning(f'Healthcheck port {port} in use: {e}. Skipping health-endpoint.')
     except Exception as e:
-        logger.exception(f"start_healthcheck failed: {e}")
-
-
-def start_metrics_server(default_port: int = 9200) -> None:
+        logger.exception(f'start_healthcheck failed: {e}')
+
+def start_metrics_server(default_port: int=9200) -> None:
     """Start Prometheus metrics server handling port conflicts."""
     try:
         start_http_server(default_port)
-        logger.debug("Metrics server started on %d", default_port)
+        logger.debug('Metrics server started on %d', default_port)
         return
     except OSError as exc:
-        if "Address already in use" in str(exc):
+        if 'Address already in use' in str(exc):
             try:
                 import requests
-
-                resp = requests.get(f"http://localhost:{default_port}", timeout=2)
+                resp = requests.get(f'http://localhost:{default_port}', timeout=2)
                 if resp.ok:
-                    logger.info(
-                        "Metrics port %d already serving; reusing", default_port
-                    )
+                    logger.info('Metrics port %d already serving; reusing', default_port)
                     return
             except Exception as e:
-                # Metrics server connectivity check failed - continue with port search
-                logger.debug(
-                    "Metrics server check failed on port %d: %s", default_port, e
-                )
+                logger.debug('Metrics server check failed on port %d: %s', default_port, e)
             port = utils.get_free_port(default_port + 1, default_port + 50)
             if port is None:
-                logger.warning("No free port available for metrics server")
+                logger.warning('No free port available for metrics server')
                 return
-            logger.warning("Metrics port %d busy; using %d", default_port, port)
+            logger.warning('Metrics port %d busy; using %d', default_port, port)
             try:
                 start_http_server(port)
             except Exception as exc2:
-                logger.warning("Failed to start metrics server on %d: %s", port, exc2)
+                logger.warning('Failed to start metrics server on %d: %s', port, exc2)
         else:
-            logger.warning(
-                "Failed to start metrics server on %d: %s", default_port, exc
-            )
-    except Exception as exc:  # pragma: no cover - unexpected error
-        logger.warning("Failed to start metrics server on %d: %s", default_port, exc)
-
+            logger.warning('Failed to start metrics server on %d: %s', default_port, exc)
+    except Exception as exc:
+        logger.warning('Failed to start metrics server on %d: %s', default_port, exc)
 
 def run_multi_strategy(ctx: BotContext) -> None:
     """Execute all modular strategies via allocator and risk engine."""
@@ -8512,41 +5272,29 @@             sigs = strat.generate(ctx)
             signals_by_strategy[strat.name] = sigs
         except Exception as e:
-            logger.warning(f"Strategy {strat.name} failed: {e}")
-    # Optionally augment strategy signals with reinforcement learning signals.
-    if config.USE_RL_AGENT:
+            logger.warning(f'Strategy {strat.name} failed: {e}')
+    if S.use_rl_agent:
         try:
-            # Lazy load the RL policy and cache it on the context
             from ai_trading.rl_trading.inference import load_policy
-
-            if not hasattr(ctx, "rl_agent"):
-                ctx.rl_agent = load_policy(config.RL_MODEL_PATH)
-            # Determine the set of symbols that currently have signals from other strategies
+            if not hasattr(ctx, 'rl_agent'):
+                ctx.rl_agent = load_policy(S.rl_model_path)
             all_symbols: list[str] = []
             for sigs in signals_by_strategy.values():
                 for sig in sigs:
-                    sym = getattr(sig, "symbol", None)
+                    sym = getattr(sig, 'symbol', None)
                     if sym and sym not in all_symbols:
                         all_symbols.append(sym)
             if all_symbols:
-                # Compute meaningful feature vectors for each symbol instead of using
-                # placeholder zeros.  The RL agent expects a 1-D observation per
-                # symbol; we derive this from recent returns and technical
-                # indicators (RSI, ATR).  Additional features can be added
-                # by modifying ``compute_features`` in ``ai_trading.rl_trading.features``.
-                import numpy as _np  # AI-AGENT-REF: alias to avoid shadowing global np
-
+                import numpy as _np
                 from ai_trading.rl_trading.features import compute_features
-
                 states: list[_np.ndarray] = []
                 for sym in all_symbols:
-                    # Try to fetch recent daily price data; fallback to minute data.
                     df = None
                     try:
                         df = ctx.data_fetcher.get_daily_df(ctx, sym)
                     except Exception:
                         df = None
-                    if df is None or getattr(df, "empty", True):
+                    if df is None or getattr(df, 'empty', True):
                         try:
                             df = ctx.data_fetcher.get_minute_df(ctx, sym)
                         except Exception:
@@ -8557,58 +5305,26 @@                     state_mat = _np.stack(states).astype(_np.float32)
                     rl_sigs = ctx.rl_agent.predict(state_mat, symbols=all_symbols)
                     if rl_sigs:
-                        signals_by_strategy["rl"] = (
-                            rl_sigs if isinstance(rl_sigs, list) else [rl_sigs]
-                        )
+                        signals_by_strategy['rl'] = rl_sigs if isinstance(rl_sigs, list) else [rl_sigs]
         except Exception as exc:
-            logger.error("RL_AGENT_ERROR", extra={"exc": str(exc)})
-
-    # AI-AGENT-REF: Add position holding logic to reduce churn
-    try:
-        # Get current positions
+            logger.error('RL_AGENT_ERROR', extra={'exc': str(exc)})
+    try:
         current_positions = ctx.api.get_all_positions()
-
-        # Generate hold signals for existing positions
-        # ai_trading/core/bot_engine.py:8588 - Convert import guard to hard import (internal module)
-        from ai_trading.signals import (  # type: ignore
-            enhance_signals_with_position_logic,
-            generate_position_hold_signals,
-        )
-
+        from ai_trading.signals import enhance_signals_with_position_logic, generate_position_hold_signals
         hold_signals = generate_position_hold_signals(ctx, current_positions)
-
-        # Apply position holding logic to all strategy signals
         enhanced_signals_by_strategy = {}
         for strategy_name, strategy_signals in signals_by_strategy.items():
-            enhanced_signals = enhance_signals_with_position_logic(
-                strategy_signals, ctx, hold_signals
-            )
+            enhanced_signals = enhance_signals_with_position_logic(strategy_signals, ctx, hold_signals)
             enhanced_signals_by_strategy[strategy_name] = enhanced_signals
-
-        # Log the effect of position holding
-        original_count = sum(len(sigs) for sigs in signals_by_strategy.values())
-        enhanced_count = sum(
-            len(sigs) for sigs in enhanced_signals_by_strategy.values()
-        )
-        logger.info(
-            "POSITION_HOLD_FILTER",
-            extra={
-                "original_signals": original_count,
-                "enhanced_signals": enhanced_count,
-                "filtered_out": original_count - enhanced_count,
-                "hold_signals_count": len(hold_signals),
-            },
-        )
-
-        # Use enhanced signals for allocation
+        original_count = sum((len(sigs) for sigs in signals_by_strategy.values()))
+        enhanced_count = sum((len(sigs) for sigs in enhanced_signals_by_strategy.values()))
+        logger.info('POSITION_HOLD_FILTER', extra={'original_signals': original_count, 'enhanced_signals': enhanced_count, 'filtered_out': original_count - enhanced_count, 'hold_signals_count': len(hold_signals)})
         signals_by_strategy = enhanced_signals_by_strategy
-
     except Exception as exc:
-        logger.warning("Position holding logic failed, using original signals: %s", exc)
-
+        logger.warning('Position holding logic failed, using original signals: %s', exc)
     final = ctx.allocator.allocate(signals_by_strategy)
     acct = ctx.api.get_account()
-    cash = float(getattr(acct, "cash", 0))
+    cash = float(getattr(acct, 'cash', 0))
     for sig in final:
         retries = 0
         price = 0.0
@@ -8617,11 +5333,9 @@             try:
                 req = StockLatestQuoteRequest(symbol_or_symbols=[sig.symbol])
                 quote: Quote = ctx.data_client.get_stock_latest_quote(req)
-                price = float(getattr(quote, "ask_price", 0) or 0)
+                price = float(getattr(quote, 'ask_price', 0) or 0)
             except APIError as e:
-                logger.warning(
-                    "[run_all_trades] quote failed for %s: %s", sig.symbol, e
-                )
+                logger.warning('[run_all_trades] quote failed for %s: %s', sig.symbol, e)
                 price = 0.0
             if price <= 0:
                 time.sleep(2)
@@ -8629,112 +5343,44 @@                     data = fetch_minute_df_safe(sig.symbol)
                 except DataFetchError:
                     data = pd.DataFrame()
-                if data is not None and not data.empty:
+                if data is not None and (not data.empty):
                     row = data.iloc[-1]
-                    logger.debug(
-                        "Fetched minute data for %s: %s",
-                        sig.symbol,
-                        row.to_dict(),
-                    )
-                    minute_close = float(row.get("close", 0))
-                    logger.info(
-                        "Using last_close=%.4f vs minute_close=%.4f",
-                        utils.get_latest_close(data),
-                        minute_close,
-                    )
-                    price = (
-                        minute_close
-                        if minute_close > 0
-                        else utils.get_latest_close(data)
-                    )
+                    logger.debug('Fetched minute data for %s: %s', sig.symbol, row.to_dict())
+                    minute_close = float(row.get('close', 0))
+                    logger.info('Using last_close=%.4f vs minute_close=%.4f', utils.get_latest_close(data), minute_close)
+                    price = minute_close if minute_close > 0 else utils.get_latest_close(data)
                 if price <= 0:
-                    logger.warning(
-                        "Retry %s: price %.2f <= 0 for %s, refetching data",
-                        retries + 1,
-                        price,
-                        sig.symbol,
-                    )
+                    logger.warning('Retry %s: price %.2f <= 0 for %s, refetching data', retries + 1, price, sig.symbol)
                     retries += 1
             else:
                 break
         if price <= 0:
-            logger.critical(
-                "Failed after retries: non-positive price for %s. Data context: %r",
-                sig.symbol,
-                (
-                    data.tail(3).to_dict()
-                    if hasattr(data, "tail") and hasattr(data, "to_dict")
-                    else data
-                ),
-            )
+            logger.critical('Failed after retries: non-positive price for %s. Data context: %r', sig.symbol, data.tail(3).to_dict() if hasattr(data, 'tail') and hasattr(data, 'to_dict') else data)
             continue
-        # Provide the account equity (cash) when sizing positions; this allows
-        # CapitalScalingEngine.scale_position to use equity rather than raw size.
-        if sig.side == "buy" and ctx.risk_engine.position_exists(ctx.api, sig.symbol):
-            logger.info("SKIP_DUPLICATE_LONG", extra={"symbol": sig.symbol})
+        if sig.side == 'buy' and ctx.risk_engine.position_exists(ctx.api, sig.symbol):
+            logger.info('SKIP_DUPLICATE_LONG', extra={'symbol': sig.symbol})
             continue
-
-        # AI-AGENT-REF: Add validation and logging for signal processing
-        logger.debug(
-            "PROCESSING_SIGNAL",
-            extra={
-                "symbol": sig.symbol,
-                "side": sig.side,
-                "confidence": sig.confidence,
-                "strategy": getattr(sig, "strategy", "unknown"),
-                "weight": getattr(sig, "weight", 0.0),
-            },
-        )
-
+        logger.debug('PROCESSING_SIGNAL', extra={'symbol': sig.symbol, 'side': sig.side, 'confidence': sig.confidence, 'strategy': getattr(sig, 'strategy', 'unknown'), 'weight': getattr(sig, 'weight', 0.0)})
         qty = ctx.risk_engine.position_size(sig, cash, price)
         if qty is None or not np.isfinite(qty) or qty <= 0:
-            logger.warning(
-                "SKIP_INVALID_QTY",
-                extra={
-                    "symbol": sig.symbol,
-                    "side": sig.side,
-                    "qty": qty,
-                    "cash": cash,
-                    "price": price,
-                },
-            )
+            logger.warning('SKIP_INVALID_QTY', extra={'symbol': sig.symbol, 'side': sig.side, 'qty': qty, 'cash': cash, 'price': price})
             continue
-
-        # AI-AGENT-REF: Validate signal side before execution to catch any corruption
-        if sig.side not in ["buy", "sell"]:
-            logger.error(
-                "INVALID_SIGNAL_SIDE",
-                extra={
-                    "symbol": sig.symbol,
-                    "side": sig.side,
-                    "expected": "buy or sell",
-                },
-            )
+        if sig.side not in ['buy', 'sell']:
+            logger.error('INVALID_SIGNAL_SIDE', extra={'symbol': sig.symbol, 'side': sig.side, 'expected': 'buy or sell'})
             continue
-
-        logger.info(
-            "EXECUTING_ORDER",
-            extra={"symbol": sig.symbol, "side": sig.side, "qty": qty, "price": price},
-        )
-
-        ctx.execution_engine.execute_order(
-            sig.symbol, qty, sig.side, asset_class=sig.asset_class
-        )
+        logger.info('EXECUTING_ORDER', extra={'symbol': sig.symbol, 'side': sig.side, 'qty': qty, 'price': price})
+        ctx.execution_engine.execute_order(sig.symbol, qty, sig.side, asset_class=sig.asset_class)
         ctx.risk_engine.register_fill(sig)
-
-    # At the end of the strategy cycle, trigger trailing-stop checks if an ExecutionEngine is present.
-    try:
-        if hasattr(ctx, "execution_engine"):
+    try:
+        if hasattr(ctx, 'execution_engine'):
             ctx.execution_engine.end_cycle()
     except Exception as exc:
-        logger.error("TRAILING_STOP_CHECK_FAILED", extra={"exc": str(exc)})
-
+        logger.error('TRAILING_STOP_CHECK_FAILED', extra={'exc': str(exc)})
 
 def _prepare_run(ctx: BotContext, state: BotState) -> tuple[float, bool, list[str]]:
     from ai_trading import portfolio
     from ai_trading.utils import portfolio_lock
-
-    """Prepare trading run by syncing positions and generating symbols."""
+    'Prepare trading run by syncing positions and generating symbols.'
     cancel_all_open_orders(ctx)
     audit_positions(ctx)
     try:
@@ -8743,127 +5389,75 @@     except Exception:
         equity = 0.0
     ctx.capital_scaler.update(ctx, equity)
-    params["CAPITAL_CAP"] = ctx.params["CAPITAL_CAP"]
+    params['CAPITAL_CAP'] = ctx.params['CAPITAL_CAP']
     compute_spy_vol_stats(ctx)
-
     full_watchlist = load_tickers(TICKERS_FILE)
     symbols = screen_candidates()
-    logger.info(
-        "Number of screened candidates: %s", len(symbols)
-    )  # AI-AGENT-REF: log candidate count
+    logger.info('Number of screened candidates: %s', len(symbols))
     if not symbols:
-        logger.warning(
-            "No candidates found after filtering, using top 5 tickers fallback."
-        )
+        logger.warning('No candidates found after filtering, using top 5 tickers fallback.')
         symbols = full_watchlist[:5]
-    logger.info("CANDIDATES_SCREENED", extra={"tickers": symbols})
+    logger.info('CANDIDATES_SCREENED', extra={'tickers': symbols})
     ctx.tickers = symbols
     try:
         summary = pre_trade_health_check(ctx, symbols)
-        logger.info("PRE_TRADE_HEALTH", extra=summary)
+        logger.info('PRE_TRADE_HEALTH', extra=summary)
     except Exception as exc:
-        logger.warning(f"pre_trade_health_check failure: {exc}")
+        logger.warning(f'pre_trade_health_check failure: {exc}')
     with portfolio_lock:
         ctx.portfolio_weights = portfolio.compute_portfolio_weights(ctx, symbols)
     acct = safe_alpaca_get_account(ctx)
     if acct:
-        current_cash = float(getattr(acct, "buying_power", acct.cash))
+        current_cash = float(getattr(acct, 'buying_power', acct.cash))
     else:
-        logger.error("Failed to get account information from Alpaca")
-        return 0.0, False, []
+        logger.error('Failed to get account information from Alpaca')
+        return (0.0, False, [])
     regime_ok = check_market_regime(state)
-    return current_cash, regime_ok, symbols
-
-
-def _process_symbols(
-    symbols: list[str],
-    current_cash: float,
-    model,
-    regime_ok: bool,
-    close_shorts: bool = False,
-    skip_duplicates: bool = False,
-) -> tuple[list[str], dict[str, int]]:
+    return (current_cash, regime_ok, symbols)
+
+def _process_symbols(symbols: list[str], current_cash: float, model, regime_ok: bool, close_shorts: bool=False, skip_duplicates: bool=False) -> tuple[list[str], dict[str, int]]:
     processed: list[str] = []
     row_counts: dict[str, int] = {}
-
-    if not hasattr(state, "trade_cooldowns"):
+    if not hasattr(state, 'trade_cooldowns'):
         state.trade_cooldowns = {}
-    if not hasattr(state, "last_trade_direction"):
+    if not hasattr(state, 'last_trade_direction'):
         state.last_trade_direction = {}
-
     now = datetime.now(UTC)
-
     filtered: list[str] = []
     cd_skipped: list[str] = []
-
-    # AI-AGENT-REF: Add circuit breaker for symbol processing to prevent resource exhaustion
-    max_symbols_per_cycle = min(50, len(symbols))  # Limit to 50 symbols per cycle
+    max_symbols_per_cycle = min(50, len(symbols))
     processed_symbols = 0
     processing_start_time = time.monotonic()
-
     for symbol in symbols:
-        # AI-AGENT-REF: Final-bar/session gating before strategy evaluation
-        if not ensure_final_bar(symbol, "1min"):  # Default to 1min timeframe
-            logger.info(
-                "SKIP_PARTIAL_BAR", extra={"symbol": symbol, "timeframe": "1min"}
-            )
+        if not ensure_final_bar(symbol, '1min'):
+            logger.info('SKIP_PARTIAL_BAR', extra={'symbol': symbol, 'timeframe': '1min'})
             continue
-
-        # Circuit breaker: limit processing time and symbol count
         if processed_symbols >= max_symbols_per_cycle:
-            logger.warning(
-                "SYMBOL_PROCESSING_CIRCUIT_BREAKER",
-                extra={
-                    "processed_count": processed_symbols,
-                    "remaining_count": len(symbols) - processed_symbols,
-                    "reason": "max_symbols_reached",
-                },
-            )
+            logger.warning('SYMBOL_PROCESSING_CIRCUIT_BREAKER', extra={'processed_count': processed_symbols, 'remaining_count': len(symbols) - processed_symbols, 'reason': 'max_symbols_reached'})
             break
-
-        # Check processing time limit (max 5 minutes per cycle)
         if time.monotonic() - processing_start_time > 300:
-            logger.warning(
-                "SYMBOL_PROCESSING_CIRCUIT_BREAKER",
-                extra={
-                    "processed_count": processed_symbols,
-                    "elapsed_seconds": time.monotonic() - processing_start_time,
-                    "reason": "time_limit_reached",
-                },
-            )
+            logger.warning('SYMBOL_PROCESSING_CIRCUIT_BREAKER', extra={'processed_count': processed_symbols, 'elapsed_seconds': time.monotonic() - processing_start_time, 'reason': 'time_limit_reached'})
             break
-
         processed_symbols += 1
-
         pos = state.position_cache.get(symbol, 0)
         if pos < 0 and close_shorts:
-            logger.info(
-                "SKIP_SHORT_CLOSE_QUEUED | symbol=%s qty=%s",
-                symbol,
-                -pos,
-            )
-            # AI-AGENT-REF: avoid submitting orders when short-close is skipped
+            logger.info('SKIP_SHORT_CLOSE_QUEUED | symbol=%s qty=%s', symbol, -pos)
             continue
         if skip_duplicates and pos != 0:
-            log_skip_cooldown(symbol, reason="duplicate")
+            log_skip_cooldown(symbol, reason='duplicate')
             skipped_duplicates.inc()
             continue
         if pos > 0:
-            logger.info("SKIP_HELD_POSITION | already long, skipping close")
+            logger.info('SKIP_HELD_POSITION | already long, skipping close')
             skipped_duplicates.inc()
             continue
         if pos < 0:
-            logger.info(
-                "SHORT_CLOSE_QUEUED | symbol=%s  qty=%d",
-                symbol,
-                abs(pos),
-            )
+            logger.info('SHORT_CLOSE_QUEUED | symbol=%s  qty=%d', symbol, abs(pos))
             try:
-                submit_order(ctx, symbol, abs(pos), "buy")
+                submit_order(ctx, symbol, abs(pos), 'buy')
             except Exception as exc:
-                logger.warning("SHORT_CLOSE_FAIL | %s %s", symbol, exc)
+                logger.warning('SHORT_CLOSE_FAIL | %s %s', symbol, exc)
             continue
-        # AI-AGENT-REF: Add thread-safe locking for trade cooldown access
         with trade_cooldowns_lock:
             ts = state.trade_cooldowns.get(symbol)
         if ts and (now - ts).total_seconds() < 60:
@@ -8871,61 +5465,44 @@             skipped_cooldown.inc()
             continue
         filtered.append(symbol)
-
-    symbols = filtered  # replace with filtered list
-
+    symbols = filtered
     if cd_skipped:
         log_skip_cooldown(cd_skipped)
 
     def process_symbol(symbol: str) -> None:
         try:
-            logger.info(f"PROCESSING_SYMBOL | symbol={symbol}")
+            logger.info(f'PROCESSING_SYMBOL | symbol={symbol}')
             if not is_market_open():
-                logger.info("MARKET_CLOSED_SKIP_SYMBOL", extra={"symbol": symbol})
+                logger.info('MARKET_CLOSED_SKIP_SYMBOL', extra={'symbol': symbol})
                 return
             try:
                 price_df = fetch_minute_df_safe(symbol)
             except DataFetchError:
-                logger.info(f"SKIP_NO_PRICE_DATA | {symbol}")
+                logger.info(f'SKIP_NO_PRICE_DATA | {symbol}')
                 return
-            # AI-AGENT-REF: record raw row count before validation
             row_counts[symbol] = len(price_df)
-            logger.info(f"FETCHED_ROWS | {symbol} rows={len(price_df)}")
-            if price_df.empty or "close" not in price_df.columns:
-                logger.info(f"SKIP_NO_PRICE_DATA | {symbol}")
+            logger.info(f'FETCHED_ROWS | {symbol} rows={len(price_df)}')
+            if price_df.empty or 'close' not in price_df.columns:
+                logger.info(f'SKIP_NO_PRICE_DATA | {symbol}')
                 return
             if symbol in state.position_cache:
-                return  # AI-AGENT-REF: skip symbol with open position
+                return
             processed.append(symbol)
             _safe_trade(ctx, state, symbol, current_cash, model, regime_ok)
         except Exception as exc:
-            logger.error(f"Error processing {symbol}: {exc}", exc_info=True)
-
+            logger.error(f'Error processing {symbol}: {exc}', exc_info=True)
     futures = [prediction_executor.submit(process_symbol, s) for s in symbols]
     for f in futures:
         f.result()
-    return processed, row_counts
-
+    return (processed, row_counts)
 
 def _log_loop_heartbeat(loop_id: str, start: float) -> None:
     duration = time.monotonic() - start
-    logger.info(
-        "HEARTBEAT",
-        extra={
-            "loop_id": loop_id,
-            "timestamp": utc_now_iso(),  # AI-AGENT-REF: Use UTC timestamp utility
-            "duration": duration,
-        },
-    )
-
+    logger.info('HEARTBEAT', extra={'loop_id': loop_id, 'timestamp': utc_now_iso(), 'duration': duration})
 
 def _send_heartbeat() -> None:
     """Lightweight heartbeat when halted."""
-    logger.info(
-        "HEARTBEAT_HALTED",
-        extra={"timestamp": utc_now_iso()},  # AI-AGENT-REF: Use UTC timestamp utility
-    )
-
+    logger.info('HEARTBEAT_HALTED', extra={'timestamp': utc_now_iso()})
 
 def manage_position_risk(ctx: BotContext, position) -> None:
     """Adjust trailing stops and position size while halted."""
@@ -8936,68 +5513,54 @@         try:
             price_df = fetch_minute_df_safe(symbol)
         except DataFetchError:
-            logger.critical(f"No minute data for {symbol}, skipping.")
+            logger.critical(f'No minute data for {symbol}, skipping.')
             return
-        logger.debug(f"Latest rows for {symbol}:\n{price_df.tail(3)}")
-        if "close" in price_df.columns:
-            price_series = price_df["close"].dropna()
+        logger.debug(f'Latest rows for {symbol}:\n{price_df.tail(3)}')
+        if 'close' in price_df.columns:
+            price_series = price_df['close'].dropna()
             if not price_series.empty:
                 price = price_series.iloc[-1]
-                logger.debug(f"Final extracted price for {symbol}: {price}")
+                logger.debug(f'Final extracted price for {symbol}: {price}')
             else:
-                logger.critical(f"No valid close prices found for {symbol}, skipping.")
+                logger.critical(f'No valid close prices found for {symbol}, skipping.')
                 price = 0.0
         else:
-            logger.critical(f"Close column missing for {symbol}, skipping.")
+            logger.critical(f'Close column missing for {symbol}, skipping.')
             price = 0.0
         if price <= 0 or pd.isna(price):
-            logger.critical(f"Invalid price computed for {symbol}: {price}")
+            logger.critical(f'Invalid price computed for {symbol}: {price}')
             return
-        side = "long" if int(position.qty) > 0 else "short"
-        if side == "long":
-            new_stop = float(position.avg_entry_price) * (
-                1 - min(0.01 + atr / 100, 0.03)
-            )
+        side = 'long' if int(position.qty) > 0 else 'short'
+        if side == 'long':
+            new_stop = float(position.avg_entry_price) * (1 - min(0.01 + atr / 100, 0.03))
         else:
-            new_stop = float(position.avg_entry_price) * (
-                1 + min(0.01 + atr / 100, 0.03)
-            )
+            new_stop = float(position.avg_entry_price) * (1 + min(0.01 + atr / 100, 0.03))
         update_trailing_stop(ctx, symbol, price, int(position.qty), atr)
-        pnl = float(getattr(position, "unrealized_plpc", 0))
+        pnl = float(getattr(position, 'unrealized_plpc', 0))
         kelly_scale = compute_kelly_scale(atr, 0.0)
         adjust_position_size(position, kelly_scale)
         volume_factor = utils.get_volume_spike_factor(symbol)
         ml_conf = utils.get_ml_confidence(symbol)
-        if (
-            volume_factor > config.VOLUME_SPIKE_THRESHOLD
-            and ml_conf > config.ML_CONFIDENCE_THRESHOLD
-        ) and side == "long" and price > vwap and pnl > 0.02:
-            pyramid_add_position(ctx, symbol, config.PYRAMID_LEVELS["low"], side)
-        logger.info(
-            f"HALT_MANAGE {symbol} stop={new_stop:.2f} vwap={vwap:.2f} vol={volume_factor:.2f} ml={ml_conf:.2f}"
-        )
-    except Exception as exc:  # pragma: no cover - handle edge cases
-        logger.warning(f"manage_position_risk failed for {symbol}: {exc}")
-
-
-def pyramid_add_position(
-    ctx: BotContext, symbol: str, fraction: float, side: str
-) -> None:
+        if (volume_factor > S.volume_spike_threshold and ml_conf > S.ml_confidence_threshold) and side == 'long' and (price > vwap) and (pnl > 0.02):
+            pyramid_add_position(ctx, symbol, S.pyramid_levels['low'], side)
+        logger.info(f'HALT_MANAGE {symbol} stop={new_stop:.2f} vwap={vwap:.2f} vol={volume_factor:.2f} ml={ml_conf:.2f}')
+    except Exception as exc:
+        logger.warning(f'manage_position_risk failed for {symbol}: {exc}')
+
+def pyramid_add_position(ctx: BotContext, symbol: str, fraction: float, side: str) -> None:
     current_qty = _current_position_qty(ctx, symbol)
     add_qty = max(1, int(abs(current_qty) * fraction))
-    submit_order(ctx, symbol, add_qty, "buy" if side == "long" else "sell")
-    logger.info("PYRAMID_ADD", extra={"symbol": symbol, "qty": add_qty, "side": side})
-
+    submit_order(ctx, symbol, add_qty, 'buy' if side == 'long' else 'sell')
+    logger.info('PYRAMID_ADD', extra={'symbol': symbol, 'qty': add_qty, 'side': side})
 
 def reduce_position_size(ctx: BotContext, symbol: str, fraction: float) -> None:
     current_qty = _current_position_qty(ctx, symbol)
     reduce_qty = max(1, int(abs(current_qty) * fraction))
-    side = "sell" if current_qty > 0 else "buy"
+    side = 'sell' if current_qty > 0 else 'buy'
     submit_order(ctx, symbol, reduce_qty, side)
-    logger.info("REDUCE_POSITION", extra={"symbol": symbol, "qty": reduce_qty})
-
-
-@memory_profile  # AI-AGENT-REF: Monitor memory usage of main trading function
+    logger.info('REDUCE_POSITION', extra={'symbol': symbol, 'qty': reduce_qty})
+
+@memory_profile
 def run_all_trades_worker(state: BotState, model) -> None:
     """
     Execute the complete trading cycle for all candidate symbols.
@@ -9128,41 +5691,33 @@     """
     _init_metrics()
     import uuid
-
     loop_id = str(uuid.uuid4())
     acquired = run_lock.acquire(blocking=False)
     if not acquired:
-        logger.info("RUN_ALL_TRADES_SKIPPED_OVERLAP")
+        logger.info('RUN_ALL_TRADES_SKIPPED_OVERLAP')
         return
-    try:  # AI-AGENT-REF: ensure lock released on every exit
+    try:
         try:
             ctx.risk_engine.wait_for_exposure_update(0.5)
         except Exception as e:
-            # Risk engine update failed - log warning but continue
-            logger.warning("Risk engine exposure update failed: %s", e)
-        if not hasattr(state, "trade_cooldowns"):
+            logger.warning('Risk engine exposure update failed: %s', e)
+        if not hasattr(state, 'trade_cooldowns'):
             state.trade_cooldowns = {}
-        if not hasattr(state, "last_trade_direction"):
+        if not hasattr(state, 'last_trade_direction'):
             state.last_trade_direction = {}
         if state.running:
-            logger.warning(
-                "RUN_ALL_TRADES_SKIPPED_OVERLAP",
-                extra={"last_duration": getattr(state, "last_loop_duration", 0.0)},
-            )
+            logger.warning('RUN_ALL_TRADES_SKIPPED_OVERLAP', extra={'last_duration': getattr(state, 'last_loop_duration', 0.0)})
             return
         now = datetime.now(UTC)
         for sym, ts in list(state.trade_cooldowns.items()):
             if (now - ts).total_seconds() > TRADE_COOLDOWN_MIN * 60:
                 state.trade_cooldowns.pop(sym, None)
-        if (
-            state.last_run_at
-            and (now - state.last_run_at).total_seconds() < RUN_INTERVAL_SECONDS
-        ):
-            logger.warning("RUN_ALL_TRADES_SKIPPED_RECENT")
+        if state.last_run_at and (now - state.last_run_at).total_seconds() < RUN_INTERVAL_SECONDS:
+            logger.warning('RUN_ALL_TRADES_SKIPPED_RECENT')
             return
         if not is_market_open():
-            logger.info("MARKET_CLOSED_NO_FETCH")
-            return  # FIXED: skip work when market closed
+            logger.info('MARKET_CLOSED_NO_FETCH')
+            return
         state.pdt_blocked = check_pdt_rule(ctx)
         if state.pdt_blocked:
             return
@@ -9170,198 +5725,93 @@         state.last_run_at = now
         loop_start = time.monotonic()
         try:
-            # AI-AGENT-REF: avoid overlapping cycles if any orders are pending
             try:
-                open_orders = ctx.api.list_orders(status="open")
-            except Exception as exc:  # pragma: no cover - network issues
-                logger.debug(f"order check failed: {exc}")
+                open_orders = ctx.api.list_orders(status='open')
+            except Exception as exc:
+                logger.debug(f'order check failed: {exc}')
                 open_orders = []
-            if any(o.status in ("new", "pending_new") for o in open_orders):
-                logger.warning("Detected pending orders; skipping this trade cycle")
+            if any((o.status in ('new', 'pending_new') for o in open_orders)):
+                logger.warning('Detected pending orders; skipping this trade cycle')
                 return
-            if config.VERBOSE:
-                logger.info(
-                    "RUN_ALL_TRADES_START",
-                    extra={"timestamp": utc_now_iso()},
-                )
-
+            if S.verbose:
+                logger.info('RUN_ALL_TRADES_START', extra={'timestamp': utc_now_iso()})
             current_cash, regime_ok, symbols = _prepare_run(ctx, state)
-
-            # AI-AGENT-REF: Add memory monitoring and cleanup to prevent resource issues
             if MEMORY_OPTIMIZATION_AVAILABLE:
                 try:
                     memory_stats = optimize_memory()
-                    if (
-                        memory_stats.get("memory_usage_mb", 0) > 512
-                    ):  # If using more than 512MB
-                        logger.warning(
-                            "HIGH_MEMORY_USAGE_DETECTED",
-                            extra={
-                                "memory_usage_mb": memory_stats.get(
-                                    "memory_usage_mb", 0
-                                ),
-                                "symbols_count": len(symbols),
-                            },
-                        )
-                        # Emergency cleanup if memory is too high
-                        if (
-                            memory_stats.get("memory_usage_mb", 0) > 1024
-                        ):  # 1GB threshold
-                            logger.critical("EMERGENCY_MEMORY_CLEANUP_TRIGGERED")
+                    if memory_stats.get('memory_usage_mb', 0) > 512:
+                        logger.warning('HIGH_MEMORY_USAGE_DETECTED', extra={'memory_usage_mb': memory_stats.get('memory_usage_mb', 0), 'symbols_count': len(symbols)})
+                        if memory_stats.get('memory_usage_mb', 0) > 1024:
+                            logger.critical('EMERGENCY_MEMORY_CLEANUP_TRIGGERED')
                             emergency_memory_cleanup()
                 except Exception as exc:
-                    logger.debug(f"Memory optimization check failed: {exc}")
-
-            # AI-AGENT-REF: Update drawdown circuit breaker with current equity
+                    logger.debug(f'Memory optimization check failed: {exc}')
             if ctx.drawdown_circuit_breaker:
                 try:
                     acct = ctx.api.get_account()
                     current_equity = float(acct.equity) if acct else 0.0
-                    trading_allowed = ctx.drawdown_circuit_breaker.update_equity(
-                        current_equity
-                    )
-
-                    # AI-AGENT-REF: Get status once to avoid UnboundLocalError in else block
+                    trading_allowed = ctx.drawdown_circuit_breaker.update_equity(current_equity)
                     status = ctx.drawdown_circuit_breaker.get_status()
-
                     if not trading_allowed:
-                        logger.critical(
-                            "TRADING_HALTED_DRAWDOWN_PROTECTION",
-                            extra={
-                                "current_drawdown": status["current_drawdown"],
-                                "max_drawdown": status["max_drawdown"],
-                                "peak_equity": status["peak_equity"],
-                                "current_equity": current_equity,
-                            },
-                        )
-                        # Manage existing positions but skip new trades
+                        logger.critical('TRADING_HALTED_DRAWDOWN_PROTECTION', extra={'current_drawdown': status['current_drawdown'], 'max_drawdown': status['max_drawdown'], 'peak_equity': status['peak_equity'], 'current_equity': current_equity})
                         try:
                             portfolio = ctx.api.get_all_positions()
                             for pos in portfolio:
                                 manage_position_risk(ctx, pos)
                         except Exception as exc:
-                            logger.warning(f"HALT_MANAGE_FAIL: {exc}")
+                            logger.warning(f'HALT_MANAGE_FAIL: {exc}')
                         return
                     else:
-                        # Log drawdown status for monitoring
-                        logger.debug(
-                            "DRAWDOWN_STATUS_OK",
-                            extra={
-                                "current_drawdown": status["current_drawdown"],
-                                "max_drawdown": status["max_drawdown"],
-                                "trading_allowed": status["trading_allowed"],
-                            },
-                        )
+                        logger.debug('DRAWDOWN_STATUS_OK', extra={'current_drawdown': status['current_drawdown'], 'max_drawdown': status['max_drawdown'], 'trading_allowed': status['trading_allowed']})
                 except Exception as exc:
-                    logger.error(f"Drawdown circuit breaker update failed: {exc}")
-                    # Continue trading but log the error for investigation
-
-            # AI-AGENT-REF: honor global halt flag before processing symbols
+                    logger.error(f'Drawdown circuit breaker update failed: {exc}')
             if check_halt_flag(ctx):
-                _log_health_diagnostics(ctx, "halt_flag_loop")
-                logger.info(
-                    "TRADING_HALTED_VIA_FLAG: Managing existing positions only."
-                )
+                _log_health_diagnostics(ctx, 'halt_flag_loop')
+                logger.info('TRADING_HALTED_VIA_FLAG: Managing existing positions only.')
                 try:
                     portfolio = ctx.api.get_all_positions()
                     for pos in portfolio:
                         manage_position_risk(ctx, pos)
-                except Exception as exc:  # pragma: no cover - network issues
-                    logger.warning(f"HALT_MANAGE_FAIL: {exc}")
-                logger.info("HALT_SKIP_NEW_TRADES")
+                except Exception as exc:
+                    logger.warning(f'HALT_MANAGE_FAIL: {exc}')
+                logger.info('HALT_SKIP_NEW_TRADES')
                 _send_heartbeat()
-                # log summary even when halted
                 try:
                     acct = ctx.api.get_account()
                     cash = float(acct.cash)
                     equity = float(acct.equity)
                     positions = ctx.api.get_all_positions()
-                    logger.debug("Raw Alpaca positions: %s", positions)
-                    exposure = (
-                        sum(abs(float(p.market_value)) for p in positions)
-                        / equity
-                        * 100
-                        if equity > 0
-                        else 0.0
-                    )
-                    logger.info(
-                        f"Portfolio summary: cash=${cash:.2f}, equity=${equity:.2f}, exposure={exposure:.2f}%, positions={len(positions)}"
-                    )
-                    logger.info(
-                        "POSITIONS_DETAIL",
-                        extra={
-                            "positions": [
-                                {
-                                    "symbol": p.symbol,
-                                    "qty": int(p.qty),
-                                    "avg_price": float(p.avg_entry_price),
-                                    "market_value": float(p.market_value),
-                                }
-                                for p in positions
-                            ],
-                        },
-                    )
-                    logger.info(
-                        "WEIGHTS_VS_POSITIONS",
-                        extra={
-                            "weights": ctx.portfolio_weights,
-                            "positions": {p.symbol: int(p.qty) for p in positions},
-                            "cash": cash,
-                        },
-                    )
-                except Exception as exc:  # pragma: no cover - network issues
-                    logger.warning(f"SUMMARY_FAIL: {exc}")
+                    logger.debug('Raw Alpaca positions: %s', positions)
+                    exposure = sum((abs(float(p.market_value)) for p in positions)) / equity * 100 if equity > 0 else 0.0
+                    logger.info(f'Portfolio summary: cash=${cash:.2f}, equity=${equity:.2f}, exposure={exposure:.2f}%, positions={len(positions)}')
+                    logger.info('POSITIONS_DETAIL', extra={'positions': [{'symbol': p.symbol, 'qty': int(p.qty), 'avg_price': float(p.avg_entry_price), 'market_value': float(p.market_value)} for p in positions]})
+                    logger.info('WEIGHTS_VS_POSITIONS', extra={'weights': ctx.portfolio_weights, 'positions': {p.symbol: int(p.qty) for p in positions}, 'cash': cash})
+                except Exception as exc:
+                    logger.warning(f'SUMMARY_FAIL: {exc}')
                 return
-
             retries = 3
-            processed, row_counts = [], {}
+            processed, row_counts = ([], {})
             for attempt in range(retries):
-                processed, row_counts = _process_symbols(
-                    symbols, current_cash, model, regime_ok
-                )
+                processed, row_counts = _process_symbols(symbols, current_cash, model, regime_ok)
                 if processed:
                     if attempt:
-                        logger.info(
-                            "DATA_SOURCE_RETRY_SUCCESS",
-                            extra={"attempt": attempt + 1, "symbols": symbols},
-                        )
+                        logger.info('DATA_SOURCE_RETRY_SUCCESS', extra={'attempt': attempt + 1, 'symbols': symbols})
                     break
                 time.sleep(2)
-
-            # AI-AGENT-REF: abort only if all symbols returned zero rows
             if sum(row_counts.values()) == 0:
                 last_ts = None
                 for sym in symbols:
                     ts = ctx.data_fetcher._minute_timestamps.get(sym)
                     if last_ts is None or (ts and ts > last_ts):
                         last_ts = ts
-                logger.critical(
-                    "DATA_SOURCE_EMPTY",
-                    extra={
-                        "symbols": symbols,
-                        "endpoint": "minute",
-                        "last_success": last_ts.isoformat() if last_ts else "unknown",
-                        "row_counts": row_counts,
-                    },
-                )
-                logger.info(
-                    "DATA_SOURCE_RETRY_FAILED",
-                    extra={"attempts": retries, "symbols": symbols},
-                )
-                # AI-AGENT-REF: exit immediately on repeated data failure
+                logger.critical('DATA_SOURCE_EMPTY', extra={'symbols': symbols, 'endpoint': 'minute', 'last_success': last_ts.isoformat() if last_ts else 'unknown', 'row_counts': row_counts})
+                logger.info('DATA_SOURCE_RETRY_FAILED', extra={'attempts': retries, 'symbols': symbols})
                 return
             else:
-                logger.info(
-                    "DATA_SOURCE_RETRY_FINAL",
-                    extra={"success": True, "attempts": attempt + 1},
-                )
-
+                logger.info('DATA_SOURCE_RETRY_FINAL', extra={'success': True, 'attempts': attempt + 1})
             skipped = [s for s in symbols if s not in processed]
             if skipped:
-                logger.info(
-                    "CYCLE_SKIPPED_SUMMARY",
-                    extra={"count": len(skipped), "symbols": skipped},
-                )
+                logger.info('CYCLE_SKIPPED_SUMMARY', extra={'count': len(skipped), 'symbols': skipped})
                 if len(skipped) == len(symbols):
                     state.skipped_cycles += 1
                 else:
@@ -9369,248 +5819,148 @@             else:
                 state.skipped_cycles = 0
             if state.skipped_cycles >= 2:
-                logger.critical(
-                    "ALL_SYMBOLS_SKIPPED_TWO_CYCLES",
-                    extra={
-                        "hint": "Check data provider API keys and entitlements; test data fetch manually from the server; review data fetcher logs",
-                    },
-                )
-
+                logger.critical('ALL_SYMBOLS_SKIPPED_TWO_CYCLES', extra={'hint': 'Check data provider API keys and entitlements; test data fetch manually from the server; review data fetcher logs'})
             run_multi_strategy(ctx)
             try:
                 ctx.risk_engine.refresh_positions(ctx.api)
                 pos_list = ctx.api.get_all_positions()
                 state.position_cache = {p.symbol: int(p.qty) for p in pos_list}
-                state.long_positions = {
-                    s for s, q in state.position_cache.items() if q > 0
-                }
-                state.short_positions = {
-                    s for s, q in state.position_cache.items() if q < 0
-                }
+                state.long_positions = {s for s, q in state.position_cache.items() if q > 0}
+                state.short_positions = {s for s, q in state.position_cache.items() if q < 0}
                 if ctx.execution_engine:
                     ctx.execution_engine.check_trailing_stops()
-            except Exception as exc:  # pragma: no cover - safety
-                logger.warning("refresh_positions failed: %s", exc)
-            logger.info(
-                f"RUN_ALL_TRADES_COMPLETE | processed={len(row_counts)} symbols, total_rows={sum(row_counts.values())}"
-            )
+            except Exception as exc:
+                logger.warning('refresh_positions failed: %s', exc)
+            logger.info(f'RUN_ALL_TRADES_COMPLETE | processed={len(row_counts)} symbols, total_rows={sum(row_counts.values())}')
             try:
                 acct = ctx.api.get_account()
                 cash = float(acct.cash)
                 equity = float(acct.equity)
                 positions = ctx.api.get_all_positions()
-                logger.debug("Raw Alpaca positions: %s", positions)
-                # ai_trading.csv:9422 - Replace import guard with hard import (required dependencies)
+                logger.debug('Raw Alpaca positions: %s', positions)
                 from ai_trading import portfolio
                 from ai_trading.utils import portfolio_lock
                 try:
                     with portfolio_lock:
-                        ctx.portfolio_weights = portfolio.compute_portfolio_weights(
-                            ctx, [p.symbol for p in positions]
-                        )
+                        ctx.portfolio_weights = portfolio.compute_portfolio_weights(ctx, [p.symbol for p in positions])
                 except Exception:
-                    logger.warning("weight recompute failed", exc_info=True)
-                exposure = (
-                    sum(abs(float(p.market_value)) for p in positions) / equity * 100
-                    if equity > 0
-                    else 0.0
-                )
-                logger.info(
-                    f"Portfolio summary: cash=${cash:.2f}, equity=${equity:.2f}, exposure={exposure:.2f}%, positions={len(positions)}"
-                )
-                logger.info(
-                    "POSITIONS_DETAIL",
-                    extra={
-                        "positions": [
-                            {
-                                "symbol": p.symbol,
-                                "qty": int(p.qty),
-                                "avg_price": float(p.avg_entry_price),
-                                "market_value": float(p.market_value),
-                            }
-                            for p in positions
-                        ],
-                    },
-                )
-                logger.info(
-                    "WEIGHTS_VS_POSITIONS",
-                    extra={
-                        "weights": ctx.portfolio_weights,
-                        "positions": {p.symbol: int(p.qty) for p in positions},
-                        "cash": cash,
-                    },
-                )
+                    logger.warning('weight recompute failed', exc_info=True)
+                exposure = sum((abs(float(p.market_value)) for p in positions)) / equity * 100 if equity > 0 else 0.0
+                logger.info(f'Portfolio summary: cash=${cash:.2f}, equity=${equity:.2f}, exposure={exposure:.2f}%, positions={len(positions)}')
+                logger.info('POSITIONS_DETAIL', extra={'positions': [{'symbol': p.symbol, 'qty': int(p.qty), 'avg_price': float(p.avg_entry_price), 'market_value': float(p.market_value)} for p in positions]})
+                logger.info('WEIGHTS_VS_POSITIONS', extra={'weights': ctx.portfolio_weights, 'positions': {p.symbol: int(p.qty) for p in positions}, 'cash': cash})
                 try:
                     adaptive_cap = ctx.risk_engine._adaptive_global_cap()
                 except Exception:
                     adaptive_cap = 0.0
-                logger.info(
-                    "CYCLE SUMMARY: cash=$%.0f equity=$%.0f exposure=%.0f%% positions=%d adaptive_cap=%.1f",
-                    cash,
-                    equity,
-                    exposure,
-                    len(positions),
-                    adaptive_cap,
-                )
-            except Exception as exc:  # pragma: no cover - network issues
-                logger.warning(f"SUMMARY_FAIL: {exc}")
+                logger.info('CYCLE SUMMARY: cash=$%.0f equity=$%.0f exposure=%.0f%% positions=%d adaptive_cap=%.1f', cash, equity, exposure, len(positions), adaptive_cap)
+            except Exception as exc:
+                logger.warning(f'SUMMARY_FAIL: {exc}')
             try:
                 acct = ctx.api.get_account()
-                # Handle case where account object might not have last_equity attribute
-                last_equity = getattr(acct, "last_equity", acct.equity)
+                last_equity = getattr(acct, 'last_equity', acct.equity)
                 pnl = float(acct.equity) - float(last_equity)
-                logger.info(
-                    "LOOP_PNL",
-                    extra={
-                        "loop_id": loop_id,
-                        "pnl": pnl,
-                        "mode": "SHADOW" if config.SHADOW_MODE else "LIVE",
-                    },
-                )
+                logger.info('LOOP_PNL', extra={'loop_id': loop_id, 'pnl': pnl, 'mode': 'SHADOW' if S.shadow_mode else 'LIVE'})
             except Exception as e:
-                logger.warning(f"Failed P&L retrieval: {e}")
+                logger.warning(f'Failed P&L retrieval: {e}')
         except Exception as e:
-            logger.error(f"Exception in trading loop: {e}", exc_info=True)
+            logger.error(f'Exception in trading loop: {e}', exc_info=True)
         finally:
-            # Always reset running flag
             state.running = False
             state.last_loop_duration = time.monotonic() - loop_start
             _log_loop_heartbeat(loop_id, loop_start)
-
-            # AI-AGENT-REF: Perform memory cleanup after trading cycle
             if MEMORY_OPTIMIZATION_AVAILABLE:
                 try:
                     gc_result = optimize_memory()
-                    if gc_result.get("objects_collected", 0) > 50:
-                        logger.info(
-                            f"Post-cycle GC: {gc_result['objects_collected']} objects collected"
-                        )
+                    if gc_result.get('objects_collected', 0) > 50:
+                        logger.info(f"Post-cycle GC: {gc_result['objects_collected']} objects collected")
                 except Exception as e:
-                    logger.warning(f"Memory optimization failed: {e}")
+                    logger.warning(f'Memory optimization failed: {e}')
     finally:
         if acquired:
             run_lock.release()
 
-
 def schedule_run_all_trades(model):
-    """Spawn run_all_trades_worker if market is open."""  # FIXED
+    """Spawn run_all_trades_worker if market is open."""
     if is_market_open():
-        t = threading.Thread(
-            target=run_all_trades_worker,
-            args=(
-                state,
-                model,
-            ),
-            daemon=True,
-        )
+        t = threading.Thread(target=run_all_trades_worker, args=(state, model), daemon=True)
         t.start()
     else:
-        logger.info("Market closed—skipping run_all_trades.")
-
+        logger.info('Market closed—skipping run_all_trades.')
 
 def schedule_run_all_trades_with_delay(model):
     time.sleep(30)
     schedule_run_all_trades(model)
 
-
 def initial_rebalance(ctx: BotContext, symbols: list[str]) -> None:
     """Initial portfolio rebalancing."""
-
     if ctx.api is None:
-        logger.warning("ctx.api is None - cannot perform initial rebalance")
+        logger.warning('ctx.api is None - cannot perform initial rebalance')
         return
-
     try:
         datetime.now(UTC).astimezone(PACIFIC)
         acct = ctx.api.get_account()
         float(acct.equity)
-
         cash = float(acct.cash)
-        buying_power = float(getattr(acct, "buying_power", cash))
+        buying_power = float(getattr(acct, 'buying_power', cash))
         n = len(symbols)
         if n == 0 or cash <= 0 or buying_power <= 0:
-            logger.info("INITIAL_REBALANCE_NO_SYMBOLS_OR_NO_CASH")
+            logger.info('INITIAL_REBALANCE_NO_SYMBOLS_OR_NO_CASH')
             return
     except Exception as exc:
-        logger.warning("Failed to get account info for initial rebalance: %s", exc)
+        logger.warning('Failed to get account info for initial rebalance: %s', exc)
         return
-
-    # Determine current UTC time
     now_utc = datetime.now(UTC)
-    # If it’s between 00:00 and 00:15 UTC, daily bars may not be published yet.
     if now_utc.hour == 0 and now_utc.minute < 15:
-        logger.info("INITIAL_REBALANCE: Too early—daily bars not live yet.")
+        logger.info('INITIAL_REBALANCE: Too early—daily bars not live yet.')
     else:
-        # Gather all symbols that have a valid, nonzero close
         valid_symbols = []
         valid_prices = {}
         for symbol in symbols:
             df_daily = ctx.data_fetcher.get_daily_df(ctx, symbol)
             price = get_latest_close(df_daily)
             if price <= 0:
-                # skip symbols with no real close data
                 continue
             valid_symbols.append(symbol)
             valid_prices[symbol] = price
-
         if not valid_symbols:
             log_level = logging.ERROR if in_trading_hours(now_utc) else logging.WARNING
-            logger.log(
-                log_level,
-                (
-                    "INITIAL_REBALANCE: No valid prices for any symbol—skipping "
-                    "rebalance. Possible data outage or market holiday. "
-                    "Check data provider/API status."
-                ),
-            )
+            logger.log(log_level, 'INITIAL_REBALANCE: No valid prices for any symbol—skipping rebalance. Possible data outage or market holiday. Check data provider/API status.')
         else:
-            # Compute equal weights on valid symbols only
             total_capital = cash
             weight_per = 1.0 / len(valid_symbols)
-
             positions = {p.symbol: int(p.qty) for p in ctx.api.get_all_positions()}
-
             for sym in valid_symbols:
                 price = valid_prices[sym]
-                target_qty = int((total_capital * weight_per) // price)
+                target_qty = int(total_capital * weight_per // price)
                 current_qty = int(positions.get(sym, 0))
-
                 if current_qty < target_qty:
-                    qty_to_buy = target_qty  # AI-AGENT-REF: retry full amount
+                    qty_to_buy = target_qty
                     if qty_to_buy < 1:
                         continue
                     try:
-                        # AI-AGENT-REF: preserve consistent client_order_id across retries
                         cid = ctx.rebalance_ids.get(sym)
                         if not cid:
-                            cid = f"{sym}-{uuid.uuid4().hex[:8]}"
+                            cid = f'{sym}-{uuid.uuid4().hex[:8]}'
                             ctx.rebalance_ids[sym] = cid
                             ctx.rebalance_attempts[sym] = 0
-                        order = submit_order(ctx, sym, qty_to_buy, "buy")
-                        # AI-AGENT-REF: confirm order result before logging success
+                        order = submit_order(ctx, sym, qty_to_buy, 'buy')
                         if order:
-                            logger.info(f"INITIAL_REBALANCE: Bought {qty_to_buy} {sym}")
+                            logger.info(f'INITIAL_REBALANCE: Bought {qty_to_buy} {sym}')
                             ctx.rebalance_buys[sym] = datetime.now(UTC)
                         else:
-                            logger.error(
-                                f"INITIAL_REBALANCE: Buy failed for {sym}: order not placed"
-                            )
+                            logger.error(f'INITIAL_REBALANCE: Buy failed for {sym}: order not placed')
                     except Exception as e:
-                        logger.error(
-                            f"INITIAL_REBALANCE: Buy failed for {sym}: {repr(e)}"
-                        )
+                        logger.error(f'INITIAL_REBALANCE: Buy failed for {sym}: {repr(e)}')
                 elif current_qty > target_qty:
                     qty_to_sell = current_qty - target_qty
                     if qty_to_sell < 1:
                         continue
                     try:
-                        submit_order(ctx, sym, qty_to_sell, "sell")
-                        logger.info(f"INITIAL_REBALANCE: Sold {qty_to_sell} {sym}")
+                        submit_order(ctx, sym, qty_to_sell, 'sell')
+                        logger.info(f'INITIAL_REBALANCE: Sold {qty_to_sell} {sym}')
                     except Exception as e:
-                        logger.error(
-                            f"INITIAL_REBALANCE: Sell failed for {sym}: {repr(e)}"
-                        )
-
+                        logger.error(f'INITIAL_REBALANCE: Sell failed for {sym}: {repr(e)}')
     ctx.initial_rebalance_done = True
     try:
         pos_list = ctx.api.get_all_positions()
@@ -9618,554 +5968,320 @@         state.long_positions = {s for s, q in state.position_cache.items() if q > 0}
         state.short_positions = {s for s, q in state.position_cache.items() if q < 0}
     except Exception as e:
-        # Failed to refresh position cache - log error but continue
-        logger.error("Failed to refresh position cache after rebalance: %s", e)
-        # Initialize empty cache to prevent AttributeError
+        logger.error('Failed to refresh position cache after rebalance: %s', e)
         state.position_cache = {}
         state.long_positions = set()
         state.short_positions = set()
 
-
 def main() -> None:
-    logger.info("Main trading bot starting...")
-
-    # AI-AGENT-REF: Initialize runtime config and validate credentials
+    logger.info('Main trading bot starting...')
     try:
         init_runtime_config()
     except RuntimeError as e:
-        logger.critical("Runtime configuration failed: %s", e)
+        logger.critical('Runtime configuration failed: %s', e)
         sys.exit(2)
-
-    # AI-AGENT-REF: Validate Alpaca credentials using settings singleton
     cfg = get_settings()
     api_key, api_secret = cfg.get_alpaca_keys()
     if not api_key or not api_secret:
-        logger.critical("Alpaca credentials missing – aborting startup")
-        logger.critical(
-            "Please set ALPACA_API_KEY/APCA_API_KEY_ID and ALPACA_SECRET_KEY/APCA_API_SECRET_KEY"
-        )
+        logger.critical('Alpaca credentials missing – aborting startup')
+        logger.critical('Please set ALPACA_API_KEY/APCA_API_KEY_ID and ALPACA_SECRET_KEY/APCA_API_SECRET_KEY')
         sys.exit(2)
-
-    # Log masked config for verification
-    logger.info("Config: ALPACA_API_KEY=***MASKED***", extra={"present": bool(api_key)})
-    logger.info(
-        "Config: ALPACA_SECRET_KEY=***MASKED***", extra={"present": bool(api_secret)}
-    )
-    logger.info(f"Config: ALPACA_BASE_URL={cfg.alpaca_base_url}")
-    logger.info(f"Config: TRADING_MODE={cfg.trading_mode}")
-
+    logger.info('Config: ALPACA_API_KEY=***MASKED***', extra={'present': bool(api_key)})
+    logger.info('Config: ALPACA_SECRET_KEY=***MASKED***', extra={'present': bool(api_secret)})
+    logger.info(f'Config: ALPACA_BASE_URL={cfg.alpaca_base_url}')
+    logger.info(f'Config: TRADING_MODE={cfg.trading_mode}')
     config.reload_env()
-
-    # AI-AGENT-REF: Ensure only one bot instance is running
     try:
         from process_manager import ProcessManager
-
         pm = ProcessManager()
         if not pm.ensure_single_instance():
-            logger.error("Another trading bot instance is already running. Exiting.")
+            logger.error('Another trading bot instance is already running. Exiting.')
             sys.exit(1)
-        logger.info("Single instance lock acquired successfully")
+        logger.info('Single instance lock acquired successfully')
     except Exception as e:
-        logger.error("Failed to acquire single instance lock: %s", e)
+        logger.error('Failed to acquire single instance lock: %s', e)
         sys.exit(1)
-
-    # AI-AGENT-REF: Add comprehensive health check on startup
     try:
         from health_check import log_health_summary
-
         log_health_summary()
     except Exception as e:
-        logger.warning("Health check failed on startup: %s", e)
+        logger.warning('Health check failed on startup: %s', e)
 
     def _handle_term(signum, frame):
-        logger.info("PROCESS_TERMINATION", extra={"signal": signum})
+        logger.info('PROCESS_TERMINATION', extra={'signal': signum})
         sys.exit(0)
-
     signal.signal(signal.SIGTERM, _handle_term)
     signal.signal(signal.SIGINT, _handle_term)
-
     parser = ArgumentParser()
-    parser.add_argument(
-        "--mode",
-        choices=["aggressive", "balanced", "conservative"],
-        default=BOT_MODE_ENV or "balanced",
-    )
+    parser.add_argument('--mode', choices=['aggressive', 'balanced', 'conservative'], default=BOT_MODE_ENV or 'balanced')
     args = parser.parse_args()
     if args.mode != state.mode_obj.mode:
         state.mode_obj = BotMode(args.mode)
         params.update(state.mode_obj.get_config())
-
-    try:
-        logger.info(">>> BOT __main__ ENTERED – starting up")
-
-        # --- Market hours check ---
-
-        # pd.Timestamp.utcnow() already returns a timezone-aware UTC timestamp,
-        # so calling tz_localize("UTC") would raise an error. Simply use the
-        # timestamp directly to avoid "Cannot localize tz-aware Timestamp".
-        now_utc = pd.Timestamp.now(tz="UTC")
+    try:
+        logger.info('>>> BOT __main__ ENTERED – starting up')
+        now_utc = pd.Timestamp.now(tz='UTC')
         if is_holiday(now_utc):
-            logger.warning(
-                f"No NYSE market schedule for {now_utc.date()}; skipping market open/close check."
-            )
+            logger.warning(f'No NYSE market schedule for {now_utc.date()}; skipping market open/close check.')
             market_open = False
         else:
             try:
                 market_open = NY.open_at_time(get_market_schedule(), now_utc)
             except ValueError as e:
-                logger.warning(
-                    f"Invalid schedule time {now_utc}: {e}; assuming market closed"
-                )
+                logger.warning(f'Invalid schedule time {now_utc}: {e}; assuming market closed')
                 market_open = False
-
         sleep_minutes = 60
         if not market_open:
-            logger.info("Market is closed. Sleeping for %d minutes.", sleep_minutes)
+            logger.info('Market is closed. Sleeping for %d minutes.', sleep_minutes)
             time.sleep(sleep_minutes * 60)
-            # Return control to outer loop instead of exiting
             return
-
-        logger.info("Market is open. Starting trade cycle.")
-
-        # Start Prometheus metrics server on an available port
+        logger.info('Market is open. Starting trade cycle.')
         start_metrics_server(9200)
-
         if RUN_HEALTH:
             Thread(target=start_healthcheck, daemon=True).start()
-
-        # Daily jobs
-        schedule.every().day.at("00:30").do(
-            lambda: Thread(target=daily_summary, daemon=True).start()
-        )
-        schedule.every().day.at("00:05").do(
-            lambda: Thread(target=daily_reset, args=(state,), daemon=True).start()
-        )
-        schedule.every().day.at("10:00").do(
-            lambda: Thread(
-                target=run_meta_learning_weight_optimizer, daemon=True
-            ).start()
-        )
-        schedule.every().day.at("02:00").do(
-            lambda: Thread(
-                target=run_bayesian_meta_learning_optimizer, daemon=True
-            ).start()
-        )
-
-        # Retraining after market close (~16:05 US/Eastern)
-        close_time = (
-            dt_.now(UTC)
-            .astimezone(ZoneInfo("America/New_York"))
-            .replace(hour=16, minute=5, second=0, microsecond=0)
-            .astimezone(UTC)
-            .strftime("%H:%M")
-        )
-        schedule.every().day.at(close_time).do(
-            lambda: Thread(target=on_market_close, daemon=True).start()
-        )
-
-        # ai_trading/core/bot_engine.py:9768 - Convert import guard to settings-gated import
+        schedule.every().day.at('00:30').do(lambda: Thread(target=daily_summary, daemon=True).start())
+        schedule.every().day.at('00:05').do(lambda: Thread(target=daily_reset, args=(state,), daemon=True).start())
+        schedule.every().day.at('10:00').do(lambda: Thread(target=run_meta_learning_weight_optimizer, daemon=True).start())
+        schedule.every().day.at('02:00').do(lambda: Thread(target=run_bayesian_meta_learning_optimizer, daemon=True).start())
+        close_time = dt_.now(UTC).astimezone(ZoneInfo('America/New_York')).replace(hour=16, minute=5, second=0, microsecond=0).astimezone(UTC).strftime('%H:%M')
+        schedule.every().day.at(close_time).do(lambda: Thread(target=on_market_close, daemon=True).start())
         from ai_trading.config import get_settings
         settings = get_settings()
         if not settings.disable_daily_retrain:
-            if settings.enable_sklearn:  # Meta-learning requires sklearn
+            if settings.enable_sklearn:
                 try:
                     from ai_trading.meta_learning import retrain_meta_learner as _tmp_retrain
-                    globals()["retrain_meta_learner"] = _tmp_retrain
+                    globals()['retrain_meta_learner'] = _tmp_retrain
                 except ImportError as e:
-                    globals()["retrain_meta_learner"] = None
-                    raise RuntimeError(
-                        f"Meta-learning enabled but retrain_meta_learner unavailable: {e}. "
-                        "Install sklearn or set ENABLE_SKLEARN=False"
-                    )
+                    globals()['retrain_meta_learner'] = None
+                    raise RuntimeError(f'Meta-learning enabled but retrain_meta_learner unavailable: {e}. Install sklearn or set ENABLE_SKLEARN=False')
             else:
-                globals()["retrain_meta_learner"] = None
-                logger.info("Daily retraining disabled: sklearn not enabled")
+                globals()['retrain_meta_learner'] = None
+                logger.info('Daily retraining disabled: sklearn not enabled')
         else:
-            logger.info("Daily retraining disabled via DISABLE_DAILY_RETRAIN")
-
+            logger.info('Daily retraining disabled via DISABLE_DAILY_RETRAIN')
         try:
             model = load_model(MODEL_PATH)
         except Exception as e:
-            logger.fatal("Could not load model", exc_info=e)
+            logger.fatal('Could not load model', exc_info=e)
             sys.exit(1)
-        logger.info("BOT_LAUNCHED")
+        logger.info('BOT_LAUNCHED')
         cancel_all_open_orders(ctx)
         audit_positions(ctx)
         try:
             initial_list = load_tickers(TICKERS_FILE)
             summary = pre_trade_health_check(ctx, initial_list)
-            logger.info("STARTUP_HEALTH", extra=summary)
-            failures = (
-                summary["failures"]
-                or summary["insufficient_rows"]
-                or summary["missing_columns"]
-                or summary.get("invalid_values")
-                or summary["timezone_issues"]
-            )
-
-            # AI-AGENT-REF: Add bypass for stale data during initial deployment
-            stale_data = summary.get("stale_data", [])
-            allow_stale_on_startup = (
-                os.getenv("ALLOW_STALE_DATA_STARTUP", "true").lower() == "true"
-            )
-
+            logger.info('STARTUP_HEALTH', extra=summary)
+            failures = summary['failures'] or summary['insufficient_rows'] or summary['missing_columns'] or summary.get('invalid_values') or summary['timezone_issues']
+            stale_data = summary.get('stale_data', [])
+            allow_stale_on_startup = os.getenv('ALLOW_STALE_DATA_STARTUP', 'true').lower() == 'true'
             if stale_data and allow_stale_on_startup:
-                logger.warning(
-                    "BYPASS_STALE_DATA_STARTUP: Allowing trading with stale data for initial deployment",
-                    extra={"stale_symbols": stale_data, "count": len(stale_data)},
-                )
-            elif stale_data and not allow_stale_on_startup:
+                logger.warning('BYPASS_STALE_DATA_STARTUP: Allowing trading with stale data for initial deployment', extra={'stale_symbols': stale_data, 'count': len(stale_data)})
+            elif stale_data and (not allow_stale_on_startup):
                 failures = failures or stale_data
-
             health_ok = not failures
             if not health_ok:
-                logger.error("HEALTH_CHECK_FAILED", extra=summary)
+                logger.error('HEALTH_CHECK_FAILED', extra=summary)
                 sys.exit(1)
             else:
-                logger.info("HEALTH_OK")
-            # Prefetch minute history so health check rows are available
+                logger.info('HEALTH_OK')
             for sym in initial_list:
                 try:
-                    ctx.data_fetcher.get_minute_df(
-                        ctx, sym, lookback_minutes=config.MIN_HEALTH_ROWS
-                    )
+                    ctx.data_fetcher.get_minute_df(ctx, sym, lookback_minutes=S.min_health_rows)
                 except Exception as exc:
-                    logger.warning(
-                        "Initial minute prefetch failed for %s: %s", sym, exc
-                    )
+                    logger.warning('Initial minute prefetch failed for %s: %s', sym, exc)
         except Exception as exc:
-            logger.error(f"startup health check failed: {exc}")
+            logger.error(f'startup health check failed: {exc}')
             sys.exit(1)
-
-        # ─── WARM-CACHE SENTIMENT FOR ALL TICKERS ─────────────────────────────────────
-        # This will prevent the initial burst of NewsAPI calls and 429s
         all_tickers = load_tickers(TICKERS_FILE)
         now_ts = pytime.time()
         with sentiment_lock:
             for t in all_tickers:
                 _SENTIMENT_CACHE[t] = (now_ts, 0.0)
-
-        # Initial rebalance (once) only if health check passed
         try:
-            if health_ok and not getattr(ctx, "_rebalance_done", False):
+            if health_ok and (not getattr(ctx, '_rebalance_done', False)):
                 universe = load_tickers(TICKERS_FILE)
                 initial_rebalance(ctx, universe)
                 ctx._rebalance_done = True
         except Exception as e:
-            logger.warning(f"[REBALANCE] aborted due to error: {e}")
-
-        # Recurring jobs
+            logger.warning(f'[REBALANCE] aborted due to error: {e}')
+
         def gather_minute_data_with_delay():
             try:
-                # delay can be configured via env SCHEDULER_SLEEP_SECONDS
-                time.sleep(config.SCHEDULER_SLEEP_SECONDS)
+                time.sleep(S.scheduler_sleep_seconds)
                 schedule_run_all_trades(model)
             except Exception as e:
-                logger.exception(f"gather_minute_data_with_delay failed: {e}")
-
-        schedule.every(1).minutes.do(
-            lambda: Thread(target=gather_minute_data_with_delay, daemon=True).start()
-        )
-
-        # --- run one fetch right away, before entering the loop ---
+                logger.exception(f'gather_minute_data_with_delay failed: {e}')
+        schedule.every(1).minutes.do(lambda: Thread(target=gather_minute_data_with_delay, daemon=True).start())
         try:
             gather_minute_data_with_delay()
         except Exception as e:
-            logger.exception("Initial data fetch failed", exc_info=e)
-        schedule.every(1).minutes.do(
-            lambda: Thread(
-                target=validate_open_orders, args=(ctx,), daemon=True
-            ).start()
-        )
-        schedule.every(6).hours.do(
-            lambda: Thread(target=update_signal_weights, daemon=True).start()
-        )
-        schedule.every(30).minutes.do(
-            lambda: Thread(target=update_bot_mode, args=(state,), daemon=True).start()
-        )
-        schedule.every(30).minutes.do(
-            lambda: Thread(
-                target=adaptive_risk_scaling, args=(ctx,), daemon=True
-            ).start()
-        )
-        schedule.every(config.REBALANCE_INTERVAL_MIN).minutes.do(
-            lambda: Thread(target=maybe_rebalance, args=(ctx,), daemon=True).start()
-        )
-        schedule.every().day.at("23:55").do(
-            lambda: Thread(target=check_disaster_halt, daemon=True).start()
-        )
-
-        # Start listening for trade updates in a background thread
+            logger.exception('Initial data fetch failed', exc_info=e)
+        schedule.every(1).minutes.do(lambda: Thread(target=validate_open_orders, args=(ctx,), daemon=True).start())
+        schedule.every(6).hours.do(lambda: Thread(target=update_signal_weights, daemon=True).start())
+        schedule.every(30).minutes.do(lambda: Thread(target=update_bot_mode, args=(state,), daemon=True).start())
+        schedule.every(30).minutes.do(lambda: Thread(target=adaptive_risk_scaling, args=(ctx,), daemon=True).start())
+        schedule.every(S.rebalance_interval_min).minutes.do(lambda: Thread(target=maybe_rebalance, args=(ctx,), daemon=True).start())
+        schedule.every().day.at('23:55').do(lambda: Thread(target=check_disaster_halt, daemon=True).start())
         ctx.stream_event = asyncio.Event()
         ctx.stream_event.set()
-        threading.Thread(
-            target=lambda: asyncio.run(
-                start_trade_updates_stream(
-                    API_KEY,
-                    API_SECRET,
-                    trading_client,
-                    state,
-                    paper=True,
-                    running=ctx.stream_event,
-                )
-            ),
-            daemon=True,
-        ).start()
-
+        threading.Thread(target=lambda: asyncio.run(start_trade_updates_stream(API_KEY, API_SECRET, trading_client, state, paper=True, running=ctx.stream_event)), daemon=True).start()
     except Exception as e:
-        logger.exception(f"Fatal error in main: {e}")
+        logger.exception(f'Fatal error in main: {e}')
         raise
-
 
 @profile
 def prepare_indicators_simple(df: pd.DataFrame) -> pd.DataFrame:
     if df is None or df.empty:
-        logger.error("Input dataframe is None or empty in prepare_indicators.")
-        raise ValueError("Input dataframe is None or empty")
-
-    try:
-        macd_line, signal_line, hist = simple_calculate_macd(df["close"])
+        logger.error('Input dataframe is None or empty in prepare_indicators.')
+        raise ValueError('Input dataframe is None or empty')
+    try:
+        macd_line, signal_line, hist = simple_calculate_macd(df['close'])
     except Exception as e:
-        logger.error(f"MACD calculation failed: {e}", exc_info=True)
-        raise ValueError("MACD calculation failed") from e
-
+        logger.error(f'MACD calculation failed: {e}', exc_info=True)
+        raise ValueError('MACD calculation failed') from e
     if macd_line is None or signal_line is None or hist is None:
-        logger.error("MACD returned None")
-        raise ValueError("MACD returned None")
-
-    df["macd_line"] = macd_line
-    df["signal_line"] = signal_line
-    df["histogram"] = hist
-
+        logger.error('MACD returned None')
+        raise ValueError('MACD returned None')
+    df['macd_line'] = macd_line
+    df['signal_line'] = signal_line
+    df['histogram'] = hist
     return df
 
-
-def simple_calculate_macd(
-    close_prices: pd.Series,
-    fast: int = 12,
-    slow: int = 26,
-    signal: int = 9,
-) -> tuple[pd.Series | None, pd.Series | None, pd.Series | None]:
+def simple_calculate_macd(close_prices: pd.Series, fast: int=12, slow: int=26, signal: int=9) -> tuple[pd.Series | None, pd.Series | None, pd.Series | None]:
     if close_prices is None or close_prices.empty:
-        logger.warning("Empty or None close_prices passed to calculate_macd.")
-        return None, None, None
-
+        logger.warning('Empty or None close_prices passed to calculate_macd.')
+        return (None, None, None)
     try:
         exp1 = close_prices.ewm(span=fast, adjust=False).mean()
         exp2 = close_prices.ewm(span=slow, adjust=False).mean()
         macd_line = exp1 - exp2
         signal_line = macd_line.ewm(span=signal, adjust=False).mean()
         histogram = macd_line - signal_line
-        return macd_line, signal_line, histogram
+        return (macd_line, signal_line, histogram)
     except Exception as e:
-        logger.error(f"Exception in MACD calculation: {e}", exc_info=True)
-        return None, None, None
-
-
-def compute_ichimoku(
-    high: pd.Series, low: pd.Series, close: pd.Series
-) -> tuple[pd.DataFrame, pd.DataFrame]:
+        logger.error(f'Exception in MACD calculation: {e}', exc_info=True)
+        return (None, None, None)
+
+def compute_ichimoku(high: pd.Series, low: pd.Series, close: pd.Series) -> tuple[pd.DataFrame, pd.DataFrame]:
     """Return Ichimoku lines and signal DataFrames."""
     try:
-        ich_func = getattr(ta, "ichimoku", None)
+        ich_func = getattr(ta, 'ichimoku', None)
         if ich_func is None:
             try:
-                from ai_trading.indicators import ichimoku_fallback  # type: ignore
-
+                from ai_trading.indicators import ichimoku_fallback
                 ich_func = ichimoku_fallback
-            except Exception:  # pragma: no cover
-                logger.warning("ichimoku indicators not available")
+            except Exception:
+                logger.warning('ichimoku indicators not available')
                 ich_func = None
-
         if ich_func:
             ich = ich_func(high=high, low=low, close=close)
         else:
-            # Return empty dataframes if no ichimoku function available
-            return pd.DataFrame(index=high.index), pd.DataFrame(index=high.index)
+            return (pd.DataFrame(index=high.index), pd.DataFrame(index=high.index))
         if isinstance(ich, tuple):
             ich_df = ich[0]
             signal_df = ich[1] if len(ich) > 1 else pd.DataFrame(index=ich_df.index)
         else:
             ich_df = ich
             signal_df = pd.DataFrame(index=ich_df.index)
-        # AI-AGENT-REF: Use attribute check instead of isinstance to avoid type errors
-        if not hasattr(ich_df, "iloc") or not hasattr(ich_df, "columns"):
+        if not hasattr(ich_df, 'iloc') or not hasattr(ich_df, 'columns'):
             ich_df = pd.DataFrame(ich_df)
-        if not hasattr(signal_df, "iloc") or not hasattr(signal_df, "columns"):
+        if not hasattr(signal_df, 'iloc') or not hasattr(signal_df, 'columns'):
             signal_df = pd.DataFrame(signal_df)
-        return ich_df, signal_df
-    except Exception as exc:  # pragma: no cover - defensive
-        log_warning("INDICATOR_ICHIMOKU_FAIL", exc=exc)
-        return pd.DataFrame(), pd.DataFrame()
-
-
-def ichimoku_indicator(
-    df: pd.DataFrame,
-    symbol: str,
-    state: BotState | None = None,
-) -> tuple[pd.DataFrame, Any | None]:
+        return (ich_df, signal_df)
+    except Exception as exc:
+        log_warning('INDICATOR_ICHIMOKU_FAIL', exc=exc)
+        return (pd.DataFrame(), pd.DataFrame())
+
+def ichimoku_indicator(df: pd.DataFrame, symbol: str, state: BotState | None=None) -> tuple[pd.DataFrame, Any | None]:
     """Return Ichimoku indicator DataFrame and optional params."""
     try:
-        ich_func = getattr(ta, "ichimoku", None)
+        ich_func = getattr(ta, 'ichimoku', None)
         if ich_func is None:
             try:
-                from ai_trading.indicators import ichimoku_fallback  # type: ignore
-
+                from ai_trading.indicators import ichimoku_fallback
                 ich_func = ichimoku_fallback
-            except Exception:  # pragma: no cover
-                logger.warning("ichimoku indicators not available")
+            except Exception:
+                logger.warning('ichimoku indicators not available')
                 ich_func = None
-
         if ich_func:
-            ich = ich_func(high=df["high"], low=df["low"], close=df["close"])
+            ich = ich_func(high=df['high'], low=df['low'], close=df['close'])
         if isinstance(ich, tuple):
             ich_df = ich[0]
             params = ich[1] if len(ich) > 1 else None
         else:
             ich_df = ich
             params = None
-        return ich_df, params
-    except Exception as exc:  # pragma: no cover - defensive
-        log_warning("INDICATOR_ICHIMOKU_FAIL", exc=exc, extra={"symbol": symbol})
+        return (ich_df, params)
+    except Exception as exc:
+        log_warning('INDICATOR_ICHIMOKU_FAIL', exc=exc, extra={'symbol': symbol})
         if state:
             state.indicator_failures += 1
-        return pd.DataFrame(), None
-
-
-def _check_trade_frequency_limits(
-    state: BotState, symbol: str, current_time: datetime
-) -> bool:
+        return (pd.DataFrame(), None)
+
+def _check_trade_frequency_limits(state: BotState, symbol: str, current_time: datetime) -> bool:
     """
     Check if trading would exceed frequency limits.
 
     AI-AGENT-REF: Enhanced overtrading prevention with configurable frequency limits.
     Returns True if trade should be skipped due to frequency limits.
     """
-    if not hasattr(state, "trade_history"):
+    if not hasattr(state, 'trade_history'):
         state.trade_history = []
-
-    # Clean up old entries (older than 1 day)
     day_ago = current_time - timedelta(days=1)
     state.trade_history = [(sym, ts) for sym, ts in state.trade_history if ts > day_ago]
-
-    # Count trades in different time windows
     hour_ago = current_time - timedelta(hours=TRADE_FREQUENCY_WINDOW_HOURS)
-
-    # Count symbol-specific trades in last hour
-    symbol_trades_hour = len(
-        [
-            (sym, ts)
-            for sym, ts in state.trade_history
-            if sym == symbol and ts > hour_ago
-        ]
-    )
-
-    # Count total trades in last hour
-    total_trades_hour = len(
-        [(sym, ts) for sym, ts in state.trade_history if ts > hour_ago]
-    )
-
-    # Count total trades in last day
+    symbol_trades_hour = len([(sym, ts) for sym, ts in state.trade_history if sym == symbol and ts > hour_ago])
+    total_trades_hour = len([(sym, ts) for sym, ts in state.trade_history if ts > hour_ago])
     total_trades_day = len(state.trade_history)
-
-    # Check hourly limits
     if total_trades_hour >= MAX_TRADES_PER_HOUR:
-        logger.warning(
-            "FREQUENCY_LIMIT_HOURLY_EXCEEDED",
-            extra={
-                "symbol": symbol,
-                "trades_last_hour": total_trades_hour,
-                "max_per_hour": MAX_TRADES_PER_HOUR,
-                "recommendation": "Reduce trading frequency to prevent overtrading",
-            },
-        )
+        logger.warning('FREQUENCY_LIMIT_HOURLY_EXCEEDED', extra={'symbol': symbol, 'trades_last_hour': total_trades_hour, 'max_per_hour': MAX_TRADES_PER_HOUR, 'recommendation': 'Reduce trading frequency to prevent overtrading'})
         return True
-
-    # Check daily limits
     if total_trades_day >= MAX_TRADES_PER_DAY:
-        logger.warning(
-            "FREQUENCY_LIMIT_DAILY_EXCEEDED",
-            extra={
-                "symbol": symbol,
-                "trades_today": total_trades_day,
-                "max_per_day": MAX_TRADES_PER_DAY,
-                "recommendation": "Daily trade limit reached - consider reviewing strategy",
-            },
-        )
+        logger.warning('FREQUENCY_LIMIT_DAILY_EXCEEDED', extra={'symbol': symbol, 'trades_today': total_trades_day, 'max_per_day': MAX_TRADES_PER_DAY, 'recommendation': 'Daily trade limit reached - consider reviewing strategy'})
         return True
-
-    # Check symbol-specific hourly limit (prevent rapid ping-pong on same symbol)
-    symbol_hourly_limit = max(
-        1, MAX_TRADES_PER_HOUR // 10
-    )  # 10% of hourly limit per symbol
+    symbol_hourly_limit = max(1, MAX_TRADES_PER_HOUR // 10)
     if symbol_trades_hour >= symbol_hourly_limit:
-        logger.info(
-            "FREQUENCY_LIMIT_SYMBOL_HOURLY",
-            extra={
-                "symbol": symbol,
-                "symbol_trades_hour": symbol_trades_hour,
-                "symbol_hourly_limit": symbol_hourly_limit,
-                "note": "Preventing rapid trading on single symbol",
-            },
-        )
+        logger.info('FREQUENCY_LIMIT_SYMBOL_HOURLY', extra={'symbol': symbol, 'symbol_trades_hour': symbol_trades_hour, 'symbol_hourly_limit': symbol_hourly_limit, 'note': 'Preventing rapid trading on single symbol'})
         return True
-
     return False
 
-
-def _record_trade_in_frequency_tracker(
-    state: BotState, symbol: str, timestamp: datetime
-) -> None:
+def _record_trade_in_frequency_tracker(state: BotState, symbol: str, timestamp: datetime) -> None:
     """
     Record a trade in the frequency tracking system.
 
     AI-AGENT-REF: Part of overtrading prevention system.
     """
-    if not hasattr(state, "trade_history"):
+    if not hasattr(state, 'trade_history'):
         state.trade_history = []
-
     state.trade_history.append((symbol, timestamp))
-
-    # Log frequency stats for monitoring
     hour_ago = timestamp - timedelta(hours=1)
     recent_trades = len([ts for _, ts in state.trade_history if ts > hour_ago])
-
-    logger.debug(
-        "TRADE_FREQUENCY_UPDATED",
-        extra={
-            "symbol": symbol,
-            "trades_last_hour": recent_trades,
-            "total_tracked_trades": len(state.trade_history),
-        },
-    )
-
+    logger.debug('TRADE_FREQUENCY_UPDATED', extra={'symbol': symbol, 'trades_last_hour': recent_trades, 'total_tracked_trades': len(state.trade_history)})
 
 def get_latest_price(symbol: str):
     try:
-        data = alpaca_get(f"/v2/stocks/{symbol}/quotes/latest")
-        price = float(data.get("ap", 0)) if data else None
+        data = alpaca_get(f'/v2/stocks/{symbol}/quotes/latest')
+        price = float(data.get('ap', 0)) if data else None
         if price is None:
-            raise ValueError(f"Price returned None for symbol {symbol}")
+            raise ValueError(f'Price returned None for symbol {symbol}')
         return price
     except Exception as e:
-        logger.error("Failed to get latest price for %s: %s", symbol, e, exc_info=True)
+        logger.error('Failed to get latest price for %s: %s', symbol, e, exc_info=True)
         return None
-
 
 def initialize_bot(api=None, data_loader=None):
     """Return a minimal context and state for unit tests."""
     ctx = types.SimpleNamespace(api=api, data_loader=data_loader)
-    state = {"positions": {}}
-    return ctx, state
-
+    state = {'positions': {}}
+    return (ctx, state)
 
 def generate_signals(df):
     """+1 if price rise, -1 if price fall, else 0."""
-    price = df["price"]  # KeyError if missing
-    diff = price.diff().fillna(0)  # NaN → 0 for the first row
-    signals = diff.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))
-    return signals  # pandas Series → .items()
-
+    price = df['price']
+    diff = price.diff().fillna(0)
+    signals = diff.apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
+    return signals
 
 def execute_trades(ctx, signals: pd.Series) -> list[tuple[str, str]]:
     """Return orders inferred from ``signals`` without hitting real APIs."""
@@ -10173,56 +6289,44 @@     for symbol, sig in signals.items():
         if sig == 0:
             continue
-        side = "buy" if sig > 0 else "sell"
-        api = getattr(ctx, "api", None)
-        if api is not None and hasattr(api, "submit_order"):
+        side = 'buy' if sig > 0 else 'sell'
+        api = getattr(ctx, 'api', None)
+        if api is not None and hasattr(api, 'submit_order'):
             try:
                 api.submit_order(symbol, 1, side)
             except Exception as e:
-                # Order submission failed - log error and add to failed orders
-                logger.error(
-                    "Failed to submit test order for %s %s: %s", symbol, side, e
-                )
+                logger.error('Failed to submit test order for %s %s: %s', symbol, side, e)
         orders.append((symbol, side))
     return orders
-
 
 def run_trading_cycle(ctx, df: pd.DataFrame) -> list[tuple[str, str]]:
     """Generate signals from ``df`` and execute trades via ``ctx``."""
     signals = generate_signals(df)
     return execute_trades(ctx, signals)
 
-
 def health_check(df: pd.DataFrame, resolution: str) -> bool:
     """Delegate to :func:`utils.health_check` for convenience."""
     return utils.health_check(df, resolution)
 
-
 def compute_atr_stop(df, atr_window=14, multiplier=2):
-    # AI-AGENT-REF: helper for ATR-based trailing stop
-    high_low = df["high"] - df["low"]
-    high_close = np.abs(df["high"] - df["close"].shift())
-    low_close = np.abs(df["low"] - df["close"].shift())
+    high_low = df['high'] - df['low']
+    high_close = np.abs(df['high'] - df['close'].shift())
+    low_close = np.abs(df['low'] - df['close'].shift())
     tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
     atr = tr.rolling(atr_window).mean()
-    stop_level = df["close"] - (atr * multiplier)
+    stop_level = df['close'] - atr * multiplier
     return stop_level
-
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     try:
         main()
     except Exception as exc:
-        logger.exception("Fatal error in main: %s", exc)
+        logger.exception('Fatal error in main: %s', exc)
         raise
-
     import time
-
     import schedule
-
     while True:
         try:
             schedule.run_pending()
         except Exception as exc:
-            logger.exception("Scheduler loop error: %s", exc)
-        time.sleep(config.SCHEDULER_SLEEP_SECONDS)
+            logger.exception('Scheduler loop error: %s', exc)
+        time.sleep(S.scheduler_sleep_seconds)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/utils/base.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/ai_trading/utils/base.py@@ -1,3 +1,4 @@+S = get_settings()
 """Utility functions for common operations across the bot."""
 
 import datetime as dt
@@ -258,7 +259,7 @@     now = time.time()
     state = "OPEN" if "OPEN" in message else "CLOSED"
     if state != _LAST_MARKET_STATE or now - _LAST_MARKET_HOURS_LOG >= 3600:
-        if config.VERBOSE_LOGGING:
+        if S.verbose_logging:
             logger.info(message)
         else:
             logger.debug(message)
@@ -276,7 +277,7 @@         or passed != _LAST_HEALTH_STATUS
         or now - _LAST_HEALTH_ROW_LOG >= 10
     ):
-        level = logger.info if config.VERBOSE_LOGGING or not passed else logger.debug
+        level = logger.info if S.verbose_logging or not passed else logger.debug
         status = "PASSED" if passed else "FAILED"
         level("HEALTH_ROWS_%s: received %d rows", status, rows)
         _LAST_HEALTH_ROW_LOG = now
@@ -571,7 +572,7 @@         logger.error("load_model failed", exc_info=e)
         return 0.5
 
-    model_path = config.MODEL_PATH
+    model_path = S.model_path
     model = load_model(model_path)
     if model is None:
         return 0.5
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/audit.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/audit.py@@ -1,7 +1,9 @@+from ai_trading.config import get_settings
 import csv
 import logging
 import os
 import uuid
+S = get_settings()
 
 from validate_env import settings
 
@@ -9,7 +11,7 @@ 
 import config
 
-TRADE_LOG_FILE = config.TRADE_LOG_FILE
+TRADE_LOG_FILE = S.trade_log_file
 
 logger = logging.getLogger(__name__)
 _disable_trade_log = False
@@ -223,9 +225,9 @@ def log_json_audit(details: dict) -> None:
     """Write detailed trade audit record to JSON file."""
     # AI-AGENT-REF: compliance style audit logging
-    os.makedirs(config.TRADE_AUDIT_DIR, exist_ok=True)
+    os.makedirs(S.trade_audit_dir, exist_ok=True)
     order_id = details.get("client_order_id") or str(uuid.uuid4())
-    fname = os.path.join(config.TRADE_AUDIT_DIR, f"{order_id}.json")
+    fname = os.path.join(S.trade_audit_dir, f"{order_id}.json")
     try:
         with open(fname, "w", encoding="utf-8") as f:
             json.dump(details, f, indent=2, default=str)
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/backtest_framework.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/backtest_framework.py@@ -3,14 +3,13 @@ """
 import gc
 import weakref
-
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 import pytest
 
+class TradingBotValidator:
 
-class TradingBotValidator:
     def __init__(self, bot_engine, risk_engine):
         self.bot_engine = bot_engine
         self.risk_engine = risk_engine
@@ -20,78 +19,57 @@     def test_no_lookahead_bias(self, price_series):
         for i in range(50, len(price_series), 10):
             available_data = price_series[:i]
-            signal = self.bot_engine.generate_signal("TEST", available_data)
-
-            future_data = price_series[: i + 10]
-            future_signal = self.bot_engine.generate_signal("TEST", future_data)
-
-            # Validate signal doesn't use future data
+            signal = self.bot_engine.generate_signal('TEST', available_data)
+            future_data = price_series[:i + 10]
+            future_signal = self.bot_engine.generate_signal('TEST', future_data)
             if hasattr(signal, 'timestamp'):
                 latest_data_time = available_data.index[-1] if hasattr(available_data, 'index') else None
                 if latest_data_time and signal.timestamp > latest_data_time:
-                    raise ValueError(f"Signal timestamp {signal.timestamp} is after latest data {latest_data_time}")
-
+                    raise ValueError(f'Signal timestamp {signal.timestamp} is after latest data {latest_data_time}')
             assert signal.confidence == pytest.approx(future_signal.confidence)
 
     def test_positive_expectancy(self, historical_data_path: str):
-        # Use cached data to avoid repeated file I/O
-        cache_key = f"historical_data_{hash(historical_data_path)}"
+        cache_key = f'historical_data_{hash(historical_data_path)}'
         if cache_key in self._test_data_cache:
             data = self._test_data_cache[cache_key]
         else:
             data = pd.read_csv(historical_data_path)
             self._test_data_cache[cache_key] = data
-
         total_pnl = 0
         win_rate = 0
         trade_count = 0
-
-        for symbol in data["symbol"].unique():
-            symbol_data = data[data["symbol"] == symbol]
+        for symbol in data['symbol'].unique():
+            symbol_data = data[data['symbol'] == symbol]
             pnl, wins, trades = self._backtest_symbol(symbol_data)
             total_pnl += pnl
             win_rate += wins
             trade_count += trades
-
-        assert total_pnl > 0, f"Negative total PnL: {total_pnl}"
+        assert total_pnl > 0, f'Negative total PnL: {total_pnl}'
         assert win_rate / max(1, trade_count) > 0.4
 
     def test_volatility_calculation(self):
         np.random.seed(42)
         returns = np.random.normal(0.001, 0.01, 100)
         vol = self.risk_engine.compute_volatility(returns)
-
-        assert 0.009 <= vol["volatility"] <= 0.011
-
+        assert 0.009 <= vol['volatility'] <= 0.011
         returns_with_outliers = returns.copy()
         returns_with_outliers[0] = 0.1
         vol_with_outliers = self.risk_engine.compute_volatility(returns_with_outliers)
-
-        assert vol_with_outliers["mad"] < vol_with_outliers["std_vol"]
+        assert vol_with_outliers['mad'] < vol_with_outliers['std_vol']
 
     def test_position_sizing(self):
-        class MockSignal:
-            def __init__(self):
-                self.symbol = "AAPL"
-                self.confidence = 0.8
-                self.side = "buy"
-
         signal = MockSignal()
-
         qty1 = self.risk_engine.position_size(signal, cash=10000, price=150, api=None)
         assert qty1 > 0
-
         qty2 = self.risk_engine.position_size(signal, cash=500, price=150, api=None)
         if qty2 > 0:
             assert qty2 >= self.risk_engine.config.position_size_min_usd / 150
-
         qty3 = self.risk_engine.position_size(signal, cash=10000, price=0, api=None)
         assert qty3 == 0
 
     def _backtest_symbol(self, data) -> tuple:
-        # Simulate basic backtesting with proper cleanup
         try:
-            return 100, 7, 10
+            return (100, 7, 10)
         finally:
             if hasattr(data, 'memory_usage'):
                 data = None
@@ -110,7 +88,4 @@             self.cleanup()
         except Exception:
             pass
-
-
-__all__ = ["TradingBotValidator"]
-
+__all__ = ['TradingBotValidator']--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/critical_fixes_validation.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/critical_fixes_validation.py@@ -1,208 +1,110 @@-#!/usr/bin/env python3
 import logging
-
-"""
-Final validation test for the critical trading bot fixes.
-This validates that the P0 and P1 critical issues have been resolved.
-"""
-
+'\nFinal validation test for the critical trading bot fixes.\nThis validates that the P0 and P1 critical issues have been resolved.\n'
 import os
 import sys
 import unittest
-
-# Set testing environment
 os.environ['TESTING'] = '1'
-
-# Add project root to path
 sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
-
 
 class TestCriticalFixesValidation(unittest.TestCase):
     """Final validation for critical fixes addressing production issues."""
 
     def setUp(self):
         """Set up test environment."""
-        # Import modules after setting TESTING flag
         self.bot_engine = bot_engine
 
     def test_p0_quantity_calculation_fix(self):
         """Test P0 CRITICAL: Quantity calculation bug fix."""
-        logging.info("\n🔧 Testing P0 Fix: Quantity Calculation Bug")
-
+        logging.info('\n🔧 Testing P0 Fix: Quantity Calculation Bug')
         from trade_execution import ExecutionEngine
-
-        class MockOrder:
-            def __init__(self, filled_qty):
-                self.filled_qty = filled_qty
-                self.id = "test_order_123"
-
-        class MockContext:
-            def __init__(self):
-                self.api = None
-
-        # Test the fixed _reconcile_partial_fills method
         ctx = MockContext()
         engine = ExecutionEngine(ctx)
-
-        # Mock order with actual filled quantity different from calculation
-        mock_order = MockOrder(filled_qty=2)  # Actual filled from order
-
-        # The old bug would use requested_qty - remaining_qty = 10 - 5 = 5 (incorrect)
-        # The new fix should use actual filled_qty = 2 (correct)
-
-        # We can't easily test the logging without capturing it, but we can verify
-        # the method exists and doesn't crash
+        mock_order = MockOrder(filled_qty=2)
         try:
-            # This should not raise an exception
-            engine._reconcile_partial_fills("NFLX", 10, 5, "buy", mock_order)
-            logging.info("  ✓ Quantity calculation uses actual order filled_qty")
-            logging.info("  ✓ Fixed discrepancy between calculated vs actual quantities")
+            engine._reconcile_partial_fills('NFLX', 10, 5, 'buy', mock_order)
+            logging.info('  ✓ Quantity calculation uses actual order filled_qty')
+            logging.info('  ✓ Fixed discrepancy between calculated vs actual quantities')
         except Exception as e:
-            self.fail(f"Quantity fix failed: {e}")
+            self.fail(f'Quantity fix failed: {e}')
 
     def test_p0_sentiment_circuit_breaker_fix(self):
         """Test P0 CRITICAL: Sentiment circuit breaker fix."""
-        logging.info("\n🔧 Testing P0 Fix: Sentiment Circuit Breaker")
-
-        # Verify increased thresholds
-        self.assertEqual(self.sentiment.SENTIMENT_FAILURE_THRESHOLD, 25,
-                        "Failure threshold should be increased from 15 to 25")
-        self.assertEqual(self.sentiment.SENTIMENT_RECOVERY_TIMEOUT, 3600,
-                        "Recovery timeout should be increased from 1800s to 3600s")
-
-        logging.info(f"  ✓ Failure threshold increased to {self.sentiment.SENTIMENT_FAILURE_THRESHOLD}")
-        logging.info(f"  ✓ Recovery timeout increased to {self.sentiment.SENTIMENT_RECOVERY_TIMEOUT}s (1 hour)")
-        logging.info("  ✓ Circuit breaker now more tolerant of API rate limiting")
+        logging.info('\n🔧 Testing P0 Fix: Sentiment Circuit Breaker')
+        self.assertEqual(self.sentiment.SENTIMENT_FAILURE_THRESHOLD, 25, 'Failure threshold should be increased from 15 to 25')
+        self.assertEqual(self.sentiment.SENTIMENT_RECOVERY_TIMEOUT, 3600, 'Recovery timeout should be increased from 1800s to 3600s')
+        logging.info(f'  ✓ Failure threshold increased to {self.sentiment.SENTIMENT_FAILURE_THRESHOLD}')
+        logging.info(f'  ✓ Recovery timeout increased to {self.sentiment.SENTIMENT_RECOVERY_TIMEOUT}s (1 hour)')
+        logging.info('  ✓ Circuit breaker now more tolerant of API rate limiting')
 
     def test_p1_confidence_normalization_fix(self):
         """Test P1 HIGH: Signal confidence normalization fix."""
-        logging.info("\n🔧 Testing P1 Fix: Signal Confidence Normalization")
-
+        logging.info('\n🔧 Testing P1 Fix: Signal Confidence Normalization')
         allocator = self.strategy_allocator.StrategyAllocator()
-
-        # Create signals with out-of-range confidence from production logs
-        class MockSignal:
-            def __init__(self, symbol, side, confidence):
-                self.symbol = symbol
-                self.side = side
-                self.confidence = confidence
-
-        signals_by_strategy = {
-            "test_strategy": [
-                MockSignal("NFLX", "buy", 2.7904717584079948),  # From production logs
-                MockSignal("META", "sell", 1.7138986011550261),  # From production logs
-            ]
-        }
-
-        # Process signals and verify confidence normalization
+        signals_by_strategy = {'test_strategy': [MockSignal('NFLX', 'buy', 2.7904717584079948), MockSignal('META', 'sell', 1.7138986011550261)]}
         result = allocator.allocate(signals_by_strategy)
-
-        # Verify all returned signals have confidence in [0,1] range
         for signal in result:
-            self.assertTrue(0 <= signal.confidence <= 1,
-                          f"Signal {signal.symbol} confidence {signal.confidence} not in [0,1]")
-
-        logging.info("  ✓ Out-of-range confidence values normalized to [0,1]")
-        logging.info("  ✓ Production log confidence issues resolved")
-        logging.info("  ✓ Algorithm integrity monitoring added")
+            self.assertTrue(0 <= signal.confidence <= 1, f'Signal {signal.symbol} confidence {signal.confidence} not in [0,1]')
+        logging.info('  ✓ Out-of-range confidence values normalized to [0,1]')
+        logging.info('  ✓ Production log confidence issues resolved')
+        logging.info('  ✓ Algorithm integrity monitoring added')
 
     def test_p2_sector_classification_fix(self):
         """Test P2 MEDIUM: Sector classification fallback."""
-        logging.info("\n🔧 Testing P2 Fix: Sector Classification Fallback")
-
-        # Test specific symbols mentioned in production logs
-        baba_sector = self.bot_engine.get_sector("BABA")
-        self.assertNotEqual(baba_sector, "Unknown", "BABA should have fallback sector")
-        self.assertEqual(baba_sector, "Technology", "BABA should be Technology")
-
-        # Test other symbols for robustness
-        aapl_sector = self.bot_engine.get_sector("AAPL")
-        self.assertEqual(aapl_sector, "Technology", "AAPL should be Technology")
-
-        logging.info(f"  ✓ BABA classified as {baba_sector} (was Unknown)")
+        logging.info('\n🔧 Testing P2 Fix: Sector Classification Fallback')
+        baba_sector = self.bot_engine.get_sector('BABA')
+        self.assertNotEqual(baba_sector, 'Unknown', 'BABA should have fallback sector')
+        self.assertEqual(baba_sector, 'Technology', 'BABA should be Technology')
+        aapl_sector = self.bot_engine.get_sector('AAPL')
+        self.assertEqual(aapl_sector, 'Technology', 'AAPL should be Technology')
+        logging.info(f'  ✓ BABA classified as {baba_sector} (was Unknown)')
         logging.info("  ✓ Fallback sector mappings prevent 'Unknown' classification")
-        logging.info("  ✓ Risk allocation now works correctly for common securities")
+        logging.info('  ✓ Risk allocation now works correctly for common securities')
 
     def test_p2_short_selling_validation_foundation(self):
         """Test P2 MEDIUM: Short selling validation foundation."""
-        logging.info("\n🔧 Testing P2 Fix: Short Selling Validation (Foundation)")
-
+        logging.info('\n🔧 Testing P2 Fix: Short Selling Validation (Foundation)')
         from trade_execution import ExecutionEngine
-
-        class MockContext:
-            def __init__(self):
-                self.api = None
-                self.allow_short_selling = True  # Enable short selling
-
         ctx = MockContext()
         engine = ExecutionEngine(ctx)
-
-        # Verify the validation method exists
-        self.assertTrue(hasattr(engine, '_validate_short_selling'),
-                       "Short selling validation method should exist")
-
-        logging.info("  ✓ Short selling validation method implemented")
-        logging.info("  ✓ Foundation for margin requirement checks added")
-        logging.info("  ✓ Broker permissions validation framework ready")
+        self.assertTrue(hasattr(engine, '_validate_short_selling'), 'Short selling validation method should exist')
+        logging.info('  ✓ Short selling validation method implemented')
+        logging.info('  ✓ Foundation for margin requirement checks added')
+        logging.info('  ✓ Broker permissions validation framework ready')
 
     def test_production_log_issues_addressed(self):
         """Test that specific production log issues are addressed."""
-        logging.info("\n🔧 Validating Production Log Issues Fixed")
-
-        # Issue: "Calculated qty=2, submitted 1 share, but logs claim filled_qty=2"
-        # Fix: Now uses actual order.filled_qty instead of calculation
-        logging.info("  ✓ NFLX quantity logging discrepancy: FIXED")
-
-        # Issue: "Sentiment circuit breaker opened after 15 failures"
-        # Fix: Threshold increased to 25, timeout increased to 1 hour
-        logging.info("  ✓ Sentiment circuit breaker stuck open: FIXED")
-
-        # Issue: "Signal confidence out of range [0,1]: 2.7904717584079948"
-        # Fix: Added proper normalization and clamping
-        logging.info("  ✓ Signal confidence out of range: FIXED")
-
-        # Issue: "Could not determine sector for BABA, using Unknown"
-        # Fix: Added BABA to fallback sector mappings
-        logging.info("  ✓ BABA sector classification: FIXED")
-
-        # Issue: "SIGNAL_SHORT | symbol=GOOGL qty=5" followed by "SKIP_NO_POSITION"
-        # Fix: Added short selling validation framework
-        logging.info("  ✓ Short selling validation framework: IMPLEMENTED")
-
-
+        logging.info('\n🔧 Validating Production Log Issues Fixed')
+        logging.info('  ✓ NFLX quantity logging discrepancy: FIXED')
+        logging.info('  ✓ Sentiment circuit breaker stuck open: FIXED')
+        logging.info('  ✓ Signal confidence out of range: FIXED')
+        logging.info('  ✓ BABA sector classification: FIXED')
+        logging.info('  ✓ Short selling validation framework: IMPLEMENTED')
 if __name__ == '__main__':
-    logging.info("🚀 CRITICAL TRADING BOT FIXES - FINAL VALIDATION")
-    logging.info(str("=" * 70))
-    logging.info("Validating fixes for P&L loss prevention and decision quality...")
-
-    # Create test suite
+    logging.info('🚀 CRITICAL TRADING BOT FIXES - FINAL VALIDATION')
+    logging.info(str('=' * 70))
+    logging.info('Validating fixes for P&L loss prevention and decision quality...')
     suite = unittest.TestSuite()
     test_class = TestCriticalFixesValidation
-
-    # Add validation tests for each critical issue
     suite.addTest(test_class('test_p0_quantity_calculation_fix'))
     suite.addTest(test_class('test_p0_sentiment_circuit_breaker_fix'))
     suite.addTest(test_class('test_p1_confidence_normalization_fix'))
     suite.addTest(test_class('test_p2_sector_classification_fix'))
     suite.addTest(test_class('test_p2_short_selling_validation_foundation'))
     suite.addTest(test_class('test_production_log_issues_addressed'))
-
-    # Run tests
     runner = unittest.TextTestRunner(verbosity=1)
     result = runner.run(suite)
-
-    logging.info(str("\n" + "=" * 70))
+    logging.info(str('\n' + '=' * 70))
     if result.wasSuccessful():
-        logging.info("✅ ALL CRITICAL FIXES VALIDATED SUCCESSFULLY!")
-        logging.info("\n📊 SUMMARY:")
-        logging.info("  • P0 Quantity calculation bug: FIXED")
-        logging.info("  • P0 Sentiment circuit breaker: FIXED")
-        logging.info("  • P1 Confidence normalization: FIXED")
-        logging.info("  • P2 Sector classification: FIXED")
-        logging.info("  • P2 Short selling foundation: IMPLEMENTED")
-        logging.info("\n🛡️  Financial risk reduced, decision quality improved!")
+        logging.info('✅ ALL CRITICAL FIXES VALIDATED SUCCESSFULLY!')
+        logging.info('\n📊 SUMMARY:')
+        logging.info('  • P0 Quantity calculation bug: FIXED')
+        logging.info('  • P0 Sentiment circuit breaker: FIXED')
+        logging.info('  • P1 Confidence normalization: FIXED')
+        logging.info('  • P2 Sector classification: FIXED')
+        logging.info('  • P2 Short selling foundation: IMPLEMENTED')
+        logging.info('\n🛡️  Financial risk reduced, decision quality improved!')
         sys.exit(0)
     else:
-        logging.info("❌ SOME CRITICAL FIXES FAILED VALIDATION!")
-        logging.info("🚨 Manual review required before production deployment.")
-        sys.exit(1)
+        logging.info('❌ SOME CRITICAL FIXES FAILED VALIDATION!')
+        logging.info('🚨 Manual review required before production deployment.')
+        sys.exit(1)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/demo_drawdown_protection.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/demo_drawdown_protection.py@@ -1,3 +1,5 @@+from ai_trading.config import get_settings
+S = get_settings()
 #!/usr/bin/env python3
 import logging
 
@@ -80,8 +82,8 @@         logging.info(f"{time}: ${equity:>8,.0f} {change:<15} | {trading_status:<12} | Drawdown: {drawdown_pct:>4.1f}% | {description}")
         
         # Additional logging for important events
-        if not trading_allowed and status["current_drawdown"] > config.MAX_DRAWDOWN_THRESHOLD:
-            logging.info(str(f"      💥 CIRCUIT BREAKER TRIGGERED: {status['current_drawdown']:.1%} > {config.MAX_DRAWDOWN_THRESHOLD:.1%}"))
+        if not trading_allowed and status["current_drawdown"] > S.max_drawdown_threshold:
+            logging.info(str(f"      💥 CIRCUIT BREAKER TRIGGERED: {status['current_drawdown']:.1%} > {S.max_drawdown_threshold:.1%}"))
         elif trading_allowed and status["current_drawdown"] > 0:
             recovery_ratio = equity / status["peak_equity"] if status["peak_equity"] > 0 else 0
             if recovery_ratio >= breaker.recovery_threshold:
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/predict.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/predict.py@@ -1,3 +1,5 @@+from ai_trading.config import get_settings
+S = get_settings()
 """Prediction utilities using trained models."""
 
 from __future__ import annotations
@@ -48,7 +50,7 @@     """Return a sentiment score for ``symbol`` using NewsAPI with rate limiting and TTL cache."""
 
     # Support both SENTIMENT_API_KEY and NEWS_API_KEY for backwards compatibility
-    api_key = getattr(config, 'SENTIMENT_API_KEY', None) or config.NEWS_API_KEY
+    api_key = getattr(config, 'SENTIMENT_API_KEY', None) or S.news_api_key
     if not api_key:
         logger.debug("No sentiment API key configured (checked SENTIMENT_API_KEY and NEWS_API_KEY)")
         return 0.0
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/retrain.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/retrain.py@@ -1,8 +1,10 @@+from ai_trading.config import get_settings
 import csv
 import json
 import logging
 import os
 import random
+S = get_settings()
 
 # pandas_ta SyntaxWarning now filtered globally in pytest.ini
 
@@ -25,7 +27,7 @@ config.reload_env()
 
 # Set deterministic random seeds for reproducibility
-SEED = config.SEED
+SEED = S.seed
 random.seed(SEED)
 np.random.seed(SEED)
 try:
@@ -55,7 +57,7 @@ 
 import config
 
-NEWS_API_KEY = config.NEWS_API_KEY
+NEWS_API_KEY = S.news_api_key
 
 if not NEWS_API_KEY:
     logger.warning("NEWS_API_KEY is not set; sentiment features will be zero")
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/risk_engine.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/risk_engine.py@@ -1,9 +1,11 @@+from ai_trading.config import get_settings
 import logging
 import os
 import random
 from collections.abc import Sequence
 from datetime import UTC, datetime
 from typing import Any
+S = get_settings()
 
 # AI-AGENT-REF: guard numpy import for test environments
 import numpy as np
@@ -21,8 +23,8 @@ logger = get_phase_logger(__name__, "RISK_CHECK")
 
 # Set deterministic seed from configuration
-random.seed(config.SEED)
-np.random.seed(config.SEED)
+random.seed(S.seed)
+np.random.seed(S.seed)
 # AI-AGENT-REF: compatibility with pandas_ta expecting numpy.NaN constant
 if not hasattr(np, "NaN"):
     np.NaN = np.nan
@@ -42,7 +44,7 @@     def __init__(self, cfg: config.TradingConfig | None = None) -> None:
         """Initialize the engine with an optional trading config."""
         # AI-AGENT-REF: fix param shadowing bug when ``config`` is None
-        self.config = cfg if cfg is not None else config.CONFIG
+        self.config = cfg if cfg is not None else S.config
 
         # AI-AGENT-REF: Add comprehensive validation for risk parameters
         try:
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/validate_critical_fix.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/validate_critical_fix.py@@ -1,149 +1,70 @@-#!/usr/bin/env python3
 import logging
-
-"""
-Critical Fix Validation - Type Conversion in Order Fill Reconciliation
-
-This script validates that the critical TypeError fix in _reconcile_partial_fills
-properly handles string values from Alpaca API's order.filled_qty attribute.
-
-Production Issue: TypeError: '>' not supported between instances of 'str' and 'int'
-Fix: Safe string-to-numeric conversion before comparison
-
-BEFORE FIX: Every successful trade crashed during reconciliation
-AFTER FIX: All trades complete successfully with proper reconciliation
-"""
-
+"\nCritical Fix Validation - Type Conversion in Order Fill Reconciliation\n\nThis script validates that the critical TypeError fix in _reconcile_partial_fills\nproperly handles string values from Alpaca API's order.filled_qty attribute.\n\nProduction Issue: TypeError: '>' not supported between instances of 'str' and 'int'\nFix: Safe string-to-numeric conversion before comparison\n\nBEFORE FIX: Every successful trade crashed during reconciliation\nAFTER FIX: All trades complete successfully with proper reconciliation\n"
 import os
 import sys
-
-# Set up test environment
-os.environ.update({
-    'ALPACA_API_KEY': 'test_key',
-    'ALPACA_SECRET_KEY': 'test_secret',
-    'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets',
-    'WEBHOOK_SECRET': 'test_webhook',
-    'FLASK_PORT': '5000'
-})
-
+os.environ.update({'ALPACA_API_KEY': 'test_key', 'ALPACA_SECRET_KEY': 'test_secret', 'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets', 'WEBHOOK_SECRET': 'test_webhook', 'FLASK_PORT': '5000'})
 sys.path.append('.')
-
 from unittest.mock import MagicMock
-
 from ai_trading.trade_execution import ExecutionEngine
-
-
-class MockOrder:
-    """Simulates Alpaca order response with various filled_qty data types."""
-    def __init__(self, filled_qty=None, status="filled", order_id="test-order"):
-        self.filled_qty = filled_qty  # String from API (the bug cause)
-        self.status = status
-        self.id = order_id
-        self.symbol = "TEST"
-
-class MockContext:
-    """Mock trading context."""
-    def __init__(self):
-        self.api = MagicMock()
-        self.data_client = MagicMock()
-        self.data_fetcher = MagicMock()
-        self.capital_band = "small"
 
 def test_production_scenarios():
     """Test the exact scenarios from production logs."""
-    logging.info("🔄 VALIDATING CRITICAL PRODUCTION FIX")
-    logging.info(str("=" * 60))
-
+    logging.info('🔄 VALIDATING CRITICAL PRODUCTION FIX')
+    logging.info(str('=' * 60))
     ctx = MockContext()
     engine = ExecutionEngine(ctx)
-
-    # Production scenarios from the logs
-    scenarios = [
-        ("NFLX", "1", 1, "Filled 1 share @ $1,177.38"),
-        ("TSLA", "16", 16, "Filled 16 shares @ $319.34"),
-        ("MSFT", "5", 5, "Filled 5 shares @ $526.37"),
-        ("SPY", "4", 4, "Filled 4 shares @ $632.98"),
-        ("QQQ", "10", 10, "Order example"),
-        ("PLTR", "7", 7, "Order example"),
-    ]
-
-    logging.info("Testing production crash scenarios:")
-    logging.info("Before fix: ❌ TypeError on every trade")
-    logging.info("After fix:  ✅ All trades complete successfully")
+    scenarios = [('NFLX', '1', 1, 'Filled 1 share @ $1,177.38'), ('TSLA', '16', 16, 'Filled 16 shares @ $319.34'), ('MSFT', '5', 5, 'Filled 5 shares @ $526.37'), ('SPY', '4', 4, 'Filled 4 shares @ $632.98'), ('QQQ', '10', 10, 'Order example'), ('PLTR', '7', 7, 'Order example')]
+    logging.info('Testing production crash scenarios:')
+    logging.info('Before fix: ❌ TypeError on every trade')
+    logging.info('After fix:  ✅ All trades complete successfully')
     print()
-
     all_passed = True
-
     for symbol, filled_qty_str, expected_qty, description in scenarios:
-        logging.info(f"🔍 {symbol}: {description}")
-
-        # Create order with STRING filled_qty (the production issue)
+        logging.info(f'🔍 {symbol}: {description}')
         order = MockOrder(filled_qty=filled_qty_str)
-
         try:
-            # This would have crashed EVERY TIME before the fix
-            engine._reconcile_partial_fills(
-                symbol=symbol,
-                requested_qty=expected_qty + 10,  # Request more than filled
-                remaining_qty=10,  # Some remaining
-                side="buy",
-                last_order=order
-            )
+            engine._reconcile_partial_fills(symbol=symbol, requested_qty=expected_qty + 10, remaining_qty=10, side='buy', last_order=order)
             logging.info(str(f"   ✅ SUCCESS: String '{filled_qty_str}' → int {expected_qty}"))
-
         except TypeError as e:
             if "'>' not supported between instances of 'str' and 'int'" in str(e):
-                logging.info(f"   ❌ FAILED: TypeError still occurs - {e}")
+                logging.info(f'   ❌ FAILED: TypeError still occurs - {e}')
                 all_passed = False
             else:
                 raise
         except Exception as e:
-            logging.info(f"   ⚠️  Other exception (acceptable): {type(e).__name__}")
-
+            logging.info(f'   ⚠️  Other exception (acceptable): {type(e).__name__}')
     print()
-    logging.info("🧪 TESTING EDGE CASES:")
-
-    edge_cases = [
-        ("", "empty string"),
-        ("0", "zero string"),
-        ("abc", "invalid string"),
-        (None, "None value"),
-        ("25.5", "decimal string"),
-        ("-5", "negative string"),
-    ]
-
+    logging.info('🧪 TESTING EDGE CASES:')
+    edge_cases = [('', 'empty string'), ('0', 'zero string'), ('abc', 'invalid string'), (None, 'None value'), ('25.5', 'decimal string'), ('-5', 'negative string')]
     for value, description in edge_cases:
         order = MockOrder(filled_qty=value)
         try:
-            engine._reconcile_partial_fills("TEST", 100, 50, "buy", order)
-            logging.info(f"   ✅ {description}: handled gracefully")
+            engine._reconcile_partial_fills('TEST', 100, 50, 'buy', order)
+            logging.info(f'   ✅ {description}: handled gracefully')
         except TypeError as e:
             if "'>' not supported between instances of 'str' and 'int'" in str(e):
-                logging.info(f"   ❌ {description}: TypeError still occurs")
+                logging.info(f'   ❌ {description}: TypeError still occurs')
                 all_passed = False
             else:
                 raise
         except Exception:
-            logging.info(f"   ✅ {description}: handled gracefully")
-
+            logging.info(f'   ✅ {description}: handled gracefully')
     print()
-    logging.info(str("=" * 60))
-
+    logging.info(str('=' * 60))
     if all_passed:
-        logging.info("🎉 ALL TESTS PASSED - CRITICAL FIX VALIDATED")
+        logging.info('🎉 ALL TESTS PASSED - CRITICAL FIX VALIDATED')
         print()
-        logging.info("✅ Production Issue RESOLVED:")
-        logging.info("   - No more TypeError crashes during trade reconciliation")
-        logging.info("   - String filled_qty values are safely converted to integers")
-        logging.info("   - Invalid values fall back to calculated quantities")
-        logging.info("   - Proper logging for type conversion failures")
+        logging.info('✅ Production Issue RESOLVED:')
+        logging.info('   - No more TypeError crashes during trade reconciliation')
+        logging.info('   - String filled_qty values are safely converted to integers')
+        logging.info('   - Invalid values fall back to calculated quantities')
+        logging.info('   - Proper logging for type conversion failures')
         print()
-        logging.info("🚀 READY FOR PRODUCTION DEPLOYMENT")
+        logging.info('🚀 READY FOR PRODUCTION DEPLOYMENT')
         return True
     else:
-        logging.info("❌ SOME TESTS FAILED - FIX NEEDS REVIEW")
+        logging.info('❌ SOME TESTS FAILED - FIX NEEDS REVIEW')
         return False
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     success = test_production_scenarios()
-    sys.exit(0 if success else 1)
+    sys.exit(0 if success else 1)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/validate_env.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/validate_env.py@@ -1,41 +1,37 @@ from ai_trading.utils import paths
-
-"""Environment validation using pydantic-settings with enhanced security checks."""
-
+'Environment validation using pydantic-settings with enhanced security checks.'
 import logging
 import re
 import os
 from typing import Optional, List, Dict, Any
 from datetime import datetime
-
-# AI-AGENT-REF: graceful fallback for missing pydantic-settings in testing
 try:
     from pydantic_settings import BaseSettings, SettingsConfigDict
     from pydantic import field_validator, Field
     PYDANTIC_AVAILABLE = True
 except ImportError:
-    # Create minimal fallback classes for testing mode
+
     class BaseSettings:
+
         def __init__(self, **kwargs):
             for k, v in kwargs.items():
                 setattr(self, k, v)
-    
+
     class SettingsConfigDict:
+
         def __init__(self, **kwargs):
             self.dict = kwargs
-    
+
     def field_validator(*args, **kwargs):
+
         def decorator(func):
             return func
         return decorator
-    
+
     def Field(*args, **kwargs):
         return None
-    
     PYDANTIC_AVAILABLE = False
-
 logger = logging.getLogger(__name__)
-
 
 class Settings(BaseSettings):
     """
@@ -45,254 +41,162 @@     built-in validation, type checking, and security measures. It ensures that
     critical settings are properly configured before the bot starts trading.
     """
-    
-    # Server Configuration
-    FLASK_PORT: int = Field(default=9000, ge=1024, le=65535, description="Flask web server port")
-    HEALTHCHECK_PORT: int = Field(default=8081, ge=1024, le=65535, description="Health check service port")
-    RUN_HEALTHCHECK: str = Field(default="0", description="Enable health check endpoint")
-    WEBHOOK_PORT: int = Field(default=9000, ge=1024, le=65535, description="Webhook listener port")
-    
-    # Critical API Keys (Required)
-    ALPACA_API_KEY: str = Field(..., min_length=1, description="Alpaca API key for trading")
-    ALPACA_SECRET_KEY: str = Field(..., min_length=1, description="Alpaca secret key for trading")
-    
-    # AI-AGENT-REF: Support for live vs paper trading configuration
-    TRADING_MODE: str = Field(default="paper", description="Trading mode (paper/live)")
-    ALPACA_BASE_URL: str = Field(default="https://paper-api.alpaca.markets", description="Alpaca API base URL")
-    ALPACA_DATA_FEED: str = Field(default="iex", description="Alpaca data feed source")
-    
-    # Optional API Keys
-    FINNHUB_API_KEY: Optional[str] = Field(default=None, description="Finnhub API key for market data")
-    FUNDAMENTAL_API_KEY: Optional[str] = Field(default=None, description="Fundamental data API key")
-    NEWS_API_KEY: Optional[str] = Field(default=None, description="News API key")
-    IEX_API_TOKEN: Optional[str] = Field(default=None, description="IEX Cloud API token")
-    
-    # Trading Configuration
-    BOT_MODE: str = Field(default="balanced", description="Trading mode (conservative/balanced/aggressive)")
-    MODEL_PATH: str = Field(default="trained_model.pkl", description="Path to ML model file")
-    HALT_FLAG_PATH: str = Field(default="halt.flag", description="Emergency halt flag file")
-    MAX_PORTFOLIO_POSITIONS: int = Field(default=20, ge=1, le=100, description="Maximum portfolio positions")
-    MAX_OPEN_POSITIONS: int = Field(default=10, ge=1, le=50, description="Maximum concurrent positions")
-    
-    # Risk Management
-    LIMIT_ORDER_SLIPPAGE: float = Field(default=0.005, ge=0.0, le=0.1, description="Maximum slippage for limit orders")
-    SLIPPAGE_THRESHOLD: float = Field(default=0.003, ge=0.0, le=0.05, description="Slippage warning threshold")
-    DISASTER_DD_LIMIT: float = Field(default=0.2, ge=0.05, le=0.5, description="Emergency drawdown limit")
-    WEEKLY_DRAWDOWN_LIMIT: float = Field(default=0.15, ge=0.01, le=0.5, description="Weekly drawdown limit")
-    DOLLAR_RISK_LIMIT: float = Field(default=0.05, ge=0.001, le=0.1, description="Dollar risk per trade")
-    
-    # Signal and Entry Thresholds
-    BUY_THRESHOLD: float = Field(default=0.5, ge=0.0, le=1.0, description="Signal strength threshold for buys")
-    VOLUME_THRESHOLD: int = Field(default=50000, ge=1000, description="Minimum daily volume requirement")
-    
-    # Security and Compliance
-    WEBHOOK_SECRET: str = Field(default="", description="Webhook authentication secret")
-    
-    # Operation Modes
-    SHADOW_MODE: bool = Field(default=False, description="Log trades without execution")
-    DRY_RUN: bool = Field(default=False, description="Simulate trading without real orders")
-    DISABLE_DAILY_RETRAIN: bool = Field(default=False, description="Skip daily model retraining")
-    FORCE_TRADES: bool = Field(default=False, description="Override safety checks (DANGEROUS)")
-    
-    # File Paths
-    TRADE_LOG_FILE: str = Field(default=paths.data_dir() / "trades.csv", description="Trade history log file")
-    MODEL_RF_PATH: str = Field(default="model_rf.pkl", description="Random Forest model path")
-    MODEL_XGB_PATH: str = Field(default="model_xgb.pkl", description="XGBoost model path")
-    MODEL_LGB_PATH: str = Field(default="model_lgb.pkl", description="LightGBM model path")
-    RL_MODEL_PATH: str = Field(default="rl_agent.zip", description="Reinforcement learning model path")
-    
-    # Advanced Features
-    USE_RL_AGENT: bool = Field(default=False, description="Enable reinforcement learning agent")
-    SECTOR_EXPOSURE_CAP: float = Field(default=0.4, ge=0.1, le=1.0, description="Maximum sector exposure")
-    
-    # Performance and Rate Limiting
-    REBALANCE_INTERVAL_MIN: int = Field(default=1440, ge=1, description="Portfolio rebalance interval (minutes)")
-    FINNHUB_RPM: int = Field(default=60, ge=1, le=300, description="Finnhub requests per minute limit")
-    
-    # Additional fields from main branch
-    MINUTE_CACHE_TTL: int = Field(default=60, description="Cache TTL for minute data")
-    EQUITY_EXPOSURE_CAP: float = Field(default=2.5, description="Maximum equity exposure")
-    PORTFOLIO_EXPOSURE_CAP: float = Field(default=2.5, description="Maximum portfolio exposure")
-    SEED: int = Field(default=42, description="Random seed for reproducibility")
-    RATE_LIMIT_BUDGET: int = Field(default=190, description="API rate limit budget")
-
-    model_config = SettingsConfigDict(
-        env_file=".env",
-        env_file_encoding="utf-8",
-        case_sensitive=True,
-        extra="ignore"  # allow unknown env vars (e.g. SLACK_WEBHOOK)
-    )
-    
+    FLASK_PORT: int = Field(default=9000, ge=1024, le=65535, description='Flask web server port')
+    HEALTHCHECK_PORT: int = Field(default=8081, ge=1024, le=65535, description='Health check service port')
+    RUN_HEALTHCHECK: str = Field(default='0', description='Enable health check endpoint')
+    WEBHOOK_PORT: int = Field(default=9000, ge=1024, le=65535, description='Webhook listener port')
+    ALPACA_API_KEY: str = Field(..., min_length=1, description='Alpaca API key for trading')
+    ALPACA_SECRET_KEY: str = Field(..., min_length=1, description='Alpaca secret key for trading')
+    TRADING_MODE: str = Field(default='paper', description='Trading mode (paper/live)')
+    ALPACA_BASE_URL: str = Field(default='https://paper-api.alpaca.markets', description='Alpaca API base URL')
+    ALPACA_DATA_FEED: str = Field(default='iex', description='Alpaca data feed source')
+    FINNHUB_API_KEY: Optional[str] = Field(default=None, description='Finnhub API key for market data')
+    FUNDAMENTAL_API_KEY: Optional[str] = Field(default=None, description='Fundamental data API key')
+    NEWS_API_KEY: Optional[str] = Field(default=None, description='News API key')
+    IEX_API_TOKEN: Optional[str] = Field(default=None, description='IEX Cloud API token')
+    BOT_MODE: str = Field(default='balanced', description='Trading mode (conservative/balanced/aggressive)')
+    MODEL_PATH: str = Field(default='trained_model.pkl', description='Path to ML model file')
+    HALT_FLAG_PATH: str = Field(default='halt.flag', description='Emergency halt flag file')
+    MAX_PORTFOLIO_POSITIONS: int = Field(default=20, ge=1, le=100, description='Maximum portfolio positions')
+    MAX_OPEN_POSITIONS: int = Field(default=10, ge=1, le=50, description='Maximum concurrent positions')
+    LIMIT_ORDER_SLIPPAGE: float = Field(default=0.005, ge=0.0, le=0.1, description='Maximum slippage for limit orders')
+    SLIPPAGE_THRESHOLD: float = Field(default=0.003, ge=0.0, le=0.05, description='Slippage warning threshold')
+    DISASTER_DD_LIMIT: float = Field(default=0.2, ge=0.05, le=0.5, description='Emergency drawdown limit')
+    WEEKLY_DRAWDOWN_LIMIT: float = Field(default=0.15, ge=0.01, le=0.5, description='Weekly drawdown limit')
+    DOLLAR_RISK_LIMIT: float = Field(default=0.05, ge=0.001, le=0.1, description='Dollar risk per trade')
+    BUY_THRESHOLD: float = Field(default=0.5, ge=0.0, le=1.0, description='Signal strength threshold for buys')
+    VOLUME_THRESHOLD: int = Field(default=50000, ge=1000, description='Minimum daily volume requirement')
+    WEBHOOK_SECRET: str = Field(default='', description='Webhook authentication secret')
+    SHADOW_MODE: bool = Field(default=False, description='Log trades without execution')
+    DRY_RUN: bool = Field(default=False, description='Simulate trading without real orders')
+    DISABLE_DAILY_RETRAIN: bool = Field(default=False, description='Skip daily model retraining')
+    FORCE_TRADES: bool = Field(default=False, description='Override safety checks (DANGEROUS)')
+    TRADE_LOG_FILE: str = Field(default=paths.data_dir() / 'trades.csv', description='Trade history log file')
+    MODEL_RF_PATH: str = Field(default='model_rf.pkl', description='Random Forest model path')
+    MODEL_XGB_PATH: str = Field(default='model_xgb.pkl', description='XGBoost model path')
+    MODEL_LGB_PATH: str = Field(default='model_lgb.pkl', description='LightGBM model path')
+    RL_MODEL_PATH: str = Field(default='rl_agent.zip', description='Reinforcement learning model path')
+    USE_RL_AGENT: bool = Field(default=False, description='Enable reinforcement learning agent')
+    SECTOR_EXPOSURE_CAP: float = Field(default=0.4, ge=0.1, le=1.0, description='Maximum sector exposure')
+    REBALANCE_INTERVAL_MIN: int = Field(default=1440, ge=1, description='Portfolio rebalance interval (minutes)')
+    FINNHUB_RPM: int = Field(default=60, ge=1, le=300, description='Finnhub requests per minute limit')
+    MINUTE_CACHE_TTL: int = Field(default=60, description='Cache TTL for minute data')
+    EQUITY_EXPOSURE_CAP: float = Field(default=2.5, description='Maximum equity exposure')
+    PORTFOLIO_EXPOSURE_CAP: float = Field(default=2.5, description='Maximum portfolio exposure')
+    SEED: int = Field(default=42, description='Random seed for reproducibility')
+    RATE_LIMIT_BUDGET: int = Field(default=190, description='API rate limit budget')
+    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', case_sensitive=True, extra='ignore')
+
     @field_validator('ALPACA_API_KEY')
     @classmethod
     def validate_alpaca_api_key(cls, v):
         """Validate Alpaca API key format and security."""
         if not v:
-            raise ValueError("ALPACA_API_KEY is required")
-        
-        # Basic format validation (Alpaca keys are typically alphanumeric)
-        if not re.match(r'^[A-Z0-9]+$', v):
-            logger.warning("ALPACA_API_KEY format may be invalid")
-        
-        # Check for common placeholder values
+            raise ValueError('ALPACA_API_KEY is required')
+        if not re.match('^[A-Z0-9]+$', v):
+            logger.warning('ALPACA_API_KEY format may be invalid')
         placeholder_values = ['your_api_key', 'placeholder', 'changeme', 'test']
         if v.lower() in placeholder_values:
-            raise ValueError("ALPACA_API_KEY appears to be a placeholder value")
-        
-        # Minimum length check
+            raise ValueError('ALPACA_API_KEY appears to be a placeholder value')
         if len(v) < 10:
-            raise ValueError("ALPACA_API_KEY appears too short to be valid")
-            
+            raise ValueError('ALPACA_API_KEY appears too short to be valid')
         return v
-    
+
     @field_validator('ALPACA_SECRET_KEY')
     @classmethod
     def validate_alpaca_secret_key(cls, v):
         """Validate Alpaca secret key format and security."""
         if not v:
-            raise ValueError("ALPACA_SECRET_KEY is required")
-        
-        # Check for common placeholder values
+            raise ValueError('ALPACA_SECRET_KEY is required')
         placeholder_values = ['your_secret_key', 'placeholder', 'changeme', 'test']
         if v.lower() in placeholder_values:
-            raise ValueError("ALPACA_SECRET_KEY appears to be a placeholder value")
-        
-        # Minimum length check
+            raise ValueError('ALPACA_SECRET_KEY appears to be a placeholder value')
         if len(v) < 20:
-            raise ValueError("ALPACA_SECRET_KEY appears too short to be valid")
-            
+            raise ValueError('ALPACA_SECRET_KEY appears too short to be valid')
         return v
-    
+
     @field_validator('ALPACA_BASE_URL')
     @classmethod
     def validate_alpaca_base_url(cls, v):
         """Validate Alpaca base URL format."""
-        valid_urls = [
-            'https://api.alpaca.markets',      # Live trading
-            'https://paper-api.alpaca.markets'  # Paper trading
-        ]
-        
+        valid_urls = ['https://api.alpaca.markets', 'https://paper-api.alpaca.markets']
         if v not in valid_urls:
             logger.warning(f"ALPACA_BASE_URL '{v}' is not a standard Alpaca URL")
-        
         if not v.startswith('https://'):
-            raise ValueError("ALPACA_BASE_URL must use HTTPS")
-            
+            raise ValueError('ALPACA_BASE_URL must use HTTPS')
         return v
-    
+
     @field_validator('BOT_MODE')
     @classmethod
     def validate_bot_mode(cls, v):
         """Validate trading bot mode."""
         valid_modes = ['conservative', 'balanced', 'aggressive', 'paper', 'testing']
         if v.lower() not in valid_modes:
-            raise ValueError(f"BOT_MODE must be one of: {valid_modes}")
+            raise ValueError(f'BOT_MODE must be one of: {valid_modes}')
         return v.lower()
-    
+
     @field_validator('TRADING_MODE')
     @classmethod
     def validate_trading_mode(cls, v):
         """Validate trading mode."""
-        if v not in ["paper", "live"]:
+        if v not in ['paper', 'live']:
             raise ValueError(f"Invalid TRADING_MODE: {v}. Must be 'paper' or 'live'")
         return v
-    
+
     @field_validator('FORCE_TRADES')
     @classmethod
     def validate_force_trades(cls, v):
         """Warn about dangerous FORCE_TRADES setting."""
         if v:
-            logger.warning("⚠️  FORCE_TRADES is enabled - this bypasses safety checks!")
+            logger.warning('⚠️  FORCE_TRADES is enabled - this bypasses safety checks!')
         return v
-
-
-# Create global settings instance
 if PYDANTIC_AVAILABLE:
     settings = Settings()
 else:
-    # AI-AGENT-REF: Fallback settings for testing mode when pydantic unavailable
+
     class FallbackSettings:
+
         def __init__(self):
-            # Set reasonable defaults for testing
-            self.ALPACA_API_KEY = os.getenv("ALPACA_API_KEY", "test_key")
-            self.ALPACA_SECRET_KEY = os.getenv("ALPACA_SECRET_KEY", "test_secret")
-            self.ALPACA_BASE_URL = os.getenv("ALPACA_BASE_URL", "https://paper-api.alpaca.markets")
-            self.WEBHOOK_SECRET = os.getenv("WEBHOOK_SECRET", "test_webhook_secret")
-            self.BOT_MODE = os.getenv("BOT_MODE", "testing")
-            self.TRADING_MODE = os.getenv("TRADING_MODE", "paper")
-            self.MODEL_PATH = os.getenv("MODEL_PATH", "trained_model.pkl")
-            self.TRADE_LOG_FILE = os.getenv("TRADE_LOG_FILE", "trades.csv")
-            self.DRY_RUN = bool(int(os.getenv("DRY_RUN", "1")))
-            self.SHADOW_MODE = bool(int(os.getenv("SHADOW_MODE", "1")))
-            self.FORCE_TRADES = bool(int(os.getenv("FORCE_TRADES", "0")))
-            # Add additional fields that config.py might need
-            self.NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")
-            self.FUNDAMENTAL_API_KEY = os.getenv("FUNDAMENTAL_API_KEY", "")
-            self.FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY", "")
-            self.TRADE_AUDIT_DIR = os.getenv("TRADE_AUDIT_DIR", "logs/audit")
-            self.FLASK_PORT = int(os.getenv("FLASK_PORT", "9001"))
-            # AI-AGENT-REF: Increase default position limit for better portfolio utilization
-            self.MAX_PORTFOLIO_POSITIONS = int(os.getenv("MAX_PORTFOLIO_POSITIONS", "20"))
-            
-        def __getattr__(self, name):
-            # AI-AGENT-REF: Dynamic fallback for any missing attributes
-            # Provide sensible defaults for common numeric fields
-            numeric_fields = [
-                'SEED', 'HEALTHCHECK_PORT', 'MIN_HEALTH_ROWS', 'MIN_HEALTH_ROWS_DAILY',
-                'RATE_LIMIT_BUDGET', 'SCHEDULER_SLEEP_SECONDS', 'POSITION_SCALE_FACTOR',
-                'VOLATILITY_LOOKBACK_DAYS', 'SIGNAL_CONFIRMATION_BARS', 'TRADE_COOLDOWN_MIN'
-            ]
-            float_fields = [
-                'BUY_THRESHOLD', 'LIMIT_ORDER_SLIPPAGE', 'DELTA_THRESHOLD', 'MIN_CONFIDENCE',
-                'ATR_MULTIPLIER', 'MAX_EXPOSURE', 'RISK_BUDGET'
-            ]
-            bool_fields = [
-                'RUN_HEALTHCHECK', 'USE_RL_AGENT', 'FORCE_TRADES', 'MEMORY_OPTIMIZED',
-                'DRY_RUN', 'SHADOW_MODE', 'ENABLE_CACHE'
-            ]
-            
-            if name.upper() in numeric_fields:
-                return 42  # Default numeric value
-            elif name.upper() in float_fields:
-                return 0.5  # Default float value
-            elif name.upper() in bool_fields:
-                return 0  # Default boolean (as int)
-            return os.getenv(name, "")
-    
+            self.ALPACA_API_KEY = os.getenv('ALPACA_API_KEY', 'test_key')
+            self.ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY', 'test_secret')
+            self.ALPACA_BASE_URL = os.getenv('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
+            self.WEBHOOK_SECRET = os.getenv('WEBHOOK_SECRET', 'test_webhook_secret')
+            self.BOT_MODE = os.getenv('BOT_MODE', 'testing')
+            self.TRADING_MODE = os.getenv('TRADING_MODE', 'paper')
+            self.MODEL_PATH = os.getenv('MODEL_PATH', 'trained_model.pkl')
+            self.TRADE_LOG_FILE = os.getenv('TRADE_LOG_FILE', 'trades.csv')
+            self.DRY_RUN = bool(int(os.getenv('DRY_RUN', '1')))
+            self.SHADOW_MODE = bool(int(os.getenv('SHADOW_MODE', '1')))
+            self.FORCE_TRADES = bool(int(os.getenv('FORCE_TRADES', '0')))
+            self.NEWS_API_KEY = os.getenv('NEWS_API_KEY', '')
+            self.FUNDAMENTAL_API_KEY = os.getenv('FUNDAMENTAL_API_KEY', '')
+            self.FINNHUB_API_KEY = os.getenv('FINNHUB_API_KEY', '')
+            self.TRADE_AUDIT_DIR = os.getenv('TRADE_AUDIT_DIR', 'logs/audit')
+            self.FLASK_PORT = int(os.getenv('FLASK_PORT', '9001'))
+            self.MAX_PORTFOLIO_POSITIONS = int(os.getenv('MAX_PORTFOLIO_POSITIONS', '20'))
     settings = FallbackSettings()
-    logger.warning("Using fallback settings - pydantic-settings not available")
-
+    logger.warning('Using fallback settings - pydantic-settings not available')
 
 def validate_trading_mode() -> None:
     """Validate trading mode configuration and set appropriate URLs."""
-    # AI-AGENT-REF: Trading mode validation and URL configuration
-    if settings.TRADING_MODE not in ["paper", "live"]:
+    if settings.TRADING_MODE not in ['paper', 'live']:
         raise ValueError(f"Invalid TRADING_MODE: {settings.TRADING_MODE}. Must be 'paper' or 'live'")
-    
-    # Auto-configure base URL based on trading mode if not explicitly set
-    if settings.TRADING_MODE == "live":
-        if settings.ALPACA_BASE_URL == "https://paper-api.alpaca.markets":
+    if settings.TRADING_MODE == 'live':
+        if settings.ALPACA_BASE_URL == 'https://paper-api.alpaca.markets':
             logger.warning("TRADING_MODE is 'live' but ALPACA_BASE_URL is paper trading URL")
-            logger.warning("Auto-configuring for live trading. Set ALPACA_BASE_URL explicitly to override.")
-            settings.ALPACA_BASE_URL = "https://api.alpaca.markets"
-        logger.critical("LIVE TRADING MODE ENABLED - Real money at risk!")
+            logger.warning('Auto-configuring for live trading. Set ALPACA_BASE_URL explicitly to override.')
+            settings.ALPACA_BASE_URL = 'https://api.alpaca.markets'
+        logger.critical('LIVE TRADING MODE ENABLED - Real money at risk!')
     else:
-        if settings.ALPACA_BASE_URL != "https://paper-api.alpaca.markets":
+        if settings.ALPACA_BASE_URL != 'https://paper-api.alpaca.markets':
             logger.info("TRADING_MODE is 'paper', ensuring paper trading URL")
-            settings.ALPACA_BASE_URL = "https://paper-api.alpaca.markets"
-        logger.info("Paper trading mode enabled - Safe for testing")
-
+            settings.ALPACA_BASE_URL = 'https://paper-api.alpaca.markets'
+        logger.info('Paper trading mode enabled - Safe for testing')
 
 def get_alpaca_config() -> dict:
     """Get Alpaca client configuration based on current settings."""
     validate_trading_mode()
-    return {
-        "api_key": settings.ALPACA_API_KEY,
-        "secret_key": settings.ALPACA_SECRET_KEY,
-        "base_url": settings.ALPACA_BASE_URL,
-        "paper": settings.TRADING_MODE == "paper"
-    }
-
+    return {'api_key': settings.ALPACA_API_KEY, 'secret_key': settings.ALPACA_SECRET_KEY, 'base_url': settings.ALPACA_BASE_URL, 'paper': settings.TRADING_MODE == 'paper'}
 
 def validate_environment() -> tuple[bool, List[str], Settings]:
     """
@@ -306,110 +210,71 @@         - settings: Validated settings object (None if validation failed)
     """
     errors = []
-    
     try:
-        # Additional security checks
         security_errors = _perform_security_checks(settings)
         errors.extend(security_errors)
-        
-        # File system checks
         filesystem_errors = _validate_file_paths(settings)
         errors.extend(filesystem_errors)
-        
-        # API connectivity checks (optional)
         api_errors = _validate_api_connectivity(settings)
         errors.extend(api_errors)
-        
         is_valid = len(errors) == 0
-        
         if is_valid:
-            logger.info("✅ Environment validation passed")
+            logger.info('✅ Environment validation passed')
         else:
-            logger.error("❌ Environment validation failed with %d errors", len(errors))
+            logger.error('❌ Environment validation failed with %d errors', len(errors))
             for error in errors:
-                logger.error("  - %s", error)
-        
-        return is_valid, errors, settings
-        
+                logger.error('  - %s', error)
+        return (is_valid, errors, settings)
     except Exception as e:
-        error_msg = f"Critical validation error: {e}"
+        error_msg = f'Critical validation error: {e}'
         logger.error(error_msg)
-        return False, [error_msg], None
-
+        return (False, [error_msg], None)
 
 def _perform_security_checks(settings: Settings) -> List[str]:
     """Perform additional security validation checks."""
     errors = []
-    
-    # Check for development/testing indicators in production
     if 'paper' not in settings.ALPACA_BASE_URL.lower():
         if settings.BOT_MODE in ['testing', 'development']:
-            errors.append("Production API URL detected with test/dev mode")
-        
+            errors.append('Production API URL detected with test/dev mode')
         if settings.DRY_RUN:
-            logger.warning("DRY_RUN enabled with production API - trades will not execute")
-    
-    # Validate webhook security
+            logger.warning('DRY_RUN enabled with production API - trades will not execute')
     if settings.WEBHOOK_SECRET and len(settings.WEBHOOK_SECRET) < 32:
-        errors.append("WEBHOOK_SECRET should be at least 32 characters for security")
-    
-    # Check for insecure configurations
+        errors.append('WEBHOOK_SECRET should be at least 32 characters for security')
     if settings.FORCE_TRADES and 'paper' not in settings.ALPACA_BASE_URL.lower():
-        errors.append("FORCE_TRADES should never be enabled in production")
-    
+        errors.append('FORCE_TRADES should never be enabled in production')
     return errors
-
 
 def _validate_file_paths(settings: Settings) -> List[str]:
     """Validate file paths and permissions."""
     errors = []
-    
-    # Check if log directory is writable
     try:
         log_dir = os.path.dirname(settings.TRADE_LOG_FILE)
-        if log_dir and not os.path.exists(log_dir):
+        if log_dir and (not os.path.exists(log_dir)):
             os.makedirs(log_dir, exist_ok=True)
-        
-        # Test write permission
         test_file = os.path.join(log_dir or '.', '.write_test')
         with open(test_file, 'w') as f:
             f.write('test')
         os.remove(test_file)
-        
     except (OSError, IOError) as e:
-        errors.append(f"Cannot write to log directory: {e}")
-    
-    # Check model file existence (if required)
-    if not settings.DRY_RUN and not settings.SHADOW_MODE:
+        errors.append(f'Cannot write to log directory: {e}')
+    if not settings.DRY_RUN and (not settings.SHADOW_MODE):
         if not os.path.exists(settings.MODEL_PATH):
-            logger.warning(f"Model file not found: {settings.MODEL_PATH}")
-    
+            logger.warning(f'Model file not found: {settings.MODEL_PATH}')
     return errors
-
 
 def _validate_api_connectivity(settings: Settings) -> List[str]:
     """Optional API connectivity validation."""
     errors = []
-    
-    # This is optional and can be skipped if APIs are temporarily unavailable
     try:
-        # Basic URL validation
         from ai_trading.utils import http
-        
-        # Test Alpaca API reachability (without authentication)
         response = http.head(settings.ALPACA_BASE_URL)
         if response.status_code >= 500:
-            logger.warning(f"Alpaca API may be experiencing issues: {response.status_code}")
-            
+            logger.warning(f'Alpaca API may be experiencing issues: {response.status_code}')
     except Exception:
-        # API connectivity issues are warnings, not errors
-        logger.warning("Could not verify API connectivity (this may be temporary)")
+        logger.warning('Could not verify API connectivity (this may be temporary)')
     except ImportError:
-        # http utilities not available - skip connectivity check
         pass
-    
     return errors
-
 
 def validate_trading_environment() -> bool:
     """
@@ -421,25 +286,18 @@         True if environment is ready for trading, False otherwise
     """
     is_valid, errors, settings_obj = validate_environment()
-    
     if not is_valid:
-        logger.error("Environment validation failed - trading cannot proceed")
+        logger.error('Environment validation failed - trading cannot proceed')
         return False
-    
-    # Additional runtime checks
     if settings.BOT_MODE == 'testing' and 'paper' not in settings.ALPACA_BASE_URL.lower():
-        logger.error("Testing mode with production API is not allowed")
+        logger.error('Testing mode with production API is not allowed')
         return False
-    
-    logger.info(f"Trading environment ready - Mode: {settings.BOT_MODE}, URL: {settings.ALPACA_BASE_URL}")
+    logger.info(f'Trading environment ready - Mode: {settings.BOT_MODE}, URL: {settings.ALPACA_BASE_URL}')
     return True
-
 
 def generate_schema() -> dict:
     """Return JSONSchema for the environment settings."""
-    # AI-AGENT-REF: expose env schema for validation in CI
     return Settings.model_json_schema()
-
 
 def debug_environment() -> Dict[str, Any]:
     """
@@ -450,131 +308,71 @@     Dict[str, Any]
         Debug report with environment status, issues, and recommendations
     """
-    debug_report = {
-        'timestamp': datetime.now(datetime.timezone.utc).isoformat(),
-        'validation_status': 'unknown',
-        'critical_issues': [],
-        'warnings': [],
-        'environment_vars': {},
-        'recommendations': []
-    }
-    
+    debug_report = {'timestamp': datetime.now(datetime.timezone.utc).isoformat(), 'validation_status': 'unknown', 'critical_issues': [], 'warnings': [], 'environment_vars': {}, 'recommendations': []}
     try:
-        # Run full validation
         is_valid, errors, settings_obj = validate_environment()
-        
         debug_report['validation_status'] = 'valid' if is_valid else 'invalid'
         debug_report['critical_issues'] = errors
-        
-        # Check specific environment variables
-        env_vars_to_check = [
-            'ALPACA_API_KEY', 'ALPACA_SECRET_KEY', 'ALPACA_BASE_URL',
-            'NEWS_API_KEY', 'SENTIMENT_API_KEY', 'SENTIMENT_API_URL',
-            'FINNHUB_API_KEY', 'BOT_MODE', 'TRADING_MODE'
-        ]
-        
+        env_vars_to_check = ['ALPACA_API_KEY', 'ALPACA_SECRET_KEY', 'ALPACA_BASE_URL', 'NEWS_API_KEY', 'SENTIMENT_API_KEY', 'SENTIMENT_API_URL', 'FINNHUB_API_KEY', 'BOT_MODE', 'TRADING_MODE']
         for var in env_vars_to_check:
             value = os.getenv(var)
             if value:
-                # Mask sensitive values
                 if 'KEY' in var.upper() or 'SECRET' in var.upper():
-                    masked_value = f"{value[:4]}...{value[-4:]}" if len(value) > 8 else "***"
-                    debug_report['environment_vars'][var] = {
-                        'status': 'set',
-                        'value': masked_value,
-                        'length': len(value)
-                    }
+                    masked_value = f'{value[:4]}...{value[-4:]}' if len(value) > 8 else '***'
+                    debug_report['environment_vars'][var] = {'status': 'set', 'value': masked_value, 'length': len(value)}
                 else:
-                    debug_report['environment_vars'][var] = {
-                        'status': 'set',
-                        'value': value
-                    }
+                    debug_report['environment_vars'][var] = {'status': 'set', 'value': value}
             else:
-                debug_report['environment_vars'][var] = {
-                    'status': 'not_set',
-                    'value': None
-                }
-        
-        # Generate recommendations
+                debug_report['environment_vars'][var] = {'status': 'not_set', 'value': None}
         if not debug_report['environment_vars']['SENTIMENT_API_KEY']['value']:
             if debug_report['environment_vars']['NEWS_API_KEY']['value']:
-                debug_report['recommendations'].append(
-                    "Consider setting SENTIMENT_API_KEY to improve sentiment analysis performance"
-                )
+                debug_report['recommendations'].append('Consider setting SENTIMENT_API_KEY to improve sentiment analysis performance')
             else:
-                debug_report['recommendations'].append(
-                    "Set NEWS_API_KEY or SENTIMENT_API_KEY to enable sentiment analysis"
-                )
-        
+                debug_report['recommendations'].append('Set NEWS_API_KEY or SENTIMENT_API_KEY to enable sentiment analysis')
         if debug_report['environment_vars']['TRADING_MODE']['value'] == 'live':
-            debug_report['warnings'].append(
-                "LIVE TRADING MODE - Real money at risk!"
-            )
-        
-        # Check .env file accessibility
+            debug_report['warnings'].append('LIVE TRADING MODE - Real money at risk!')
         env_file_path = '.env'
         if os.path.exists(env_file_path):
-            debug_report['env_file'] = {
-                'exists': True,
-                'readable': os.access(env_file_path, os.R_OK),
-                'size_bytes': os.path.getsize(env_file_path)
-            }
+            debug_report['env_file'] = {'exists': True, 'readable': os.access(env_file_path, os.R_OK), 'size_bytes': os.path.getsize(env_file_path)}
         else:
-            debug_report['env_file'] = {
-                'exists': False,
-                'readable': False,
-                'size_bytes': 0
-            }
-            debug_report['recommendations'].append(
-                "Create .env file with required environment variables"
-            )
-            
+            debug_report['env_file'] = {'exists': False, 'readable': False, 'size_bytes': 0}
+            debug_report['recommendations'].append('Create .env file with required environment variables')
     except Exception as e:
         debug_report['validation_status'] = 'error'
-        debug_report['critical_issues'].append(f"Debug environment failed: {e}")
-        logger.error(f"Environment debug failed: {e}")
-    
+        debug_report['critical_issues'].append(f'Debug environment failed: {e}')
+        logger.error(f'Environment debug failed: {e}')
     return debug_report
-
 
 def print_environment_debug() -> None:
     """Print formatted environment debug information to console."""
     debug_report = debug_environment()
-    
-    logging.info(str("\n" + "="*60))
-    logging.info("AI TRADING BOT - ENVIRONMENT DEBUG REPORT")
-    logging.info(str("="*60))
+    logging.info(str('\n' + '=' * 60))
+    logging.info('AI TRADING BOT - ENVIRONMENT DEBUG REPORT')
+    logging.info(str('=' * 60))
     logging.info(f"Timestamp: {debug_report['timestamp']}")
     logging.info(f"Validation Status: {debug_report['validation_status'].upper()}")
-    
     if debug_report['critical_issues']:
         logging.info(f"\n🚨 CRITICAL ISSUES ({len(debug_report['critical_issues'])}):")
         for issue in debug_report['critical_issues']:
-            logging.info(f"  - {issue}")
-    
+            logging.info(f'  - {issue}')
     if debug_report['warnings']:
         logging.info(f"\n⚠️  WARNINGS ({len(debug_report['warnings'])}):")
         for warning in debug_report['warnings']:
-            logging.info(f"  - {warning}")
-    
-    logging.info("\n📋 ENVIRONMENT VARIABLES:")
+            logging.info(f'  - {warning}')
+    logging.info('\n📋 ENVIRONMENT VARIABLES:')
     for var, info in debug_report['environment_vars'].items():
-        status_emoji = "✅" if info['status'] == 'set' else "❌"
+        status_emoji = '✅' if info['status'] == 'set' else '❌'
         logging.info(f"  {status_emoji} {var}: {info.get('value', 'NOT SET')}")
-    
     if debug_report['recommendations']:
         logging.info(f"\n💡 RECOMMENDATIONS ({len(debug_report['recommendations'])}):")
         for rec in debug_report['recommendations']:
-            logging.info(f"  - {rec}")
-    
-    logging.info("\n📁 .ENV FILE:")
+            logging.info(f'  - {rec}')
+    logging.info('\n📁 .ENV FILE:')
     env_info = debug_report.get('env_file', {})
-    logging.info(f"  Exists: {'✅' if env_info.get('exists') else '❌'}")
-    logging.info(f"  Readable: {'✅' if env_info.get('readable') else '❌'}")
+    logging.info(f"  Exists: {('✅' if env_info.get('exists') else '❌')}")
+    logging.info(f"  Readable: {('✅' if env_info.get('readable') else '❌')}")
     logging.info(f"  Size: {env_info.get('size_bytes', 0)} bytes")
-    
-    logging.info(str("\n" + "="*60))
-
+    logging.info(str('\n' + '=' * 60))
 
 def validate_specific_env_var(var_name: str) -> Dict[str, Any]:
     """
@@ -590,62 +388,41 @@     Dict[str, Any]
         Validation result with status and details
     """
-    result = {
-        'variable': var_name,
-        'status': 'unknown',
-        'value': None,
-        'issues': [],
-        'recommendations': []
-    }
-    
+    result = {'variable': var_name, 'status': 'unknown', 'value': None, 'issues': [], 'recommendations': []}
     value = os.getenv(var_name)
     if not value:
         result['status'] = 'missing'
-        result['issues'].append(f"{var_name} is not set")
+        result['issues'].append(f'{var_name} is not set')
         return result
-    
     result['value'] = value
     result['status'] = 'set'
-    
-    # Specific validations based on variable name
     if var_name in ['ALPACA_API_KEY', 'ALPACA_SECRET_KEY']:
         if len(value) < 10:
-            result['issues'].append(f"{var_name} appears too short")
+            result['issues'].append(f'{var_name} appears too short')
         if value.lower() in ['your_api_key', 'test', 'placeholder']:
-            result['issues'].append(f"{var_name} appears to be a placeholder value")
-    
+            result['issues'].append(f'{var_name} appears to be a placeholder value')
     elif var_name == 'ALPACA_BASE_URL':
         if not value.startswith('https://'):
-            result['issues'].append("ALPACA_BASE_URL should use HTTPS")
+            result['issues'].append('ALPACA_BASE_URL should use HTTPS')
         if 'paper' in value and var_name == 'live':
-            result['issues'].append("Paper URL with live trading mode")
-    
+            result['issues'].append('Paper URL with live trading mode')
     elif var_name == 'BOT_MODE':
         valid_modes = ['conservative', 'balanced', 'aggressive', 'paper', 'testing']
         if value.lower() not in valid_modes:
-            result['issues'].append(f"Invalid BOT_MODE. Valid options: {valid_modes}")
-    
+            result['issues'].append(f'Invalid BOT_MODE. Valid options: {valid_modes}')
     return result
 
-
-def _main() -> None:  # pragma: no cover - enhanced CLI helper
+def _main() -> None:
     """Enhanced CLI for environment validation and debugging."""
     import argparse
-    
-    parser = argparse.ArgumentParser(description="AI Trading Bot Environment Validator")
-    parser.add_argument('--debug', action='store_true', 
-                       help='Show detailed environment debug information')
-    parser.add_argument('--check', type=str, metavar='VAR_NAME',
-                       help='Check a specific environment variable')
-    parser.add_argument('--quiet', action='store_true',
-                       help='Suppress non-error output')
-    
+    parser = argparse.ArgumentParser(description='AI Trading Bot Environment Validator')
+    parser.add_argument('--debug', action='store_true', help='Show detailed environment debug information')
+    parser.add_argument('--check', type=str, metavar='VAR_NAME', help='Check a specific environment variable')
+    parser.add_argument('--quiet', action='store_true', help='Suppress non-error output')
     args = parser.parse_args()
-    
     if args.debug:
         print_environment_debug()
         return
-    
     if args.check:
         result = validate_specific_env_var(args.check)
         logging.info(f"\nVariable: {result['variable']}")
@@ -654,32 +431,24 @@             logging.info(f"Value: {result['value']}")
         elif result['value']:
             logging.info(f"Value: [MASKED - {len(result['value'])} characters]")
-        
         if result['issues']:
-            logging.info("Issues:")
+            logging.info('Issues:')
             for issue in result['issues']:
-                logging.info(f"  - {issue}")
-        
+                logging.info(f'  - {issue}')
         if result['recommendations']:
-            logging.info("Recommendations:")
+            logging.info('Recommendations:')
             for rec in result['recommendations']:
-                logging.info(f"  - {rec}")
+                logging.info(f'  - {rec}')
         return
-    
-    # Default behavior - basic validation
     is_valid, errors, _ = validate_environment()
-    
     if not args.quiet:
         if is_valid:
-            logging.info("✅ Environment validation passed")
+            logging.info('✅ Environment validation passed')
         else:
-            logging.info("❌ Environment validation failed")
+            logging.info('❌ Environment validation failed')
             for error in errors:
-                logging.info(f"  - {error}")
-            logging.info("\nUse --debug for detailed information")
-    
+                logging.info(f'  - {error}')
+            logging.info('\nUse --debug for detailed information')
     exit(0 if is_valid else 1)
-
-
-if __name__ == "__main__":
+if __name__ == '__main__':
     _main()--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/verify_config.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/scripts/verify_config.py@@ -1,3 +1,5 @@+from ai_trading.config import get_settings
+S = get_settings()
 #!/usr/bin/env python3
 import logging
 
@@ -114,8 +116,8 @@         import config
         
         # Check if keys are accessible
-        has_api_key = bool(config.ALPACA_API_KEY and config.ALPACA_API_KEY != 'YOUR_ALPACA_API_KEY_HERE')
-        has_secret = bool(config.ALPACA_SECRET_KEY and config.ALPACA_SECRET_KEY != 'YOUR_ALPACA_SECRET_KEY_HERE')
+        has_api_key = bool(S.alpaca_api_key and S.alpaca_api_key != 'YOUR_ALPACA_API_KEY_HERE')
+        has_secret = bool(S.alpaca_secret_key and S.alpaca_secret_key != 'YOUR_ALPACA_SECRET_KEY_HERE')
         
         if has_api_key and has_secret:
             return True, "✅ Configuration module imports successfully with API keys"
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/conftest.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/conftest.py@@ -1,302 +1,279 @@-
-
 import sys
 import os
-
-# AI-AGENT-REF: Mock yfinance module for deterministic tests
-class MockYfinance:
-    """Mock yfinance module to avoid network calls in tests."""
-    
-    @staticmethod
-    def download(*args, **kwargs):
-        import pandas as pd
-        return pd.DataFrame()
-    
-    class Ticker:
-        def __init__(self, *args):
-            pass
-        
-        def history(self, *args, **kwargs):
-            import pandas as pd
-            return pd.DataFrame()
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: None
-
-# Inject yfinance mock into sys.modules before any imports
-sys.modules["yfinance"] = MockYfinance()
-
-# AI-AGENT-REF: Add dotenv stub early to prevent import errors
+sys.modules['yfinance'] = MockYfinance()
 try:
     from dotenv import load_dotenv
 except Exception:
+
     def load_dotenv(*a, **k):
         pass
     import types
-    dotenv_mod = types.ModuleType("dotenv")
+    dotenv_mod = types.ModuleType('dotenv')
     dotenv_mod.load_dotenv = load_dotenv
-    dotenv_mod.__file__ = "stub"
-    sys.modules["dotenv"] = dotenv_mod
-
-# AI-AGENT-REF: Add hypothesis stub early
+    dotenv_mod.__file__ = 'stub'
+    sys.modules['dotenv'] = dotenv_mod
 try:
     from hypothesis import given, settings, HealthCheck
 except Exception:
     import types
-    
+
     def given(**strategy_kwargs):
+
         def decorator(f):
-            # For now, just skip hypothesis-based tests to avoid complexity
             import pytest
-            return pytest.mark.skip("hypothesis-based test - skipped in simple test mode")(f)
+            return pytest.mark.skip('hypothesis-based test - skipped in simple test mode')(f)
         return decorator
-    
+
     def settings(*args, **kwargs):
+
         def decorator(f):
             return f
         return decorator
-    
+
     class HealthCheck:
-        too_slow = "too_slow"
-        filter_too_much = "filter_too_much"
-        function_scoped_fixture = "function_scoped_fixture"
-    
-    # Add strategies module
+        too_slow = 'too_slow'
+        filter_too_much = 'filter_too_much'
+        function_scoped_fixture = 'function_scoped_fixture'
+
     class Strategies:
+
         @staticmethod
         def text():
-            return "test_string"
-        
+            return 'test_string'
+
         @staticmethod
         def integers(min_value=None, max_value=None):
             return 42
-            
+
         @staticmethod
         def floats(min_value=None, max_value=None, allow_nan=True, allow_infinity=True, **kwargs):
             return 1.0
-            
+
         @staticmethod
         def lists(elements, min_size=0, max_size=None, **kwargs):
-            # Generate a list based on the element strategy
-            size = max(min_size, 40)  # Use a size that meets min_size requirements
+            size = max(min_size, 40)
             if hasattr(elements, '__call__'):
                 return [elements() for _ in range(size)]
             else:
                 return [1.0] * size
-    
-    hypothesis_mod = types.ModuleType("hypothesis")
+    hypothesis_mod = types.ModuleType('hypothesis')
     hypothesis_mod.given = given
     hypothesis_mod.settings = settings
     hypothesis_mod.HealthCheck = HealthCheck
     hypothesis_mod.strategies = Strategies()
-    hypothesis_mod.__file__ = "stub"
-    sys.modules["hypothesis"] = hypothesis_mod
-
-# AI-AGENT-REF: Add portalocker stub early
-try:
-    pass
-except Exception:
-    import types
+    hypothesis_mod.__file__ = 'stub'
+    sys.modules['hypothesis'] = hypothesis_mod
+try:
+    pass
+except Exception:
+    import types
+
     class LockStub:
-        def __init__(self, *args, **kwargs):
-            pass
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def __enter__(self):
             return self
+
         def __exit__(self, *args):
             pass
-    
-    portalocker_mod = types.ModuleType("portalocker")
+    portalocker_mod = types.ModuleType('portalocker')
     portalocker_mod.Lock = LockStub
     portalocker_mod.LOCK_EX = 1
     portalocker_mod.LOCK_NB = 2
-    portalocker_mod.__file__ = "stub"
-    sys.modules["portalocker"] = portalocker_mod
-
-# AI-AGENT-REF: Add schedule stub early
-try:
-    pass
-except Exception:
-    import types
+    portalocker_mod.__file__ = 'stub'
+    sys.modules['portalocker'] = portalocker_mod
+try:
+    pass
+except Exception:
+    import types
+
     class ScheduleStub:
+
         def __init__(self):
             pass
+
         def every(self, *args):
             return self
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: self
-    
-    schedule_mod = types.ModuleType("schedule")
+    schedule_mod = types.ModuleType('schedule')
     schedule_mod.every = lambda *a: ScheduleStub()
     schedule_mod.run_pending = lambda: None
-    schedule_mod.__file__ = "stub"
-    sys.modules["schedule"] = schedule_mod
-
-# AI-AGENT-REF: Add gymnasium stub for RL tests
-try:
-    pass
-except Exception:
-    import types
-    
+    schedule_mod.__file__ = 'stub'
+    sys.modules['schedule'] = schedule_mod
+try:
+    pass
+except Exception:
+    import types
+
     class Space:
-        def __init__(self, *args, **kwargs):
-            pass
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class Discrete(Space):
+
         def __init__(self, n):
             self.n = n
-    
+
     class Box(Space):
+
         def __init__(self, low, high, shape=None, dtype=None):
             self.low = low
             self.high = high
             self.shape = shape
             self.dtype = dtype
-    
+
     class Spaces:
         Discrete = Discrete
         Box = Box
-    
+
     class Env:
+
         def __init__(self):
             self.action_space = None
             self.observation_space = None
-        
+
         def reset(self):
-            return None, {}
-        
+            return (None, {})
+
         def step(self, action):
-            return None, 0, False, False, {}
-        
+            return (None, 0, False, False, {})
+
         def render(self):
             pass
-        
+
         def close(self):
             pass
-    
-    gymnasium_mod = types.ModuleType("gymnasium")
+    gymnasium_mod = types.ModuleType('gymnasium')
     gymnasium_mod.Env = Env
     gymnasium_mod.spaces = Spaces()
-    gymnasium_mod.__file__ = "stub"
-    sys.modules["gymnasium"] = gymnasium_mod
-
-# AI-AGENT-REF: Add hmmlearn stub
-try:
-    pass
-except Exception:
-    import types
-    hmmlearn_mod = types.ModuleType("hmmlearn")
-    hmm_mod = types.ModuleType("hmmlearn.hmm")
-    
+    gymnasium_mod.__file__ = 'stub'
+    sys.modules['gymnasium'] = gymnasium_mod
+try:
+    pass
+except Exception:
+    import types
+    hmmlearn_mod = types.ModuleType('hmmlearn')
+    hmm_mod = types.ModuleType('hmmlearn.hmm')
+
     class GaussianHMM:
-        def __init__(self, *args, **kwargs):
-            pass
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def fit(self, *args, **kwargs):
             return self
+
         def predict(self, *args, **kwargs):
-            return [0, 1, 0, 1]  # Mock regime predictions
-    
+            return [0, 1, 0, 1]
     hmm_mod.GaussianHMM = GaussianHMM
     hmmlearn_mod.hmm = hmm_mod
-    hmmlearn_mod.__file__ = "stub"
-    sys.modules["hmmlearn"] = hmmlearn_mod
-    sys.modules["hmmlearn.hmm"] = hmm_mod
-
-# AI-AGENT-REF: Add finnhub stub
-try:
-    pass
-except Exception:
-    import types
-    
+    hmmlearn_mod.__file__ = 'stub'
+    sys.modules['hmmlearn'] = hmmlearn_mod
+    sys.modules['hmmlearn.hmm'] = hmm_mod
+try:
+    pass
+except Exception:
+    import types
+
     class FinnhubAPIException(Exception):
+
         def __init__(self, *args, status_code=None, **kwargs):
             self.status_code = status_code
             super().__init__(*args)
-    
+
     class Client:
+
         def __init__(self, api_key=None):
             self.api_key = api_key
-        
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
-    finnhub_mod = types.ModuleType("finnhub")
+    finnhub_mod = types.ModuleType('finnhub')
     finnhub_mod.FinnhubAPIException = FinnhubAPIException
     finnhub_mod.Client = Client
-    finnhub_mod.__file__ = "stub"
-    sys.modules["finnhub"] = finnhub_mod
-
-# AI-AGENT-REF: Add torch stub for RL tests
-try:
-    pass
-except Exception:
-    import types
-    
+    finnhub_mod.__file__ = 'stub'
+    sys.modules['finnhub'] = finnhub_mod
+try:
+    pass
+except Exception:
+    import types
+
     class Parameter:
+
         def __init__(self, data):
             self.data = data
-    
+
     class Module:
+
         def __init__(self):
             pass
-        
+
         def parameters(self):
             return [Parameter([1.0, 2.0])]
-        
+
         def __call__(self, *args, **kwargs):
             return self.forward(*args, **kwargs)
-        
+
         def forward(self, x):
             return x
-    
+
     class Linear(Module):
+
         def __init__(self, in_features, out_features):
             super().__init__()
             self.in_features = in_features
             self.out_features = out_features
-    
+
     class ReLU(Module):
         pass
-    
+
     class Tanh(Module):
         pass
-    
+
     class Sequential(Module):
+
         def __init__(self, *layers):
             super().__init__()
             self.layers = layers
-    
+
     class Tensor:
+
         def __init__(self, data):
             self.data = data
+
         def detach(self):
             return self
+
         def numpy(self):
             import numpy as np
-            return np.array([0.2, 0.2, 0.2, 0.2, 0.2])  # Mock equal weight portfolio
-    
+            return np.array([0.2, 0.2, 0.2, 0.2, 0.2])
+
     class OptimModule:
+
         class Adam:
-            def __init__(self, parameters, lr=1e-3):
+
+            def __init__(self, parameters, lr=0.001):
                 self.parameters = parameters
                 self.lr = lr
+
             def step(self):
                 pass
+
             def zero_grad(self):
                 pass
-    
+
     def tensor(data, dtype=None):
         return Tensor(data)
-    
+
     def manual_seed(seed):
         pass
-    
+
     class Softmax(Module):
+
         def __init__(self, dim=-1):
             super().__init__()
             self.dim = dim
-    
-    torch_mod = types.ModuleType("torch")
-    torch_mod.nn = types.ModuleType("torch.nn")
+    torch_mod = types.ModuleType('torch')
+    torch_mod.nn = types.ModuleType('torch.nn')
     torch_mod.optim = OptimModule()
     torch_mod.nn.Module = Module
     torch_mod.nn.Linear = Linear
@@ -308,106 +285,89 @@     torch_mod.tensor = tensor
     torch_mod.Tensor = Tensor
     torch_mod.manual_seed = manual_seed
-    torch_mod.SymInt = int  # Mock SymInt for version check
-    torch_mod.float32 = "float32"
-    torch_mod.__file__ = "stub"
-    sys.modules["torch"] = torch_mod
-    sys.modules["torch.nn"] = torch_mod.nn
-    sys.modules["torch.optim"] = torch_mod.optim
-
-# AI-AGENT-REF: Add tenacity stub for retry decorators
+    torch_mod.SymInt = int
+    torch_mod.float32 = 'float32'
+    torch_mod.__file__ = 'stub'
+    sys.modules['torch'] = torch_mod
+    sys.modules['torch.nn'] = torch_mod.nn
+    sys.modules['torch.optim'] = torch_mod.optim
 try:
     from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
 except Exception:
     import types
-    
+
     def retry(*args, **kwargs):
+
         def decorator(f):
             return f
         return decorator
-    
+
     class WaitStub:
+
         def __add__(self, other):
             return self
+
         def __radd__(self, other):
             return self
-    
+
     def wait_exponential(*args, **kwargs):
         return WaitStub()
-    
+
     def wait_random(*args, **kwargs):
         return WaitStub()
-    
+
     def stop_after_attempt(*args, **kwargs):
         return None
-    
+
     def retry_if_exception_type(*args, **kwargs):
         return None
-    
+
     class RetryError(Exception):
         pass
-    
-    tenacity_mod = types.ModuleType("tenacity")
+    tenacity_mod = types.ModuleType('tenacity')
     tenacity_mod.retry = retry
     tenacity_mod.wait_exponential = wait_exponential
     tenacity_mod.wait_random = wait_random
     tenacity_mod.stop_after_attempt = stop_after_attempt
     tenacity_mod.retry_if_exception_type = retry_if_exception_type
     tenacity_mod.RetryError = RetryError
-    tenacity_mod.__file__ = "stub"
-    sys.modules["tenacity"] = tenacity_mod
-
-# AI-AGENT-REF: Set test environment variables early to avoid config import errors
-os.environ.update({
-    "ALPACA_API_KEY": "FAKE_TEST_API_KEY_NOT_REAL_123456789",  # Valid format
-    "ALPACA_SECRET_KEY": "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789",  # Valid format
-    "ALPACA_BASE_URL": "https://paper-api.alpaca.markets",
-    "ALPACA_DATA_FEED": "iex",
-    "WEBHOOK_SECRET": "fake-test-webhook-not-real",
-    "FLASK_PORT": "9000",
-    "TESTING": "1"
-})
-
+    tenacity_mod.__file__ = 'stub'
+    sys.modules['tenacity'] = tenacity_mod
+os.environ.update({'ALPACA_API_KEY': 'FAKE_TEST_API_KEY_NOT_REAL_123456789', 'ALPACA_SECRET_KEY': 'FAKE_TEST_SECRET_KEY_NOT_REAL_123456789', 'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets', 'ALPACA_DATA_FEED': 'iex', 'WEBHOOK_SECRET': 'fake-test-webhook-not-real', 'FLASK_PORT': '9000', 'TESTING': '1'})
 sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
 from pathlib import Path
-
 import pytest
 import types
-
-# AI-AGENT-REF: Add numpy stub before any imports that might need it
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+try:
+    pass
+except Exception:
+    import types
+
     class ArrayStub(list):
+
         def __init__(self, data=None, dtype=None):
             super().__init__(data or [])
             self.dtype = dtype
-            
+
         def __array__(self):
             return self
-            
+
         def reshape(self, *args):
             return self
-            
+
         def __sub__(self, other):
             if isinstance(other, (list, ArrayStub)):
                 return ArrayStub([a - b for a, b in zip(self, other)])
             return ArrayStub([x - other for x in self])
-            
+
         def __truediv__(self, other):
             if isinstance(other, (list, ArrayStub)):
                 return ArrayStub([a / b if b != 0 else 0 for a, b in zip(self, other)])
             return ArrayStub([x / other if other != 0 else 0 for x in self])
-            
+
         def max(self):
             return max(self) if self else 0
-            
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: self
-    
-    numpy_mod = types.ModuleType("numpy")
+    numpy_mod = types.ModuleType('numpy')
     numpy_mod.array = ArrayStub
     numpy_mod.ndarray = ArrayStub
     numpy_mod.nan = float('nan')
@@ -421,9 +381,9 @@     numpy_mod.sum = sum
     numpy_mod.exp = lambda x: 2.718281828 ** x
     numpy_mod.log = lambda x: 0.0
-    
-    # Create maximum with accumulate method
+
     class MaximumStub:
+
         @staticmethod
         def accumulate(arr):
             """Mock accumulate that returns cumulative max."""
@@ -435,234 +395,224 @@                 max_so_far = max(max_so_far, val)
                 result.append(max_so_far)
             return ArrayStub(result)
-        
+
         def __call__(self, *args):
             return max(*args) if args else 0
-    
     numpy_mod.maximum = MaximumStub()
     numpy_mod.minimum = min
     numpy_mod.max = lambda x: max(x) if x else 0
     numpy_mod.isscalar = lambda x: isinstance(x, (int, float, complex))
     numpy_mod.bool_ = bool
     numpy_mod.linspace = lambda start, stop, num: ArrayStub([start + (stop - start) * i / (num - 1) for i in range(num)])
-    
-    # Add random module stub
+
     class RandomStub:
+
         @staticmethod
         def seed(x):
             pass
+
         @staticmethod
         def random(*args, **kwargs):
             if 'size' in kwargs:
                 size = kwargs['size']
                 return [0.5] * size
             return 0.5
+
         @staticmethod
         def randint(*args, **kwargs):
             if 'size' in kwargs:
                 size = kwargs['size']
                 return [1] * size
             return 1
+
         @staticmethod
         def choice(arr):
             return arr[0] if arr else None
+
         @staticmethod
         def normal(*args, **kwargs):
             if 'size' in kwargs:
                 size = kwargs['size']
                 return [0.0] * size
             return 0.0
-    
     numpy_mod.random = RandomStub()
-    numpy_mod.__file__ = "stub"
-    sys.modules["numpy"] = numpy_mod
-    sys.modules["np"] = numpy_mod
-
+    numpy_mod.__file__ = 'stub'
+    sys.modules['numpy'] = numpy_mod
+    sys.modules['np'] = numpy_mod
 try:
     import urllib3
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    urllib3 = types.ModuleType("urllib3")
-    urllib3.__file__ = "stub"
-    sys.modules["urllib3"] = urllib3
-
-# AI-AGENT-REF: Add pandas stub for strategy allocator tests
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
-    # Create pandas stub module
-    pandas_mod = types.ModuleType("pandas")
-    
-    # Create minimal DataFrame stub
+except Exception:
+    import types
+    urllib3 = types.ModuleType('urllib3')
+    urllib3.__file__ = 'stub'
+    sys.modules['urllib3'] = urllib3
+try:
+    pass
+except Exception:
+    import types
+    pandas_mod = types.ModuleType('pandas')
+
     class DataFrameStub:
+
         def __init__(self, data=None, **kwargs):
             self.data = data or {}
-            # If data is a dict with lists, use the length of the first list
-            # If data is empty dict or None, use 0 rows (empty DataFrame)
-            # Otherwise default to 5 rows for testing
             if isinstance(data, dict) and data:
                 first_key = next(iter(data))
                 self._length = len(data[first_key]) if isinstance(data[first_key], list) else 5
             elif isinstance(data, dict):
-                # Empty dict case - should be empty DataFrame
                 self._length = 0
             else:
-                # Default case for testing
                 self._length = 5
-            # Initialize index attribute for getting/setting
             self._index = None
-                
+
         def __len__(self):
             return self._length
-            
+
         def __getitem__(self, key):
-            # Handle list of column names (multiple column selection)
             if isinstance(key, list):
-                # Return a new DataFrame with selected columns
                 selected_data = {}
                 for col in key:
                     if isinstance(self.data, dict) and col in self.data:
                         selected_data[col] = self.data[col]
                     else:
-                        selected_data[col] = [1] * self._length  # Fallback data
+                        selected_data[col] = [1] * self._length
                 return DataFrameStub(selected_data)
-            # Handle single column name
             elif isinstance(self.data, dict) and key in self.data:
                 return SeriesStub(self.data[key])
-            return SeriesStub([1, 2, 3])  # Fallback for missing keys
-            
+            return SeriesStub([1, 2, 3])
+
         def iloc(self):
             return self
-            
-        @property 
+
+        @property
         def columns(self):
+
             class ColumnsStub(list):
+
                 def __init__(self, data):
                     super().__init__(data)
+
                 def tolist(self):
                     return list(self)
             if isinstance(self.data, dict):
                 return ColumnsStub(list(self.data.keys()))
-            return ColumnsStub(["open", "high", "low", "close", "volume"])  # Default columns
-            
+            return ColumnsStub(['open', 'high', 'low', 'close', 'volume'])
+
         @property
         def index(self):
             if self._index is None:
+
                 class IndexStub:
                     dtype = object
+
                     def get_level_values(self, level):
                         return [1, 2, 3]
+
                     def __getitem__(self, idx):
-                        return (1, 2)  # Return a tuple for MultiIndex-like behavior
+                        return (1, 2)
+
                     def tz_localize(self, tz):
-                        return self  # Return self for method chaining
-                    @property 
+                        return self
+
+                    @property
                     def tz(self):
-                        return None  # No timezone by default
+                        return None
                 self._index = IndexStub()
             return self._index
-            
-        @index.setter 
+
+        @index.setter
         def index(self, value):
             self._index = value
-            
+
         @property
         def empty(self):
             return self._length == 0
-            
+
         def isna(self):
             """Return a DataFrame-like object with all False values (no NaN in test data)."""
+
             class IsNaResult:
+
                 def any(self):
                     """Return a Series-like object with all False values."""
+
                     class AnyResult:
+
                         def any(self):
                             """Return False since we have no NaN values in test data."""
                             return False
                     return AnyResult()
             return IsNaResult()
-            
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: self
-    
-    # Create minimal Series stub
+
     class SeriesStub(list):
+
         def __init__(self, data=None):
             super().__init__(data or [1, 2, 3])
-            
+
         @property
         def is_monotonic_increasing(self):
-            return True  # Mock for monotonic check
-            
+            return True
+
         @property
         def empty(self):
             return len(self) == 0
-            
+
         @property
         def iloc(self):
             """Support iloc indexing for accessing elements by position."""
+
             class IlocAccessor:
+
                 def __init__(self, series):
                     self.series = series
-                
+
                 def __getitem__(self, idx):
                     if isinstance(idx, int):
-                        # Handle negative indexing like pandas
                         if idx < 0:
                             idx = len(self.series) + idx
                         return self.series[idx] if 0 <= idx < len(self.series) else 0
                     return self.series[idx] if hasattr(self.series, '__getitem__') else 0
             return IlocAccessor(self)
-        
+
         def dropna(self):
             """Return self since we're mocking without actual NaN values."""
             return SeriesStub([x for x in self if x is not None and str(x) != 'nan'])
-        
+
         def rolling(self, window):
             """Mock rolling window operations."""
+
             class RollingStub:
+
                 def __init__(self, series, window):
                     self.series = series
                     self.window = window
-                
+
                 def mean(self):
-                    # For testing mean reversion, return a series where the last value
-                    # creates a high z-score when compared to the moving average
-                    if len(self.series) >= 2:
-                        # Create a mock rolling mean that will give us the expected z-score
-                        # For test data [1, 1, 1, 1, 5], we want the last value to have high z-score
-                        result = []
-                        for i in range(len(self.series)):
-                            if i < self.window - 1:
-                                result.append(float('nan'))  # Not enough data for window
-                            else:
-                                # Mock rolling mean - for our test case, make it around 1.5 
-                                # so that when series value is 5, z-score is high
-                                result.append(1.5)
-                        return SeriesStub(result)
-                    return SeriesStub([1.5] * len(self.series))
-                
-                def std(self, ddof=0):
-                    # For z-score calculation, return std that will give us expected result
                     if len(self.series) >= 2:
                         result = []
                         for i in range(len(self.series)):
                             if i < self.window - 1:
-                                result.append(float('nan'))  # Not enough data for window
+                                result.append(float('nan'))
                             else:
-                                # Mock rolling std - for our test, use a value that creates
-                                # a z-score > 1.0 when series=5 and mean=1.5
-                                result.append(1.5)  # (5 - 1.5) / 1.5 = 2.33 > 1.0
+                                result.append(1.5)
                         return SeriesStub(result)
                     return SeriesStub([1.5] * len(self.series))
-                
+
+                def std(self, ddof=0):
+                    if len(self.series) >= 2:
+                        result = []
+                        for i in range(len(self.series)):
+                            if i < self.window - 1:
+                                result.append(float('nan'))
+                            else:
+                                result.append(1.5)
+                        return SeriesStub(result)
+                    return SeriesStub([1.5] * len(self.series))
             return RollingStub(self, window)
-            
+
         def accumulate(self, *args, **kwargs):
-            return SeriesStub(self)  # Return self for accumulate
-        
+            return SeriesStub(self)
+
         def __sub__(self, other):
             """Support subtraction for z-score calculation."""
             if isinstance(other, SeriesStub):
@@ -675,7 +625,7 @@                 return SeriesStub(result)
             else:
                 return SeriesStub([x - other if str(x) != 'nan' else float('nan') for x in self])
-        
+
         def __truediv__(self, other):
             """Support division for z-score calculation."""
             if isinstance(other, SeriesStub):
@@ -688,29 +638,21 @@                 return SeriesStub(result)
             else:
                 return SeriesStub([x / other if str(x) != 'nan' and other != 0 else float('nan') for x in self])
-            
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: self
-    
-    # Create minimal Timestamp stub
+
     class TimestampStub:
+
         def __init__(self, *args, **kwargs):
             from datetime import datetime, timezone
-            # Handle different timestamp creation patterns
             if args:
                 if isinstance(args[0], str):
-                    # String timestamp like "2023-01-01"
                     self.value = args[0]
                 else:
                     self.value = str(args[0])
             else:
                 self.value = datetime.now(timezone.utc).isoformat()
-            
-            # Handle timezone parameter
             if 'tz' in kwargs or len(args) > 1:
                 tz = kwargs.get('tz', args[1] if len(args) > 1 else None)
-                if tz == "UTC":
-                    # Return a timezone-aware datetime
+                if tz == 'UTC':
                     if args and isinstance(args[0], str):
                         try:
                             from datetime import datetime
@@ -724,96 +666,98 @@                     self._dt = datetime.now(timezone.utc)
             else:
                 self._dt = datetime.now(timezone.utc)
-                
+
         def __str__(self):
             return self.value
-            
+
         def __repr__(self):
             return f"TimestampStub('{self.value}')"
-            
+
         @staticmethod
         def utcnow():
             from datetime import datetime, timezone
             return datetime.now(timezone.utc)
-            
+
         @staticmethod
         def now(tz=None):
             from datetime import datetime, timezone
-            if tz == "UTC" or tz == timezone.utc:
+            if tz == 'UTC' or tz == timezone.utc:
                 return datetime.now(timezone.utc)
             return datetime.now(timezone.utc)
-            
+
         def __sub__(self, other):
-            # Support timestamp arithmetic for comparisons
             from datetime import datetime, timezone, timedelta
-            return datetime.now(timezone.utc) - timedelta(days=1)  # Return a reasonable past time
-        
+            return datetime.now(timezone.utc) - timedelta(days=1)
+
         def __add__(self, other):
-            # Support timestamp + timedelta operations
             from datetime import timedelta
-            if hasattr(other, 'td'):  # TimedeltaStub
+            if hasattr(other, 'td'):
                 return TimestampStub(str(self._dt + other.td))
             return TimestampStub(str(self._dt + timedelta(minutes=1)))
-        
+
         def to_pydatetime(self):
             """Return the underlying datetime object."""
             return self._dt
-    
-    # Add pandas functions
+
     def read_csv(*args, **kwargs):
         return DataFrameStub()
-    
+
     def read_parquet(*args, **kwargs):
         return DataFrameStub()
-    
+
     def concat(*args, **kwargs):
         return DataFrameStub()
-        
+
     def to_datetime(*args, **kwargs):
-        # Return an index-like object that supports tz_localize
+
         class DatetimeIndexStub:
+
             def __init__(self, *args, **kwargs):
                 pass
+
             def tz_localize(self, tz):
-                return self  # Return self for method chaining
+                return self
+
             def tz_convert(self, tz):
-                return self  # Return self for method chaining
+                return self
+
             def __getitem__(self, idx):
                 from datetime import datetime, timezone
-                return datetime.now(timezone.utc)  # Return a timestamp
+                return datetime.now(timezone.utc)
+
             @property
             def tz(self):
                 return kwargs.get('utc') if 'utc' in kwargs else None
         return DatetimeIndexStub(*args, **kwargs)
-        
+
     def isna(obj):
         """Check for NaN values."""
-        if hasattr(obj, '__iter__') and not isinstance(obj, str):
+        if hasattr(obj, '__iter__') and (not isinstance(obj, str)):
             return [str(x) == 'nan' for x in obj]
         return str(obj) == 'nan'
-        
+
     class MultiIndex:
-        def __init__(self, *args, **kwargs):
-            pass
-
-    # Simple Timedelta stub
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class TimedeltaStub:
+
         def __init__(self, days=0, **kwargs):
             from datetime import timedelta
             self.td = timedelta(days=days, **kwargs)
-        
+
         def __rmul__(self, other):
             return self
-        
+
         def __sub__(self, other):
             return self.td
-            
+
         def __rsub__(self, other):
             from datetime import datetime, timezone
             if hasattr(other, '__sub__'):
                 return other - self.td
             return datetime.now(timezone.utc) - self.td
-
     pandas_mod.DataFrame = DataFrameStub
     pandas_mod.Timestamp = TimestampStub
     pandas_mod.Timedelta = TimedeltaStub
@@ -824,273 +768,249 @@     pandas_mod.concat = concat
     pandas_mod.to_datetime = to_datetime
     pandas_mod.isna = isna
-    pandas_mod.NaT = None  # Not a Time - represents missing timestamp
-    
-    # Add testing module
+    pandas_mod.NaT = None
+
     class TestingStub:
+
         @staticmethod
         def assert_frame_equal(df1, df2, **kwargs):
             """Mock assert_frame_equal - just pass for testing."""
             pass
-    
     pandas_mod.testing = TestingStub()
-    pandas_mod.__file__ = "stub"
-    sys.modules["pandas"] = pandas_mod
-    sys.modules["pd"] = pandas_mod
-try:
-    pass  # ensure real package available
-except Exception:  # pragma: no cover - allow missing in test env
-    req_mod = types.ModuleType("requests")
-    exc_mod = types.ModuleType("requests.exceptions")
+    pandas_mod.__file__ = 'stub'
+    sys.modules['pandas'] = pandas_mod
+    sys.modules['pd'] = pandas_mod
+try:
+    pass
+except Exception:
+    req_mod = types.ModuleType('requests')
+    exc_mod = types.ModuleType('requests.exceptions')
     exc_mod.RequestException = Exception
     req_mod.get = lambda *a, **k: None
     req_mod.Session = lambda *a, **k: None
     req_mod.exceptions = exc_mod
-    sys.modules["requests"] = req_mod
-    sys.modules["requests.exceptions"] = exc_mod
-
-# AI-AGENT-REF: Add additional dependency stubs for tests
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    ta_mod = types.ModuleType("pandas_ta")
-    ta_mod.rsi = lambda *a, **k: [50] * 14  # Return dummy RSI values
-    ta_mod.atr = lambda *a, **k: [1.0] * 14  # Return dummy ATR values
-    # AI-AGENT-REF: Add missing TA-Lib compatible methods for test compatibility
-    ta_mod.SMA = lambda data, timeperiod=20: [sum(data[max(0, i-timeperiod+1):i+1])/min(timeperiod, i+1) for i in range(len(data))]
-    ta_mod.EMA = lambda data, timeperiod=20: data  # Simplified for testing
-    ta_mod.RSI = lambda data, timeperiod=14: [50.0] * len(data)  # Simplified for testing
+    sys.modules['requests'] = req_mod
+    sys.modules['requests.exceptions'] = exc_mod
+try:
+    pass
+except Exception:
+    import types
+    ta_mod = types.ModuleType('pandas_ta')
+    ta_mod.rsi = lambda *a, **k: [50] * 14
+    ta_mod.atr = lambda *a, **k: [1.0] * 14
+    ta_mod.SMA = lambda data, timeperiod=20: [sum(data[max(0, i - timeperiod + 1):i + 1]) / min(timeperiod, i + 1) for i in range(len(data))]
+    ta_mod.EMA = lambda data, timeperiod=20: data
+    ta_mod.RSI = lambda data, timeperiod=14: [50.0] * len(data)
     ta_mod.MACD = lambda data, fastperiod=12, slowperiod=26, signalperiod=9: ([0.0] * len(data), [0.0] * len(data), [0.0] * len(data))
-    ta_mod.BBANDS = lambda data, timeperiod=20, nbdevup=2, nbdevdn=2: ([x+2 for x in data], data, [x-2 for x in data])
+    ta_mod.BBANDS = lambda data, timeperiod=20, nbdevup=2, nbdevdn=2: ([x + 2 for x in data], data, [x - 2 for x in data])
     ta_mod.ATR = lambda high, low, close, timeperiod=14: [1.0] * len(close)
     ta_mod.STOCH = lambda high, low, close, fastk_period=14, slowk_period=3, slowd_period=3: ([50.0] * len(close), [50.0] * len(close))
-    ta_mod.__file__ = "stub"
-    sys.modules["pandas_ta"] = ta_mod
-
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+    ta_mod.__file__ = 'stub'
+    sys.modules['pandas_ta'] = ta_mod
+try:
+    pass
+except Exception:
+    import types
+
     def jit_stub(*args, **kwargs):
         """Stub for numba.jit decorator - just returns the function unchanged."""
         if len(args) == 1 and callable(args[0]):
-            return args[0]  # Direct decoration
+            return args[0]
         else:
-            return lambda func: func  # Parameterized decoration
-    
-    numba_mod = types.ModuleType("numba")
+            return lambda func: func
+    numba_mod = types.ModuleType('numba')
     numba_mod.jit = jit_stub
-    numba_mod.__file__ = "stub"
-    sys.modules["numba"] = numba_mod
-
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+    numba_mod.__file__ = 'stub'
+    sys.modules['numba'] = numba_mod
+try:
+    pass
+except Exception:
+    import types
+
     class BaseSettingsStub:
+
         def __init__(self, **kwargs):
-            # Read from environment variables
             import os
-            self.ALPACA_API_KEY = os.getenv("ALPACA_API_KEY", "FAKE_TEST_API_KEY_NOT_REAL_123456789")
-            self.ALPACA_SECRET_KEY = os.getenv("ALPACA_SECRET_KEY", "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789")
-            self.ALPACA_BASE_URL = os.getenv("ALPACA_BASE_URL", "https://paper-api.alpaca.markets")
-            self.ALPACA_DATA_FEED = os.getenv("ALPACA_DATA_FEED", "iex")  # Missing attribute added
-            self.FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY", None)
-            self.FUNDAMENTAL_API_KEY = os.getenv("FUNDAMENTAL_API_KEY", None)
-            self.NEWS_API_KEY = os.getenv("NEWS_API_KEY", None)
-            self.IEX_API_TOKEN = os.getenv("IEX_API_TOKEN", None)
-            self.WEBHOOK_SECRET = os.getenv("WEBHOOK_SECRET", "fake-test-webhook-not-real")
-            self.FLASK_PORT = int(os.getenv("FLASK_PORT", "9000"))
-            self.BOT_MODE = os.getenv("BOT_MODE", "balanced")
-            self.MODEL_PATH = os.getenv("MODEL_PATH", "trained_model.pkl")
-            self.HALT_FLAG_PATH = os.getenv("HALT_FLAG_PATH", "halt.flag")
-            self.MAX_PORTFOLIO_POSITIONS = int(os.getenv("MAX_PORTFOLIO_POSITIONS", "20"))
-            self.LIMIT_ORDER_SLIPPAGE = float(os.getenv("LIMIT_ORDER_SLIPPAGE", "0.005"))
-            self.HEALTHCHECK_PORT = int(os.getenv("HEALTHCHECK_PORT", "8081"))
-            self.RUN_HEALTHCHECK = os.getenv("RUN_HEALTHCHECK", "0")
-            self.BUY_THRESHOLD = float(os.getenv("BUY_THRESHOLD", "0.5"))
-            self.WEBHOOK_PORT = int(os.getenv("WEBHOOK_PORT", "9000"))
-            self.SLIPPAGE_THRESHOLD = float(os.getenv("SLIPPAGE_THRESHOLD", "0.003"))
-            self.REBALANCE_INTERVAL_MIN = int(os.getenv("REBALANCE_INTERVAL_MIN", "1440"))
-            self.SHADOW_MODE = os.getenv("SHADOW_MODE", "False").lower() == "true"
-            self.DRY_RUN = os.getenv("DRY_RUN", "False").lower() == "true"
-            self.DISABLE_DAILY_RETRAIN = os.getenv("DISABLE_DAILY_RETRAIN", "False").lower() == "true"
-            self.TRADE_LOG_FILE = os.getenv("TRADE_LOG_FILE", "data/trades.csv")
-            self.FORCE_TRADES = os.getenv("FORCE_TRADES", "False").lower() == "true"
-            self.DISASTER_DD_LIMIT = float(os.getenv("DISASTER_DD_LIMIT", "0.2"))
-            # Add missing attributes from validate_env.py
-            self.MODEL_RF_PATH = os.getenv("MODEL_RF_PATH", "model_rf.pkl")
-            self.MODEL_XGB_PATH = os.getenv("MODEL_XGB_PATH", "model_xgb.pkl")
-            self.MODEL_LGB_PATH = os.getenv("MODEL_LGB_PATH", "model_lgb.pkl")
-            self.RL_MODEL_PATH = os.getenv("RL_MODEL_PATH", "rl_agent.zip")
-            self.USE_RL_AGENT = os.getenv("USE_RL_AGENT", "False").lower() == "true"
-            self.SECTOR_EXPOSURE_CAP = float(os.getenv("SECTOR_EXPOSURE_CAP", "0.4"))
-            self.MAX_OPEN_POSITIONS = int(os.getenv("MAX_OPEN_POSITIONS", "10"))
-            self.WEEKLY_DRAWDOWN_LIMIT = float(os.getenv("WEEKLY_DRAWDOWN_LIMIT", "0.15"))
-            self.VOLUME_THRESHOLD = int(os.getenv("VOLUME_THRESHOLD", "50000"))
-            self.DOLLAR_RISK_LIMIT = float(os.getenv("DOLLAR_RISK_LIMIT", "0.05"))
-            self.FINNHUB_RPM = int(os.getenv("FINNHUB_RPM", "60"))
-            self.MINUTE_CACHE_TTL = int(os.getenv("MINUTE_CACHE_TTL", "60"))
-            self.EQUITY_EXPOSURE_CAP = float(os.getenv("EQUITY_EXPOSURE_CAP", "2.5"))
-            self.PORTFOLIO_EXPOSURE_CAP = float(os.getenv("PORTFOLIO_EXPOSURE_CAP", "2.5"))
-            self.SEED = int(os.getenv("SEED", "42"))
-            self.RATE_LIMIT_BUDGET = int(os.getenv("RATE_LIMIT_BUDGET", "190"))
-            # Additional settings needed by bot_engine
-            self.pretrade_lookback_days = int(os.getenv("PRETRADE_LOOKBACK_DAYS", "120"))
-            self.pretrade_batch_size = int(os.getenv("PRETRADE_BATCH_SIZE", "50"))
-            self.intraday_batch_enable = os.getenv("INTRADAY_BATCH_ENABLE", "True").lower() == "true"
-            self.intraday_batch_size = int(os.getenv("INTRADAY_BATCH_SIZE", "40"))
-            self.batch_fallback_workers = int(os.getenv("BATCH_FALLBACK_WORKERS", "4"))
-            self.regime_symbols_csv = os.getenv("REGIME_SYMBOLS_CSV", "SPY")
+            self.ALPACA_API_KEY = os.getenv('ALPACA_API_KEY', 'FAKE_TEST_API_KEY_NOT_REAL_123456789')
+            self.ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY', 'FAKE_TEST_SECRET_KEY_NOT_REAL_123456789')
+            self.ALPACA_BASE_URL = os.getenv('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
+            self.ALPACA_DATA_FEED = os.getenv('ALPACA_DATA_FEED', 'iex')
+            self.FINNHUB_API_KEY = os.getenv('FINNHUB_API_KEY', None)
+            self.FUNDAMENTAL_API_KEY = os.getenv('FUNDAMENTAL_API_KEY', None)
+            self.NEWS_API_KEY = os.getenv('NEWS_API_KEY', None)
+            self.IEX_API_TOKEN = os.getenv('IEX_API_TOKEN', None)
+            self.WEBHOOK_SECRET = os.getenv('WEBHOOK_SECRET', 'fake-test-webhook-not-real')
+            self.FLASK_PORT = int(os.getenv('FLASK_PORT', '9000'))
+            self.BOT_MODE = os.getenv('BOT_MODE', 'balanced')
+            self.MODEL_PATH = os.getenv('MODEL_PATH', 'trained_model.pkl')
+            self.HALT_FLAG_PATH = os.getenv('HALT_FLAG_PATH', 'halt.flag')
+            self.MAX_PORTFOLIO_POSITIONS = int(os.getenv('MAX_PORTFOLIO_POSITIONS', '20'))
+            self.LIMIT_ORDER_SLIPPAGE = float(os.getenv('LIMIT_ORDER_SLIPPAGE', '0.005'))
+            self.HEALTHCHECK_PORT = int(os.getenv('HEALTHCHECK_PORT', '8081'))
+            self.RUN_HEALTHCHECK = os.getenv('RUN_HEALTHCHECK', '0')
+            self.BUY_THRESHOLD = float(os.getenv('BUY_THRESHOLD', '0.5'))
+            self.WEBHOOK_PORT = int(os.getenv('WEBHOOK_PORT', '9000'))
+            self.SLIPPAGE_THRESHOLD = float(os.getenv('SLIPPAGE_THRESHOLD', '0.003'))
+            self.REBALANCE_INTERVAL_MIN = int(os.getenv('REBALANCE_INTERVAL_MIN', '1440'))
+            self.SHADOW_MODE = os.getenv('SHADOW_MODE', 'False').lower() == 'true'
+            self.DRY_RUN = os.getenv('DRY_RUN', 'False').lower() == 'true'
+            self.DISABLE_DAILY_RETRAIN = os.getenv('DISABLE_DAILY_RETRAIN', 'False').lower() == 'true'
+            self.TRADE_LOG_FILE = os.getenv('TRADE_LOG_FILE', 'data/trades.csv')
+            self.FORCE_TRADES = os.getenv('FORCE_TRADES', 'False').lower() == 'true'
+            self.DISASTER_DD_LIMIT = float(os.getenv('DISASTER_DD_LIMIT', '0.2'))
+            self.MODEL_RF_PATH = os.getenv('MODEL_RF_PATH', 'model_rf.pkl')
+            self.MODEL_XGB_PATH = os.getenv('MODEL_XGB_PATH', 'model_xgb.pkl')
+            self.MODEL_LGB_PATH = os.getenv('MODEL_LGB_PATH', 'model_lgb.pkl')
+            self.RL_MODEL_PATH = os.getenv('RL_MODEL_PATH', 'rl_agent.zip')
+            self.USE_RL_AGENT = os.getenv('USE_RL_AGENT', 'False').lower() == 'true'
+            self.SECTOR_EXPOSURE_CAP = float(os.getenv('SECTOR_EXPOSURE_CAP', '0.4'))
+            self.MAX_OPEN_POSITIONS = int(os.getenv('MAX_OPEN_POSITIONS', '10'))
+            self.WEEKLY_DRAWDOWN_LIMIT = float(os.getenv('WEEKLY_DRAWDOWN_LIMIT', '0.15'))
+            self.VOLUME_THRESHOLD = int(os.getenv('VOLUME_THRESHOLD', '50000'))
+            self.DOLLAR_RISK_LIMIT = float(os.getenv('DOLLAR_RISK_LIMIT', '0.05'))
+            self.FINNHUB_RPM = int(os.getenv('FINNHUB_RPM', '60'))
+            self.MINUTE_CACHE_TTL = int(os.getenv('MINUTE_CACHE_TTL', '60'))
+            self.EQUITY_EXPOSURE_CAP = float(os.getenv('EQUITY_EXPOSURE_CAP', '2.5'))
+            self.PORTFOLIO_EXPOSURE_CAP = float(os.getenv('PORTFOLIO_EXPOSURE_CAP', '2.5'))
+            self.SEED = int(os.getenv('SEED', '42'))
+            self.RATE_LIMIT_BUDGET = int(os.getenv('RATE_LIMIT_BUDGET', '190'))
+            self.pretrade_lookback_days = int(os.getenv('PRETRADE_LOOKBACK_DAYS', '120'))
+            self.pretrade_batch_size = int(os.getenv('PRETRADE_BATCH_SIZE', '50'))
+            self.intraday_batch_enable = os.getenv('INTRADAY_BATCH_ENABLE', 'True').lower() == 'true'
+            self.intraday_batch_size = int(os.getenv('INTRADAY_BATCH_SIZE', '40'))
+            self.batch_fallback_workers = int(os.getenv('BATCH_FALLBACK_WORKERS', '4'))
+            self.regime_symbols_csv = os.getenv('REGIME_SYMBOLS_CSV', 'SPY')
             for k, v in kwargs.items():
                 setattr(self, k, v)
-                
+
         @staticmethod
         def model_json_schema():
             return {}
-        
+
         def effective_executor_workers(self, cpu_count=None):
             """Return a reasonable number of workers."""
             cpu_count = cpu_count or 2
             return max(2, min(4, cpu_count))
-        
+
         def effective_prediction_workers(self, cpu_count=None):
             """Return a reasonable number of prediction workers."""
             cpu_count = cpu_count or 2
             return max(2, min(4, cpu_count))
-        
+
         def get_alpaca_keys(self):
             """Return Alpaca API credentials."""
-            return self.ALPACA_API_KEY, self.ALPACA_SECRET_KEY
-    
+            return (self.ALPACA_API_KEY, self.ALPACA_SECRET_KEY)
+
     class SettingsConfigDictStub:
+
         def __init__(self, **kwargs):
             pass
-    
-    pydantic_settings_mod = types.ModuleType("pydantic_settings")
+    pydantic_settings_mod = types.ModuleType('pydantic_settings')
     pydantic_settings_mod.BaseSettings = BaseSettingsStub
     pydantic_settings_mod.SettingsConfigDict = SettingsConfigDictStub
-    
-    # Create a get_settings function that returns a properly configured instance
     _settings_instance = None
+
     def get_settings():
         global _settings_instance
         if _settings_instance is None:
             _settings_instance = BaseSettingsStub()
         return _settings_instance
-    
     pydantic_settings_mod.get_settings = get_settings
-    pydantic_settings_mod.__file__ = "stub"
-    sys.modules["pydantic_settings"] = pydantic_settings_mod
-
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+    pydantic_settings_mod.__file__ = 'stub'
+    sys.modules['pydantic_settings'] = pydantic_settings_mod
+try:
+    pass
+except Exception:
+    import types
+
     class FieldStub:
+
         def __init__(self, *args, **kwargs):
             self.default = args[0] if args else None
             self.kwargs = kwargs
-            
+
         def __call__(self, *args, **kwargs):
-            # For Field decorators, just return the default value
             return self.default
-    
+
     def model_validator(*args, **kwargs):
         """Stub for pydantic model_validator decorator."""
+
         def decorator(func):
             return func
         return decorator
-    
+
     class AliasChoices:
-        def __init__(self, *args, **kwargs):
-            pass
-    
-    pydantic_mod = types.ModuleType("pydantic")
+
+        def __init__(self, *args, **kwargs):
+            pass
+    pydantic_mod = types.ModuleType('pydantic')
     pydantic_mod.Field = FieldStub()
     pydantic_mod.model_validator = model_validator
     pydantic_mod.AliasChoices = AliasChoices
-    pydantic_mod.__file__ = "stub"
-    sys.modules["pydantic"] = pydantic_mod
-
-# AI-AGENT-REF: Add alpaca_trade_api stubs
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
-    alpaca_mod = types.ModuleType("alpaca_trade_api")
-    rest_mod = types.ModuleType("alpaca_trade_api.rest")
-    
+    pydantic_mod.__file__ = 'stub'
+    sys.modules['pydantic'] = pydantic_mod
+try:
+    pass
+except Exception:
+    import types
+    alpaca_mod = types.ModuleType('alpaca_trade_api')
+    rest_mod = types.ModuleType('alpaca_trade_api.rest')
+
     class RESTStub:
-        def __init__(self, *args, **kwargs):
-            pass
-            
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class APIError(Exception):
         pass
-
     rest_mod.REST = RESTStub
     rest_mod.APIError = APIError
     alpaca_mod.rest = rest_mod
-    alpaca_mod.__file__ = "stub"
-    sys.modules["alpaca_trade_api"] = alpaca_mod
-    sys.modules["alpaca_trade_api.rest"] = rest_mod
-
-# AI-AGENT-REF: Add alpaca-py SDK stubs for newer API
+    alpaca_mod.__file__ = 'stub'
+    sys.modules['alpaca_trade_api'] = alpaca_mod
+    sys.modules['alpaca_trade_api.rest'] = rest_mod
 try:
     from alpaca.common.exceptions import APIError
-except Exception:  # pragma: no cover - optional dependency
+except Exception:
     import types
     from enum import Enum
-    
-    # Common module
-    common_mod = types.ModuleType("alpaca.common")
-    exceptions_mod = types.ModuleType("alpaca.common.exceptions")
-    
+    common_mod = types.ModuleType('alpaca.common')
+    exceptions_mod = types.ModuleType('alpaca.common.exceptions')
+
     class APIError(Exception):
         pass
-    
     exceptions_mod.APIError = APIError
     common_mod.exceptions = exceptions_mod
-    
-    # Data module  
-    data_mod = types.ModuleType("alpaca.data")
-    models_mod = types.ModuleType("alpaca.data.models")
-    requests_mod = types.ModuleType("alpaca.data.requests")
-    historical_mod = types.ModuleType("alpaca.data.historical")
-    timeframe_mod = types.ModuleType("alpaca.data.timeframe")
-    
+    data_mod = types.ModuleType('alpaca.data')
+    models_mod = types.ModuleType('alpaca.data.models')
+    requests_mod = types.ModuleType('alpaca.data.requests')
+    historical_mod = types.ModuleType('alpaca.data.historical')
+    timeframe_mod = types.ModuleType('alpaca.data.timeframe')
+
     class Quote:
         bid_price = 0
         ask_price = 0
-    
+
     class StockLatestQuoteRequest:
+
         def __init__(self, symbol_or_symbols):
             self.symbols = symbol_or_symbols
-    
+
     class StockBarsRequest:
-        def __init__(self, *args, **kwargs):
-            pass
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class StockHistoricalDataClient:
-        def __init__(self, *args, **kwargs):
-            pass
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class TimeFrame:
-        DAY = "day"
-        Day = "day"  # Add this for compatibility
-        HOUR = "hour"
-        MINUTE = "minute"
-
+        DAY = 'day'
+        Day = 'day'
+        HOUR = 'hour'
+        MINUTE = 'minute'
     models_mod.Quote = Quote
     requests_mod.StockLatestQuoteRequest = StockLatestQuoteRequest
     requests_mod.StockBarsRequest = StockBarsRequest
@@ -1100,95 +1020,83 @@     data_mod.requests = requests_mod
     data_mod.historical = historical_mod
     data_mod.timeframe = timeframe_mod
-    
-    # Trading module
-    trading_mod = types.ModuleType("alpaca.trading")
-    client_mod = types.ModuleType("alpaca.trading.client")
-    enums_mod = types.ModuleType("alpaca.trading.enums")
-    trading_models_mod = types.ModuleType("alpaca.trading.models")
-    trading_requests_mod = types.ModuleType("alpaca.trading.requests")
-    trading_stream_mod = types.ModuleType("alpaca.trading.stream")
-    
+    trading_mod = types.ModuleType('alpaca.trading')
+    client_mod = types.ModuleType('alpaca.trading.client')
+    enums_mod = types.ModuleType('alpaca.trading.enums')
+    trading_models_mod = types.ModuleType('alpaca.trading.models')
+    trading_requests_mod = types.ModuleType('alpaca.trading.requests')
+    trading_stream_mod = types.ModuleType('alpaca.trading.stream')
+
     class TradingClient:
-        def __init__(self, *args, **kwargs):
-            pass
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
+
     class OrderSide:
-        BUY = "buy"
-        SELL = "sell"
-    
+        BUY = 'buy'
+        SELL = 'sell'
+
     class TimeInForce:
-        DAY = "day"
-    
+        DAY = 'day'
+
     class AlpacaOrderClass(str, Enum):
-        SIMPLE = "simple"
-        MLEG = "mleg"
-        BRACKET = "bracket"
-        OCO = "oco"
-        OTO = "oto"
-    
+        SIMPLE = 'simple'
+        MLEG = 'mleg'
+        BRACKET = 'bracket'
+        OCO = 'oco'
+        OTO = 'oto'
+
     class QueryOrderStatus(str, Enum):
-        OPEN = "open"
-        CLOSED = "closed"
-        ALL = "all"
-    
+        OPEN = 'open'
+        CLOSED = 'closed'
+        ALL = 'all'
+
     class OrderStatus(str, Enum):
-        NEW = "new"
-        PARTIALLY_FILLED = "partially_filled"
-        FILLED = "filled"
-        DONE_FOR_DAY = "done_for_day"
-        CANCELED = "canceled"
-        EXPIRED = "expired"
-        REPLACED = "replaced"
-        PENDING_CANCEL = "pending_cancel"
-        PENDING_REPLACE = "pending_replace"
-        PENDING_REVIEW = "pending_review"
-        ACCEPTED = "accepted"
-        PENDING_NEW = "pending_new"
-        ACCEPTED_FOR_BIDDING = "accepted_for_bidding"
-        STOPPED = "stopped"
-        REJECTED = "rejected"
-        SUSPENDED = "suspended"
-        CALCULATED = "calculated"
-    
+        NEW = 'new'
+        PARTIALLY_FILLED = 'partially_filled'
+        FILLED = 'filled'
+        DONE_FOR_DAY = 'done_for_day'
+        CANCELED = 'canceled'
+        EXPIRED = 'expired'
+        REPLACED = 'replaced'
+        PENDING_CANCEL = 'pending_cancel'
+        PENDING_REPLACE = 'pending_replace'
+        PENDING_REVIEW = 'pending_review'
+        ACCEPTED = 'accepted'
+        PENDING_NEW = 'pending_new'
+        ACCEPTED_FOR_BIDDING = 'accepted_for_bidding'
+        STOPPED = 'stopped'
+        REJECTED = 'rejected'
+        SUSPENDED = 'suspended'
+        CALCULATED = 'calculated'
+
     class Order(dict):
         pass
-    
+
     class MarketOrderRequest(dict):
+
         def __init__(self, symbol, qty, side, time_in_force):
-            super().__init__(
-                symbol=symbol,
-                qty=qty,
-                side=side,
-                time_in_force=time_in_force,
-            )
+            super().__init__(symbol=symbol, qty=qty, side=side, time_in_force=time_in_force)
 
     class LimitOrderRequest(dict):
+
         def __init__(self, symbol, qty, side, time_in_force, limit_price):
-            super().__init__(
-                symbol=symbol,
-                qty=qty,
-                side=side,
-                time_in_force=time_in_force,
-                limit_price=limit_price,
-            )
-    
+            super().__init__(symbol=symbol, qty=qty, side=side, time_in_force=time_in_force, limit_price=limit_price)
+
     class GetOrdersRequest(dict):
+
         def __init__(self, **kwargs):
             super().__init__(**kwargs)
-    
+
     class GetAssetsRequest(dict):
+
         def __init__(self, **kwargs):
             super().__init__(**kwargs)
-    
+
     class TradingStream:
-        def __init__(self, *args, **kwargs):
-            pass
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
+
+        def __init__(self, *args, **kwargs):
+            pass
     client_mod.TradingClient = TradingClient
     enums_mod.OrderSide = OrderSide
     enums_mod.TimeInForce = TimeInForce
@@ -1206,189 +1114,181 @@     trading_mod.models = trading_models_mod
     trading_mod.requests = trading_requests_mod
     trading_mod.stream = trading_stream_mod
-    
-    # Main alpaca module
-    alpaca_main_mod = types.ModuleType("alpaca")
+    alpaca_main_mod = types.ModuleType('alpaca')
     alpaca_main_mod.common = common_mod
     alpaca_main_mod.data = data_mod
     alpaca_main_mod.trading = trading_mod
-    
-    sys.modules["alpaca"] = alpaca_main_mod
-    sys.modules["alpaca.common"] = common_mod
-    sys.modules["alpaca.common.exceptions"] = exceptions_mod
-    sys.modules["alpaca.data"] = data_mod
-    sys.modules["alpaca.data.models"] = models_mod
-    sys.modules["alpaca.data.requests"] = requests_mod
-    sys.modules["alpaca.data.historical"] = historical_mod
-    sys.modules["alpaca.data.timeframe"] = timeframe_mod
-    sys.modules["alpaca.trading"] = trading_mod
-    sys.modules["alpaca.trading.client"] = client_mod
-    sys.modules["alpaca.trading.enums"] = enums_mod
-    sys.modules["alpaca.trading.models"] = trading_models_mod
-    sys.modules["alpaca.trading.requests"] = trading_requests_mod
-    sys.modules["alpaca.trading.stream"] = trading_stream_mod
-
-# AI-AGENT-REF: Add other missing dependencies
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    psutil_mod = types.ModuleType("psutil")
-    psutil_mod.__file__ = "stub"
-    sys.modules["psutil"] = psutil_mod
-
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    tzlocal_mod = types.ModuleType("tzlocal")
+    sys.modules['alpaca'] = alpaca_main_mod
+    sys.modules['alpaca.common'] = common_mod
+    sys.modules['alpaca.common.exceptions'] = exceptions_mod
+    sys.modules['alpaca.data'] = data_mod
+    sys.modules['alpaca.data.models'] = models_mod
+    sys.modules['alpaca.data.requests'] = requests_mod
+    sys.modules['alpaca.data.historical'] = historical_mod
+    sys.modules['alpaca.data.timeframe'] = timeframe_mod
+    sys.modules['alpaca.trading'] = trading_mod
+    sys.modules['alpaca.trading.client'] = client_mod
+    sys.modules['alpaca.trading.enums'] = enums_mod
+    sys.modules['alpaca.trading.models'] = trading_models_mod
+    sys.modules['alpaca.trading.requests'] = trading_requests_mod
+    sys.modules['alpaca.trading.stream'] = trading_stream_mod
+try:
+    pass
+except Exception:
+    import types
+    psutil_mod = types.ModuleType('psutil')
+    psutil_mod.__file__ = 'stub'
+    sys.modules['psutil'] = psutil_mod
+try:
+    pass
+except Exception:
+    import types
+    tzlocal_mod = types.ModuleType('tzlocal')
     tzlocal_mod.get_localzone = lambda: None
-    tzlocal_mod.__file__ = "stub"
-    sys.modules["tzlocal"] = tzlocal_mod
-
-# AI-AGENT-REF: Add BeautifulSoup stub
+    tzlocal_mod.__file__ = 'stub'
+    sys.modules['tzlocal'] = tzlocal_mod
 try:
     from bs4 import BeautifulSoup
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+except Exception:
+    import types
+
     class BeautifulSoup:
-        def __init__(self, *args, **kwargs):
-            pass
-        
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def find(self, *args, **kwargs):
             return None
-        
+
         def find_all(self, *args, **kwargs):
             return []
-        
+
         def get_text(self, *args, **kwargs):
-            return ""
-    
-    bs4_mod = types.ModuleType("bs4")
+            return ''
+    bs4_mod = types.ModuleType('bs4')
     bs4_mod.BeautifulSoup = BeautifulSoup
-    bs4_mod.__file__ = "stub"
-    sys.modules["bs4"] = bs4_mod
-
-# AI-AGENT-REF: Add Flask stub
+    bs4_mod.__file__ = 'stub'
+    sys.modules['bs4'] = bs4_mod
 try:
     from flask import Flask
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+except Exception:
+    import types
+
     class Flask:
-        def __init__(self, *args, **kwargs):
-            pass
-        
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def route(self, *args, **kwargs):
+
             def decorator(f):
                 return f
             return decorator
-        
+
         def run(self, *args, **kwargs):
             pass
-        
-        def __getattr__(self, name):
-            return lambda *args, **kwargs: None
-    
-    flask_mod = types.ModuleType("flask")
+    flask_mod = types.ModuleType('flask')
     flask_mod.Flask = Flask
     flask_mod.request = types.SimpleNamespace()
     flask_mod.jsonify = lambda x: x
-    flask_mod.__file__ = "stub"
-    sys.modules["flask"] = flask_mod
-
-# AI-AGENT-REF: Add ratelimit stub
+    flask_mod.__file__ = 'stub'
+    sys.modules['flask'] = flask_mod
 try:
     from ratelimit import limits, sleep_and_retry
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+except Exception:
+    import types
+
     def limits(*args, **kwargs):
+
         def decorator(f):
             return f
         return decorator
-    
+
     def sleep_and_retry(f):
         return f
-    
-    ratelimit_mod = types.ModuleType("ratelimit")
+    ratelimit_mod = types.ModuleType('ratelimit')
     ratelimit_mod.limits = limits
     ratelimit_mod.sleep_and_retry = sleep_and_retry
-    ratelimit_mod.__file__ = "stub"
-    sys.modules["ratelimit"] = ratelimit_mod
-
-# AI-AGENT-REF: Add pybreaker stub
-try:
-    pass
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+    ratelimit_mod.__file__ = 'stub'
+    sys.modules['ratelimit'] = ratelimit_mod
+try:
+    pass
+except Exception:
+    import types
+
     class CircuitBreaker:
-        def __init__(self, *args, **kwargs):
-            pass
-        
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def __call__(self, func):
             return func
-        
+
         def __enter__(self):
             return self
-        
+
         def __exit__(self, *args):
             pass
-    
-    pybreaker_mod = types.ModuleType("pybreaker")
+    pybreaker_mod = types.ModuleType('pybreaker')
     pybreaker_mod.CircuitBreaker = CircuitBreaker
-    pybreaker_mod.__file__ = "stub"
-    sys.modules["pybreaker"] = pybreaker_mod
-
-# AI-AGENT-REF: Add prometheus_client stub
+    pybreaker_mod.__file__ = 'stub'
+    sys.modules['pybreaker'] = pybreaker_mod
 try:
     from prometheus_client import Counter, Gauge, Histogram, start_http_server
-except Exception:  # pragma: no cover - optional dependency
-    import types
-    
+except Exception:
+    import types
+
     class Counter:
-        def __init__(self, *args, **kwargs):
-            pass
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def inc(self, *args, **kwargs):
             pass
+
         def labels(self, *args, **kwargs):
             return self
-    
+
     class Gauge:
-        def __init__(self, *args, **kwargs):
-            pass
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def set(self, *args, **kwargs):
             pass
+
         def labels(self, *args, **kwargs):
             return self
-    
+
     class Histogram:
-        def __init__(self, *args, **kwargs):
-            pass
+
+        def __init__(self, *args, **kwargs):
+            pass
+
         def observe(self, *args, **kwargs):
             pass
+
         def time(self):
             return self
+
         def __enter__(self):
             return self
+
         def __exit__(self, *args):
             pass
+
         def labels(self, *args, **kwargs):
             return self
-    
+
     def start_http_server(*args, **kwargs):
         pass
-    
-    prometheus_mod = types.ModuleType("prometheus_client")
+    prometheus_mod = types.ModuleType('prometheus_client')
     prometheus_mod.Counter = Counter
     prometheus_mod.Gauge = Gauge
     prometheus_mod.Histogram = Histogram
     prometheus_mod.start_http_server = start_http_server
-    prometheus_mod.__file__ = "stub"
-    sys.modules["prometheus_client"] = prometheus_mod
-
+    prometheus_mod.__file__ = 'stub'
+    sys.modules['prometheus_client'] = prometheus_mod
 
 def pytest_configure() -> None:
     """Load environment variables for tests."""
@@ -1397,37 +1297,26 @@         env_file = Path('.env')
     if env_file.exists():
         load_dotenv(env_file)
-    # Ensure project root is on the import path so modules like
-    # ``ai_trading.capital_scaling`` resolve when tests are run from the ``tests``
-    # directory by CI tools or developers.
     root_dir = Path(__file__).resolve().parent.parent
     if str(root_dir) not in sys.path:
         sys.path.insert(0, str(root_dir))
 
-
 @pytest.fixture(autouse=True)
 def default_env(monkeypatch):
     """Provide standard environment variables for tests."""
-    monkeypatch.setenv("ALPACA_API_KEY", "FAKE_TEST_API_KEY_NOT_REAL_123456789")  # Valid format
-    monkeypatch.setenv("ALPACA_SECRET_KEY", "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789")  # Valid format
-    monkeypatch.setenv("ALPACA_BASE_URL", "https://paper-api.alpaca.markets")
-    monkeypatch.setenv("WEBHOOK_SECRET", "fake-test-webhook-not-real")
-    monkeypatch.setenv("FLASK_PORT", "9000")
-    monkeypatch.setenv("TESTING", "1")
+    monkeypatch.setenv('ALPACA_API_KEY', 'FAKE_TEST_API_KEY_NOT_REAL_123456789')
+    monkeypatch.setenv('ALPACA_SECRET_KEY', 'FAKE_TEST_SECRET_KEY_NOT_REAL_123456789')
+    monkeypatch.setenv('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
+    monkeypatch.setenv('WEBHOOK_SECRET', 'fake-test-webhook-not-real')
+    monkeypatch.setenv('FLASK_PORT', '9000')
+    monkeypatch.setenv('TESTING', '1')
     yield
-
-
-
-
-
 import importlib
 import types
-
 
 def reload_module(mod):
     """Reload a module within tests."""
     return importlib.reload(mod)
-
 
 @pytest.fixture(autouse=True)
 def reload_utils_module():
@@ -1436,177 +1325,117 @@     importlib.reload(utils)
     yield
 
-
-# AI-AGENT-REF: stub capital scaling helpers for unit tests
 @pytest.fixture(autouse=True)
 def stub_capital_scaling(monkeypatch):
     """Provide simple stubs for heavy capital scaling functions."""
-    
-    # Add TradingConfig stub to config module
     try:
         import config
         if not hasattr(config, 'TradingConfig'):
-            class MockTradingConfig:
-                # Risk Management Parameters
-                max_drawdown_threshold = 0.15
-                daily_loss_limit = 0.03
-                dollar_risk_limit = 0.05
-                max_portfolio_risk = 0.025
-                max_correlation_exposure = 0.15
-                max_sector_concentration = 0.15
-                min_liquidity_threshold = 1000000
-                position_size_min_usd = 100.0
-                max_position_size = 8000
-                max_position_size_pct = 0.25
-                
-                # Kelly Criterion Parameters
-                kelly_fraction = 0.6
-                kelly_fraction_max = 0.25
-                min_sample_size = 20
-                confidence_level = 0.90
-                lookback_periods = 252
-                rebalance_frequency = 21
-                
-                @classmethod
-                def from_env(cls, mode="balanced"):
-                    return cls()
-            
-            # Set the attribute on the config module instance, not the class
             if hasattr(config, '__dict__'):
                 config.TradingConfig = MockTradingConfig
             else:
-                # If config is an instance, set it as an attribute 
                 setattr(config, 'TradingConfig', MockTradingConfig)
     except ImportError as e:
-        # AI-AGENT-REF: Log config import failure for debugging 
         import logging
         logger = logging.getLogger(__name__)
-        logger.debug(f"Could not import config module for mocking: {e}")
+        logger.debug(f'Could not import config module for mocking: {e}')
         pass
-    
     try:
         import ai_trading.capital_scaling as cs
-        # Only set attributes if they exist
-        if hasattr(cs, "drawdown_adjusted_kelly"):
-            monkeypatch.setattr(cs, "drawdown_adjusted_kelly", lambda *a, **k: 0.02)
-        if hasattr(cs, "volatility_parity_position"):
-            monkeypatch.setattr(cs, "volatility_parity_position", lambda *a, **k: 0.01)
+        if hasattr(cs, 'drawdown_adjusted_kelly'):
+            monkeypatch.setattr(cs, 'drawdown_adjusted_kelly', lambda *a, **k: 0.02)
+        if hasattr(cs, 'volatility_parity_position'):
+            monkeypatch.setattr(cs, 'volatility_parity_position', lambda *a, **k: 0.01)
     except ImportError as e:
-        # AI-AGENT-REF: Log capital scaling import failure for debugging
         import logging
         logger = logging.getLogger(__name__)
-        logger.debug(f"Could not import capital_scaling module for mocking: {e}")
+        logger.debug(f'Could not import capital_scaling module for mocking: {e}')
         pass
-    
-    # Add missing bot_engine functions
     try:
         from ai_trading.core import bot_engine
-        # Add the missing function directly to the module
         bot_engine.check_alpaca_available = lambda x: True
     except ImportError as e:
-        # AI-AGENT-REF: Log bot_engine import failure for debugging
         import logging
         logger = logging.getLogger(__name__)
-        logger.debug(f"Could not import bot_engine for mocking: {e}")
+        logger.debug(f'Could not import bot_engine for mocking: {e}')
         pass
     except Exception:
-        # If bot_engine import fails due to config issues, skip it for now
         pass
-    
-    # Add missing trade_execution attributes
     try:
         import trade_execution
         if not hasattr(trade_execution, 'ExecutionEngine'):
-            class MockExecutionEngine:
-                def __init__(self, ctx):
-                    self.ctx = ctx
-                def execute_order(self, *args, **kwargs):
-                    return "ok"
-                def _execute_sliced(self, *args, **kwargs):
-                    return "ok"
             trade_execution.ExecutionEngine = MockExecutionEngine
     except ImportError:
         pass
-        
     yield
-
 
 def load_runner(monkeypatch):
     """Import and reload the runner module with a dummy bot."""
-    bot_mod = types.ModuleType("bot")
+    bot_mod = types.ModuleType('bot')
     bot_mod.main = lambda: None
-    monkeypatch.setitem(sys.modules, "bot", bot_mod)
-    req_mod = types.ModuleType("requests")
+    monkeypatch.setitem(sys.modules, 'bot', bot_mod)
+    req_mod = types.ModuleType('requests')
     req_mod.get = lambda *a, **k: None
-    exc_mod = types.ModuleType("requests.exceptions")
+    exc_mod = types.ModuleType('requests.exceptions')
     exc_mod.RequestException = Exception
     req_mod.exceptions = exc_mod
-    monkeypatch.setitem(sys.modules, "requests.exceptions", exc_mod)
-    monkeypatch.setitem(sys.modules, "requests", req_mod)
-    alpaca_mod = types.ModuleType("alpaca")
-    trading_mod = types.ModuleType("alpaca.trading")
+    monkeypatch.setitem(sys.modules, 'requests.exceptions', exc_mod)
+    monkeypatch.setitem(sys.modules, 'requests', req_mod)
+    alpaca_mod = types.ModuleType('alpaca')
+    trading_mod = types.ModuleType('alpaca.trading')
     trading_mod.__path__ = []
-    stream_mod = types.ModuleType("alpaca.trading.stream")
+    stream_mod = types.ModuleType('alpaca.trading.stream')
     stream_mod.TradingStream = object
-    monkeypatch.setitem(sys.modules, "alpaca", alpaca_mod)
-    monkeypatch.setitem(sys.modules, "alpaca.trading", trading_mod)
-    monkeypatch.setitem(sys.modules, "alpaca.trading.stream", stream_mod)
+    monkeypatch.setitem(sys.modules, 'alpaca', alpaca_mod)
+    monkeypatch.setitem(sys.modules, 'alpaca.trading', trading_mod)
+    monkeypatch.setitem(sys.modules, 'alpaca.trading.stream', stream_mod)
     import runner as r
     return importlib.reload(r)
 
-
 @pytest.fixture
 def dummy_alpaca_client():
+
     class DummyClient:
+
         def __init__(self):
             self.calls = []
+
         def submit_order(self, *args, **kwargs):
-            # Accept any combination of positional and keyword arguments
-            self.calls.append({"args": args, "kwargs": kwargs})
+            self.calls.append({'args': args, 'kwargs': kwargs})
             from types import SimpleNamespace
-            return SimpleNamespace(id="dummy-order-id", status="accepted")
+            return SimpleNamespace(id='dummy-order-id', status='accepted')
     return DummyClient()
 
-
-def _make_df(rows: int = 10):
+def _make_df(rows: int=10):
     from datetime import datetime, timezone
     now = datetime(2025, 8, 8, 15, 30, tzinfo=timezone.utc)
     try:
         import pandas as pd
-        idx = pd.date_range(end=now, periods=max(rows, 1), freq="min")
-        return pd.DataFrame(
-            {"open": 100.0, "high": 101.0, "low": 99.5, "close": 100.5, "volume": 1000},
-            index=idx
-        )
+        idx = pd.date_range(end=now, periods=max(rows, 1), freq='min')
+        return pd.DataFrame({'open': 100.0, 'high': 101.0, 'low': 99.5, 'close': 100.5, 'volume': 1000}, index=idx)
     except ImportError:
-        # Use mock DataFrame if pandas not available
         from tests.conftest import DataFrameStub
-        return DataFrameStub({
-            "timestamp": [now] * max(rows, 1),
-            "open": [100.0] * max(rows, 1),
-            "high": [101.0] * max(rows, 1),
-            "low": [99.5] * max(rows, 1),
-            "close": [100.5] * max(rows, 1),
-            "volume": [1000] * max(rows, 1)
-        })
-
+        return DataFrameStub({'timestamp': [now] * max(rows, 1), 'open': [100.0] * max(rows, 1), 'high': [101.0] * max(rows, 1), 'low': [99.5] * max(rows, 1), 'close': [100.5] * max(rows, 1), 'volume': [1000] * max(rows, 1)})
 
 @pytest.fixture
 def dummy_data_fetcher():
+
     class DF:
+
         def get_minute_bars(self, symbol, start=None, end=None, limit=None):
             return _make_df(30)
     return DF()
 
-
 @pytest.fixture
 def dummy_data_fetcher_empty():
+
     class DF:
+
         def get_minute_bars(self, symbol, start=None, end=None, limit=None):
             try:
                 import pandas as pd
-                return pd.DataFrame(columns=["open","high","low","close","volume"])
+                return pd.DataFrame(columns=['open', 'high', 'low', 'close', 'volume'])
             except ImportError:
                 from tests.conftest import DataFrameStub
-                return DataFrameStub({})  # Empty DataFrame
-    return DF()
+                return DataFrameStub({})
+    return DF()--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/mocks.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/mocks.py@@ -4,108 +4,6 @@ This module contains all mock classes previously embedded in production code,
 now properly isolated for testing purposes only.
 """
-
-
-class MockTradingClient:
-    """Mock Alpaca TradingClient for testing."""
-    
-    def __init__(self, *args, **kwargs): 
-        pass
-        
-    def get_account(self): 
-        return type('Account', (), {'equity': '100000'})()
-        
-    def submit_order(self, *args, **kwargs): 
-        return {'status': 'filled'}
-
-
-class MockMarketOrderRequest:
-    """Mock MarketOrderRequest for testing."""
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockLimitOrderRequest:
-    """Mock LimitOrderRequest for testing."""
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockGetOrdersRequest:
-    """Mock GetOrdersRequest for testing."""
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockOrderSide:
-    """Mock OrderSide enum for testing."""
-    
-    BUY = 'buy'
-    SELL = 'sell'
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockTimeInForce:
-    """Mock TimeInForce enum for testing."""
-    
-    DAY = 'day'
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockOrderStatus:
-    """Mock OrderStatus enum for testing."""
-    
-    FILLED = 'filled'
-    OPEN = 'open'
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockQueryOrderStatus:
-    """Mock QueryOrderStatus enum for testing."""
-    
-    FILLED = 'filled'
-    OPEN = 'open'
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockOrder:
-    """Mock Order model for testing."""
-    
-    def __init__(self, *args, **kwargs):
-        pass
-
-
-class MockTradingStream:
-    """Mock TradingStream for testing."""
-    
-    def __init__(self, *args, **kwargs):
-        pass
-        
-    def subscribe_trades(self, *args, **kwargs):
-        pass
-        
-    def subscribe_quotes(self, *args, **kwargs):
-        pass
-        
-    def subscribe_trade_updates(self, *args, **kwargs):
-        pass
-        
-    def run(self, *args, **kwargs):
-        pass
-
-
-# Mock instances for attribute access
 mock_order_side = MockOrderSide()
 mock_time_in_force = MockTimeInForce()
 mock_order_status = MockOrderStatus()
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_additional_coverage.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_additional_coverage.py@@ -5,11 +5,9 @@ import sys
 import types
 from datetime import datetime
-
 import numpy as np
 import pandas as pd
 import pytest
-
 import config
 from ai_trading import meta_learning
 import ml_model
@@ -18,41 +16,45 @@ from ai_trading import utils
 from strategies.mean_reversion import MeanReversionStrategy
 
-
-
-
 def test_config_missing_vars(monkeypatch):
     """_require_env_vars raises when variables missing."""
     with pytest.raises(RuntimeError):
-        config._require_env_vars("MISSING_VAR")
-
+        config._require_env_vars('MISSING_VAR')
 
 def test_get_env_reload(monkeypatch):
     """get_env reloads environment when requested."""
     called = []
-    monkeypatch.setattr(config, "reload_env", lambda: called.append(True))
-    with pytest.raises(RuntimeError):
-        config.get_env("MISSING", required=True, reload=True)
+    monkeypatch.setattr(config, 'reload_env', lambda: called.append(True))
+    with pytest.raises(RuntimeError):
+        config.get_env('MISSING', required=True, reload=True)
     assert called == [True]
-
 
 def test_create_flask_routes():
     """Health endpoints respond correctly."""
-    flask_mod = types.ModuleType("flask")
+    flask_mod = types.ModuleType('flask')
+
     class DummyClient:
+
         def __init__(self, *a, **k):
             pass
+
         def get(self, *a, **k):
-            return types.SimpleNamespace(status_code=200, json=lambda: {"status": "ok"})
+            return types.SimpleNamespace(status_code=200, json=lambda: {'status': 'ok'})
+
     class Flask:
+
         def __init__(self, *a, **k):
             pass
+
         def route(self, *a, **k):
+
             def deco(f):
                 return f
             return deco
+
         def run(self, *a, **k):
             pass
+
         def test_client(self):
             return DummyClient()
     flask_mod.Flask = Flask
@@ -62,231 +64,201 @@     sys.modules['flask.testing'] = types.ModuleType('flask.testing')
     sys.modules['flask.testing'].FlaskClient = DummyClient
     sys.modules.pop('ai_trading.main', None)
-    sys.modules.pop('ai_trading.app', None)  # Also remove app module
+    sys.modules.pop('ai_trading.app', None)
     main_mod = importlib.import_module('ai_trading.main')
     import ai_trading.app as app_mod
     app = app_mod.create_app()
     client = app.test_client()
-    assert client is not None, "test_client() returned None"
-    assert client.get("/health").json() == {"status": "ok"}
-    assert client.get("/healthz").status_code == 200
-
+    assert client is not None, 'test_client() returned None'
+    assert client.get('/health').json() == {'status': 'ok'}
+    assert client.get('/healthz').status_code == 200
 
 def test_main_starts_api_thread(monkeypatch):
     """main launches the API thread and runs a cycle."""
-    monkeypatch.setenv("SCHEDULER_ITERATIONS", "1")
-    # AI-AGENT-REF: Mock required environment variables for validation
-    monkeypatch.setenv("WEBHOOK_SECRET", "test_secret")
-    monkeypatch.setenv("ALPACA_API_KEY", "test_key")
-    # AI-AGENT-REF: Use environment variables to avoid hardcoded secrets
-    monkeypatch.setenv("TEST_ALPACA_SECRET_KEY", "test_secret_key")
-    
-    # AI-AGENT-REF: Mock the config object directly to ensure environment validation passes
-    class MockConfig:
-        WEBHOOK_SECRET = os.getenv("TEST_WEBHOOK_SECRET", "test_secret")
-        ALPACA_API_KEY = os.getenv("TEST_ALPACA_API_KEY", "test_key")
-        ALPACA_SECRET_KEY = os.getenv("TEST_ALPACA_SECRET_KEY", "test_secret_key")
-        NEWS_API_KEY = os.getenv("TEST_NEWS_API_KEY", "test_news_api_key")
-    
-    monkeypatch.setattr(main, "config", MockConfig())
-    
+    monkeypatch.setenv('SCHEDULER_ITERATIONS', '1')
+    monkeypatch.setenv('WEBHOOK_SECRET', 'test_secret')
+    monkeypatch.setenv('ALPACA_API_KEY', 'test_key')
+    monkeypatch.setenv('TEST_ALPACA_SECRET_KEY', 'test_secret_key')
+    monkeypatch.setattr(main, 'config', MockConfig())
     called = {}
 
     class DummyThread:
+
         def __init__(self, target, args=(), daemon=None):
-            called["created"] = True
+            called['created'] = True
             self.target = target
             self.args = args
 
         def start(self):
-            called["started"] = True
+            called['started'] = True
             self.target(*self.args)
 
         def is_alive(self):
-            # AI-AGENT-REF: Add missing is_alive method to prevent AttributeError
             return True
-
-    monkeypatch.setattr(main, "Thread", DummyThread)
-    # AI-AGENT-REF: Fix lambda signature to accept ready_signal parameter  
-    monkeypatch.setattr(main, "start_api", lambda ready_signal=None: called.setdefault("api", True))
-    monkeypatch.setattr(main, "run_cycle", lambda: called.setdefault("cycle", 0) or called.update(cycle=called.get("cycle", 0) + 1))
-    monkeypatch.setattr(main.time, "sleep", lambda s: None)
-
+    monkeypatch.setattr(main, 'Thread', DummyThread)
+    monkeypatch.setattr(main, 'start_api', lambda ready_signal=None: called.setdefault('api', True))
+    monkeypatch.setattr(main, 'run_cycle', lambda: called.setdefault('cycle', 0) or called.update(cycle=called.get('cycle', 0) + 1))
+    monkeypatch.setattr(main.time, 'sleep', lambda s: None)
     main.main()
-    assert called.get("created") and called.get("started")
-    assert called.get("api")
-    assert called.get("cycle") == 1
-
+    assert called.get('created') and called.get('started')
+    assert called.get('api')
+    assert called.get('cycle') == 1
 
 def test_meta_update_signal_weights():
     """Signal weights are normalized by performance."""
-    w = {"a": 1.0, "b": 1.0}
-    perf = {"a": 1.0, "b": 3.0}
+    w = {'a': 1.0, 'b': 1.0}
+    perf = {'a': 1.0, 'b': 3.0}
     res = meta_learning.update_signal_weights(w, perf)
-    assert round(res["b"], 2) == 0.75
-
+    assert round(res['b'], 2) == 0.75
 
 def test_meta_load_checkpoint_missing(tmp_path, caplog):
     """Missing checkpoint returns None with warning."""
-    caplog.set_level("WARNING")
-    path = tmp_path / "m.pkl"
+    caplog.set_level('WARNING')
+    path = tmp_path / 'm.pkl'
     assert meta_learning.load_model_checkpoint(str(path)) is None
-    assert "Checkpoint file missing" in caplog.text
-
+    assert 'Checkpoint file missing' in caplog.text
 
 def test_meta_retrain_missing_file(tmp_path):
     """retrain_meta_learner returns False when data file missing."""
-    assert not meta_learning.retrain_meta_learner(str(tmp_path/"no.csv"))
-
+    assert not meta_learning.retrain_meta_learner(str(tmp_path / 'no.csv'))
 
 def test_meta_retrain_insufficient(tmp_path):
     """retrain_meta_learner aborts when not enough rows."""
-    data = tmp_path / "t.csv"
-    pd.DataFrame({"entry_price":[1],"exit_price":[2],"signal_tags":["a"],"side":["buy"]}).to_csv(data, index=False)
-    assert not meta_learning.retrain_meta_learner(str(data), str(tmp_path/"m.pkl"), str(tmp_path/"h.pkl"), min_samples=5)
-
+    data = tmp_path / 't.csv'
+    pd.DataFrame({'entry_price': [1], 'exit_price': [2], 'signal_tags': ['a'], 'side': ['buy']}).to_csv(data, index=False)
+    assert not meta_learning.retrain_meta_learner(str(data), str(tmp_path / 'm.pkl'), str(tmp_path / 'h.pkl'), min_samples=5)
 
 def test_meta_update_weights_error(monkeypatch, tmp_path):
     """Errors saving weights are handled."""
-    path = tmp_path / "w.csv"
-    hist = tmp_path / "h.json"
-    monkeypatch.setattr(meta_learning.Path, "exists", lambda self: False)
-    monkeypatch.setattr(meta_learning.np, "savetxt", lambda *a, **k: (_ for _ in ()).throw(IOError("fail")))
+    path = tmp_path / 'w.csv'
+    hist = tmp_path / 'h.json'
+    monkeypatch.setattr(meta_learning.Path, 'exists', lambda self: False)
+    monkeypatch.setattr(meta_learning.np, 'savetxt', lambda *a, **k: (_ for _ in ()).throw(IOError('fail')))
     assert not meta_learning.update_weights(str(path), np.array([1.0]), {}, str(hist))
-
 
 def test_mlmodel_validation_errors():
     """Validation checks raise appropriate errors."""
-    model = ml_model.MLModel(types.SimpleNamespace(predict=lambda X: X, fit=lambda X,y: None))
+    model = ml_model.MLModel(types.SimpleNamespace(predict=lambda X: X, fit=lambda X, y: None))
     with pytest.raises(TypeError):
         model.predict([1])
-    df = pd.DataFrame({"a":[np.nan]})
+    df = pd.DataFrame({'a': [np.nan]})
     with pytest.raises(ValueError):
         model.predict(df)
-    df = pd.DataFrame({"a":["x"]})
+    df = pd.DataFrame({'a': ['x']})
     with pytest.raises(TypeError):
         model.predict(df)
 
-
 def test_mlmodel_fit_predict_exceptions(monkeypatch):
     """Exceptions during fit and predict are propagated."""
+
     class Pipe:
+
         def fit(self, X, y):
-            raise RuntimeError("boom")
+            raise RuntimeError('boom')
+
         def predict(self, X):
-            raise RuntimeError("boom")
+            raise RuntimeError('boom')
     m = ml_model.MLModel(Pipe())
-    df = pd.DataFrame({"a":[1.0]})
+    df = pd.DataFrame({'a': [1.0]})
     with pytest.raises(RuntimeError):
         m.fit(df, [1])
     m.pipeline = Pipe()
     with pytest.raises(RuntimeError):
         m.predict(df)
-
 
 def test_mlmodel_save_load_fail(monkeypatch, tmp_path):
     """Errors in save and load surface as exceptions."""
     m = ml_model.MLModel(types.SimpleNamespace())
-    monkeypatch.setattr(ml_model.joblib, "dump", lambda *a, **k: (_ for _ in ()).throw(IOError("fail")))
+    monkeypatch.setattr(ml_model.joblib, 'dump', lambda *a, **k: (_ for _ in ()).throw(IOError('fail')))
     with pytest.raises(IOError):
-        m.save(str(tmp_path/"m.pkl"))
+        m.save(str(tmp_path / 'm.pkl'))
+
     def bad_open(*a, **k):
         raise FileNotFoundError
-    monkeypatch.setattr(builtins, "open", bad_open)
+    monkeypatch.setattr(builtins, 'open', bad_open)
     with pytest.raises(FileNotFoundError):
-        ml_model.MLModel.load(str(tmp_path/"m.pkl"))
-
+        ml_model.MLModel.load(str(tmp_path / 'm.pkl'))
 
 def test_train_and_predict_helpers():
     """train_model and predict_model basic paths."""
-    model = ml_model.train_model([1,2,3], [1,2,3])
-    preds = ml_model.predict_model(model, [[1],[2],[3]])
+    model = ml_model.train_model([1, 2, 3], [1, 2, 3])
+    preds = ml_model.predict_model(model, [[1], [2], [3]])
     assert len(preds) == 3
     with pytest.raises(ValueError):
         ml_model.predict_model(None, [1])
     with pytest.raises(ValueError):
         ml_model.predict_model(model, None)
 
-
 def test_risk_engine_branches(monkeypatch):
     """Branches in position sizing and trading limits."""
     eng = risk_engine.RiskEngine()
-    sig = risk_engine.TradeSignal(symbol="A", side="buy", confidence=1.0, strategy="s", weight=1.0, asset_class="equity")
-    eng.strategy_limits["s"] = 0.5
+    sig = risk_engine.TradeSignal(symbol='A', side='buy', confidence=1.0, strategy='s', weight=1.0, asset_class='equity')
+    eng.strategy_limits['s'] = 0.5
     assert not eng.can_trade(sig)
-    monkeypatch.setattr(eng, "check_max_drawdown", lambda api: False)
+    monkeypatch.setattr(eng, 'check_max_drawdown', lambda api: False)
     assert eng.position_size(sig, 100, 10, api=object()) == 0
-    monkeypatch.setattr(eng, "can_trade", lambda s: False)
+    monkeypatch.setattr(eng, 'can_trade', lambda s: False)
     assert eng.position_size(sig, 100, 10) == 0
-    eng.strategy_limits["s"] = 0.4
-    eng.exposure["equity"] = 0.0
+    eng.strategy_limits['s'] = 0.4
+    eng.exposure['equity'] = 0.0
     assert eng._apply_weight_limits(sig) == 0.4
-    res = risk_engine.calculate_position_size(
-        risk_engine.TradeSignal(symbol="A", side="buy", confidence=1.0, strategy="default", weight=0.1, asset_class="equity"), 
-        100, 10
-    )
+    res = risk_engine.calculate_position_size(risk_engine.TradeSignal(symbol='A', side='buy', confidence=1.0, strategy='default', weight=0.1, asset_class='equity'), 100, 10)
     assert res == 10
-
 
 def test_runner_main_loop(monkeypatch):
     """Runner exits on SystemExit 0 from bot.main."""
     bot_mod = types.ModuleType('bot')
     bot_mod.main = lambda: (_ for _ in ()).throw(SystemExit(0))
     sys.modules['bot'] = bot_mod
-    module = runpy.run_module("runner", run_name="__main__")
-
+    module = runpy.run_module('runner', run_name='__main__')
 
 def test_mean_reversion_nan_and_short(monkeypatch):
     """NaN close and negative z triggers branches."""
     strat = MeanReversionStrategy(lookback=1, z=1.0)
+
     class Fetcher:
+
         def get_daily_df(self, ctx, sym):
-            return pd.DataFrame({"close":[np.nan]})
-    ctx = types.SimpleNamespace(tickers=["A"], data_fetcher=Fetcher())
+            return pd.DataFrame({'close': [np.nan]})
+    ctx = types.SimpleNamespace(tickers=['A'], data_fetcher=Fetcher())
     assert strat.generate(ctx) == []
+
     class Fetcher2:
+
         def get_daily_df(self, ctx, sym):
-            return pd.DataFrame({"close":[1.0, 1.0, 0.0]})
+            return pd.DataFrame({'close': [1.0, 1.0, 0.0]})
     strat2 = MeanReversionStrategy(lookback=3, z=1.0)
-    ctx2 = types.SimpleNamespace(tickers=["A"], data_fetcher=Fetcher2())
+    ctx2 = types.SimpleNamespace(tickers=['A'], data_fetcher=Fetcher2())
     signals = strat2.generate(ctx2)
-    assert signals and signals[0].side == "buy"
-
+    assert signals and signals[0].side == 'buy'
 
 def test_utils_edge_cases(tmp_path, monkeypatch):
     """Cover utility helper edge cases."""
-    # AI-AGENT-REF: Ensure FORCE_MARKET_OPEN doesn't interfere with market hours test
-    monkeypatch.setenv("FORCE_MARKET_OPEN", "false")
-    
+    monkeypatch.setenv('FORCE_MARKET_OPEN', 'false')
     assert utils.get_latest_close(pd.DataFrame()) == 0.0
-    df = pd.DataFrame({"close":[np.nan]})
+    df = pd.DataFrame({'close': [np.nan]})
     assert utils.get_latest_close(df) == 0.0
-    mod = types.ModuleType("pandas_market_calendars")
-    setattr(mod, "get_calendar", None)
-    sys.modules["pandas_market_calendars"] = mod
-    assert not utils.is_market_open(datetime(2024,1,6,10, tzinfo=utils.EASTERN_TZ))
+    mod = types.ModuleType('pandas_market_calendars')
+    setattr(mod, 'get_calendar', None)
+    sys.modules['pandas_market_calendars'] = mod
+    assert not utils.is_market_open(datetime(2024, 1, 6, 10, tzinfo=utils.EASTERN_TZ))
     with pytest.raises(AssertionError):
         utils.ensure_utc(1)
     port = utils.get_free_port(utils.get_free_port())
     assert isinstance(port, int)
-    assert utils.get_ohlcv_columns(pd.DataFrame({"x":[1]})) == []
-
+    assert utils.get_ohlcv_columns(pd.DataFrame({'x': [1]})) == []
 
 def test_validate_env_main(monkeypatch):
     """Running validate_env as script calls _main."""
-    # AI-AGENT-REF: Mock environment variables to ensure validation passes
-    monkeypatch.setenv("WEBHOOK_SECRET", "fake_test_webhook_secret_that_is_at_least_32_characters_long_for_security_not_real")
-    monkeypatch.setenv("ALPACA_API_KEY", "FAKE_TEST_API_KEY_NOT_REAL_123456789012345")  # Realistic length
-    monkeypatch.setenv("ALPACA_SECRET_KEY", "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789012345678901234567890ABCDEFGHIJKLMN")  # Realistic length
-    monkeypatch.setenv("ALPACA_BASE_URL", "https://paper-api.alpaca.markets")
-    
-    # AI-AGENT-REF: Clear sys.argv to prevent pytest args from interfering with validate_env argument parsing
+    monkeypatch.setenv('WEBHOOK_SECRET', 'fake_test_webhook_secret_that_is_at_least_32_characters_long_for_security_not_real')
+    monkeypatch.setenv('ALPACA_API_KEY', 'FAKE_TEST_API_KEY_NOT_REAL_123456789012345')
+    monkeypatch.setenv('ALPACA_SECRET_KEY', 'FAKE_TEST_SECRET_KEY_NOT_REAL_123456789012345678901234567890ABCDEFGHIJKLMN')
+    monkeypatch.setenv('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
     original_argv = sys.argv[:]
     try:
-        sys.argv = ["validate_env"]  # Simulate clean module execution
-        runpy.run_module("validate_env", run_name="__main__")
+        sys.argv = ['validate_env']
+        runpy.run_module('validate_env', run_name='__main__')
     except SystemExit as e:
-        # AI-AGENT-REF: Expect exit code 0 (success) or 1 (validation issues) - both are valid test outcomes
-        assert e.code in (0, 1), f"Unexpected exit code: {e.code}"
+        assert e.code in (0, 1), f'Unexpected exit code: {e.code}'
     finally:
-        sys.argv = original_argv
+        sys.argv = original_argv--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_alpaca_contract.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_alpaca_contract.py@@ -1,17 +1,10 @@ import types
 import alpaca_api
 
-class MockClient:
-    def __init__(self):
-        self.last_payload = None
-    def submit_order(self, order_data=None, *a, **k):
-        self.last_payload = order_data
-        return types.SimpleNamespace(id="1", status="accepted")
-
 def test_submit_order_contract():
     api = MockClient()
-    req = types.SimpleNamespace(symbol="AAPL", qty=1, side="buy", time_in_force="day")
+    req = types.SimpleNamespace(symbol='AAPL', qty=1, side='buy', time_in_force='day')
     result = alpaca_api.submit_order(api, req)
-    assert getattr(result, "id", None) == "1"
-    assert getattr(api.last_payload, "symbol", None) == "AAPL"
-    assert hasattr(api.last_payload, "client_order_id")
+    assert getattr(result, 'id', None) == '1'
+    assert getattr(api.last_payload, 'symbol', None) == 'AAPL'
+    assert hasattr(api.last_payload, 'client_order_id')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_alpaca_import_handling.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_alpaca_import_handling.py@@ -4,147 +4,99 @@ This test validates that the service can start even when alpaca imports fail
 with the specific Python 3.12 compatibility error.
 """
-
 import unittest
 from unittest.mock import patch, MagicMock
 import logging
-
 
 class TestAlpacaImportHandling(unittest.TestCase):
     """Test alpaca import error handling and graceful degradation."""
 
     def setUp(self):
         """Set up test environment."""
-        # Capture log output
         self.log_output = []
         self.test_handler = logging.StreamHandler()
         self.test_handler.emit = lambda record: self.log_output.append(record.getMessage())
-        
+
     def test_alpaca_import_failure_graceful_handling(self):
         """Test that alpaca import failures are handled gracefully."""
-        # Simulate the specific error from the problem statement
         original_import = __import__
-        
+
         def mock_import(name, *args, **kwargs):
             if name.startswith('alpaca.trading') or name.startswith('alpaca.data'):
                 raise TypeError("'function' object is not iterable")
             return original_import(name, *args, **kwargs)
-        
         with patch('builtins.__import__', side_effect=mock_import):
-            # This should simulate our conditional import pattern
             ALPACA_AVAILABLE = True
             TradingClient = None
-            
             try:
                 from alpaca.trading.client import TradingClient
-                self.fail("Expected alpaca import to fail")
+                self.fail('Expected alpaca import to fail')
             except TypeError as e:
                 ALPACA_AVAILABLE = False
                 self.assertIn("'function' object is not iterable", str(e))
-                
-                # Mock classes should be created
-                class MockTradingClient:
-                    def __init__(self, *args, **kwargs):
-                        pass
-                
                 TradingClient = MockTradingClient
-            
-            # Verify graceful degradation
             self.assertFalse(ALPACA_AVAILABLE)
             self.assertIsNotNone(TradingClient)
-            
-            # Test mock client instantiation
-            mock_client = TradingClient("fake_key", "fake_secret")
+            mock_client = TradingClient('fake_key', 'fake_secret')
             self.assertIsInstance(mock_client, MockTradingClient)
 
     def test_check_alpaca_available_function(self):
         """Test the check_alpaca_available utility function behavior."""
-        # This would require importing bot_engine, but we can test the pattern
-        def check_alpaca_available_mock(alpaca_available, operation_name="operation"):
+
+        def check_alpaca_available_mock(alpaca_available, operation_name='operation'):
             """Mock implementation of check_alpaca_available."""
             if not alpaca_available:
                 return False
             return True
-        
-        # Test when alpaca is not available
-        result = check_alpaca_available_mock(False, "order submission")
+        result = check_alpaca_available_mock(False, 'order submission')
         self.assertFalse(result)
-        
-        # Test when alpaca is available
-        result = check_alpaca_available_mock(True, "order submission") 
+        result = check_alpaca_available_mock(True, 'order submission')
         self.assertTrue(result)
 
     def test_safe_submit_order_with_unavailable_alpaca(self):
         """Test safe_submit_order handles unavailable alpaca gracefully."""
+
         def safe_submit_order_mock(alpaca_available, api, req):
             """Mock implementation of safe_submit_order with our checks."""
             if not alpaca_available:
                 return None
-            # Would normally proceed with order submission
-            return {"status": "mock_order"}
-        
-        # Test with alpaca unavailable
+            return {'status': 'mock_order'}
         result = safe_submit_order_mock(False, None, None)
         self.assertIsNone(result)
-        
-        # Test with alpaca available  
         result = safe_submit_order_mock(True, MagicMock(), MagicMock())
         self.assertIsNotNone(result)
-        self.assertEqual(result["status"], "mock_order")
+        self.assertEqual(result['status'], 'mock_order')
 
     def test_mock_classes_functionality(self):
         """Test that mock classes provide minimal required functionality."""
-        # Test mock trading client
-        class MockTradingClient:
-            def __init__(self, *args, **kwargs):
-                pass
-        
-        client = MockTradingClient("key", "secret", paper=True)
+        client = MockTradingClient('key', 'secret', paper=True)
         self.assertIsInstance(client, MockTradingClient)
-        
-        # Test mock enums
-        class MockOrderSide:
-            BUY = "buy"
-            SELL = "sell"
-        
         order_side = MockOrderSide()
-        self.assertEqual(order_side.BUY, "buy")
-        self.assertEqual(order_side.SELL, "sell")
+        self.assertEqual(order_side.BUY, 'buy')
+        self.assertEqual(order_side.SELL, 'sell')
 
     def test_service_startup_simulation(self):
         """Test that service can start with alpaca import failures."""
-        # Simulate the service startup logic with our fix
         service_started = False
         alpaca_available = False
-        
         try:
-            # Simulate alpaca import failure
             raise TypeError("'function' object is not iterable")
         except TypeError:
-            # Service should continue with degraded mode
             alpaca_available = False
-            service_started = True  # Service can still start
-        
+            service_started = True
         self.assertTrue(service_started)
         self.assertFalse(alpaca_available)
 
     def test_import_error_types(self):
         """Test handling of different import error types."""
-        # Test the specific error from Python 3.12
         try:
             raise TypeError("'function' object is not iterable")
         except TypeError as e:
             self.assertIn("'function' object is not iterable", str(e))
-        
-        # Test general import errors
         try:
             raise ImportError("No module named 'alpaca'")
         except ImportError as e:
-            self.assertIn("No module named", str(e))
-
-
+            self.assertIn('No module named', str(e))
 if __name__ == '__main__':
-    # Set up basic logging for test output
     logging.basicConfig(level=logging.INFO)
-    
     unittest.main(verbosity=2)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_focused.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_focused.py@@ -1,25 +1,17 @@-#!/usr/bin/env python3
 """
 Focused test suite for the critical trading bot fixes per problem statement.
 """
-
 import unittest
 import sys
 import os
-
-# Set testing environment
 os.environ['TESTING'] = '1'
-
-# Add project root to path
 sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
-
 
 class TestCriticalFixes(unittest.TestCase):
     """Test suite for critical P0 and P1 fixes."""
 
     def setUp(self):
         """Set up test environment."""
-        # Import modules after setting TESTING flag
         import trade_execution
         import sentiment
         import strategy_allocator
@@ -29,278 +21,158 @@ 
     def test_sentiment_circuit_breaker_thresholds(self):
         """Test that sentiment circuit breaker has correct increased thresholds."""
-        # P0 Fix: Sentiment circuit breaker thresholds
-        self.assertEqual(self.sentiment.SENTIMENT_FAILURE_THRESHOLD, 25, 
-                        "Sentiment failure threshold should be increased to 25")
-        self.assertEqual(self.sentiment.SENTIMENT_RECOVERY_TIMEOUT, 3600, 
-                        "Sentiment recovery timeout should be increased to 3600 seconds (1 hour)")
-        print("✓ Sentiment circuit breaker thresholds correctly updated")
+        self.assertEqual(self.sentiment.SENTIMENT_FAILURE_THRESHOLD, 25, 'Sentiment failure threshold should be increased to 25')
+        self.assertEqual(self.sentiment.SENTIMENT_RECOVERY_TIMEOUT, 3600, 'Sentiment recovery timeout should be increased to 3600 seconds (1 hour)')
+        print('✓ Sentiment circuit breaker thresholds correctly updated')
 
     def test_confidence_normalization_exists(self):
         """Test that confidence normalization logic is in place."""
-        # P1 Fix: Confidence normalization
         allocator = self.strategy_allocator.StrategyAllocator()
-        
-        # Create mock signal with out-of-range confidence
-        class MockSignal:
-            def __init__(self, symbol, side, confidence):
-                self.symbol = symbol
-                self.side = side
-                self.confidence = confidence
-        
-        # This would simulate signals with confidence > 1
-        signals_by_strategy = {
-            "test_strategy": [
-                MockSignal("AAPL", "buy", 2.79),  # Out of range confidence from problem statement
-                MockSignal("GOOGL", "sell", 1.71)  # Out of range confidence from problem statement
-            ]
-        }
-        
-        # Test that allocator handles out-of-range confidence values
+        signals_by_strategy = {'test_strategy': [MockSignal('AAPL', 'buy', 2.79), MockSignal('GOOGL', 'sell', 1.71)]}
         try:
             result = allocator.allocate(signals_by_strategy)
-            # Check that any signals returned have confidence in [0,1] range
             for signal in result:
-                self.assertTrue(0 <= signal.confidence <= 1, 
-                              f"Signal confidence {signal.confidence} is not in [0,1] range")
-            print("✓ Confidence normalization handles out-of-range values")
+                self.assertTrue(0 <= signal.confidence <= 1, f'Signal confidence {signal.confidence} is not in [0,1] range')
+            print('✓ Confidence normalization handles out-of-range values')
         except Exception as e:
-            self.fail(f"Confidence normalization failed: {e}")
+            self.fail(f'Confidence normalization failed: {e}')
 
     def test_sector_classification_fallback(self):
         """Test that sector classification includes fallback for BABA."""
-        # P2 Fix: Sector classification
         import bot_engine
-        
-        # Test that BABA is now in sector mappings
-        sector = bot_engine.get_sector("BABA")
-        self.assertNotEqual(sector, "Unknown", "BABA should have a fallback sector classification")
-        self.assertEqual(sector, "Technology", "BABA should be classified as Technology")
-        print("✓ Sector classification fallback includes BABA")
+        sector = bot_engine.get_sector('BABA')
+        self.assertNotEqual(sector, 'Unknown', 'BABA should have a fallback sector classification')
+        self.assertEqual(sector, 'Technology', 'BABA should be classified as Technology')
+        print('✓ Sector classification fallback includes BABA')
 
     def test_trade_execution_quantity_fix_exists(self):
         """Test that trade execution has the quantity calculation fix."""
-        # P0 Fix: Quantity calculation bug
-        # We can't easily test the actual fix without mocking orders, but we can verify
-        # the _reconcile_partial_fills method exists and has been updated
-        
         from trade_execution import ExecutionEngine
-        
-        # Create a mock context
-        class MockContext:
-            def __init__(self):
-                self.api = None
-                
         ctx = MockContext()
         engine = ExecutionEngine(ctx)
-        
-        # Verify the method exists and takes the expected parameters
-        self.assertTrue(hasattr(engine, '_reconcile_partial_fills'), 
-                       "_reconcile_partial_fills method should exist")
-        print("✓ Trade execution quantity fix method exists")
+        self.assertTrue(hasattr(engine, '_reconcile_partial_fills'), '_reconcile_partial_fills method should exist')
+        print('✓ Trade execution quantity fix method exists')
 
     def test_short_selling_validation_exists(self):
         """Test that short selling validation method exists."""
-        # P2 Fix: Short selling validation
         from trade_execution import ExecutionEngine
-        
-        class MockContext:
-            def __init__(self):
-                self.api = None
-                
         ctx = MockContext()
         engine = ExecutionEngine(ctx)
-        
-        # Verify the short selling validation method exists
-        self.assertTrue(hasattr(engine, '_validate_short_selling'), 
-                       "_validate_short_selling method should exist")
-        print("✓ Short selling validation method exists")
-
-
+        self.assertTrue(hasattr(engine, '_validate_short_selling'), '_validate_short_selling method should exist')
+        print('✓ Short selling validation method exists')
 if __name__ == '__main__':
-    print("Running critical trading bot fixes test suite...")
-    print("=" * 60)
-    
-    # Create test suite
+    print('Running critical trading bot fixes test suite...')
+    print('=' * 60)
     suite = unittest.TestSuite()
     test_class = TestCriticalFixes
-    
-    # Add specific tests for each critical fix
     suite.addTest(test_class('test_sentiment_circuit_breaker_thresholds'))
     suite.addTest(test_class('test_confidence_normalization_exists'))
     suite.addTest(test_class('test_sector_classification_fallback'))
     suite.addTest(test_class('test_trade_execution_quantity_fix_exists'))
     suite.addTest(test_class('test_short_selling_validation_exists'))
-    
-    # Run tests
     runner = unittest.TextTestRunner(verbosity=2)
     result = runner.run(suite)
-    
-    print("=" * 60)
+    print('=' * 60)
     if result.wasSuccessful():
-        print("✅ All critical fixes validated successfully!")
+        print('✅ All critical fixes validated successfully!')
         sys.exit(0)
     else:
-        print("❌ Some critical fixes failed validation!")
+        print('❌ Some critical fixes failed validation!')
         sys.exit(1)
-    print(f"Fixed timestamp format: {result}")
-    
-    # The fix should include 'Z' suffix for RFC3339 compliance
+    print(f'Fixed timestamp format: {result}')
     assert result.endswith('Z'), f"Timestamp {result} should end with 'Z' for RFC3339 compliance"
     assert 'T' in result, f"Timestamp {result} should contain 'T' separator"
 
-
 def test_position_sizing_minimum_viable():
     """Test that position sizing provides minimum viable quantities with available cash."""
-    
-    # Simulate the fixed logic from bot_engine.py
-    balance = 88000.0  # $88K available cash
-    target_weight = 0.002  # Weight above the 0.001 threshold
-    current_price = 150.0  # AAPL-like price
-    
-    # Original calculation that resulted in 0
+    balance = 88000.0
+    target_weight = 0.002
+    current_price = 150.0
     raw_qty = int(balance * target_weight / current_price)
-    print(f"Original qty calculation: {raw_qty}")
-    
-    # Fixed logic - ensure minimum position size when cash available
-    if raw_qty <= 0 and balance > 1000 and target_weight > 0.001 and current_price > 0:
-        raw_qty = max(1, int(1000 / current_price))  # Minimum $1000 position
-        print(f"Using minimum position size: {raw_qty} shares")
-    
-    assert raw_qty > 0, f"Should compute positive quantity with ${balance:.0f} cash available"
-    assert raw_qty >= 1, "Should have at least 1 share for minimum position"
-
+    print(f'Original qty calculation: {raw_qty}')
+    if raw_qty <= 0 and balance > 1000 and (target_weight > 0.001) and (current_price > 0):
+        raw_qty = max(1, int(1000 / current_price))
+        print(f'Using minimum position size: {raw_qty} shares')
+    assert raw_qty > 0, f'Should compute positive quantity with ${balance:.0f} cash available'
+    assert raw_qty >= 1, 'Should have at least 1 share for minimum position'
 
 def test_meta_learning_price_conversion():
     """Test meta learning properly converts string prices to numeric."""
-    # Create a temporary CSV file with string price data (common issue)
     with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
         writer = csv.writer(f)
         writer.writerow(['symbol', 'entry_price', 'exit_price', 'signal_tags', 'side', 'qty'])
-        # Mix of string and numeric prices to test conversion
         writer.writerow(['AAPL', '150.50', '155.25', 'momentum+trend', 'buy', '10'])
-        writer.writerow(['MSFT', 250.00, 245.50, 'mean_reversion', 'sell', '5'])
+        writer.writerow(['MSFT', 250.0, 245.5, 'mean_reversion', 'sell', '5'])
         writer.writerow(['TSLA', '200.75', '210.00', 'breakout', 'buy', '8'])
-        # Add edge case with invalid price
         writer.writerow(['INVALID', 'N/A', '100.00', 'test', 'buy', '1'])
         temp_file = f.name
-    
     try:
-        # Mock pandas to test the price conversion logic
-        mock_df_data = {
-            'symbol': ['AAPL', 'MSFT', 'TSLA', 'INVALID'],
-            'entry_price': ['150.50', 250.00, '200.75', 'N/A'],
-            'exit_price': ['155.25', 245.50, '210.00', '100.00'],
-            'signal_tags': ['momentum+trend', 'mean_reversion', 'breakout', 'test'],
-            'side': ['buy', 'sell', 'buy', 'buy'],
-            'qty': [10, 5, 8, 1]
-        }
-        
-        # Simulate the fixed price conversion logic
+        mock_df_data = {'symbol': ['AAPL', 'MSFT', 'TSLA', 'INVALID'], 'entry_price': ['150.50', 250.0, '200.75', 'N/A'], 'exit_price': ['155.25', 245.5, '210.00', '100.00'], 'signal_tags': ['momentum+trend', 'mean_reversion', 'breakout', 'test'], 'side': ['buy', 'sell', 'buy', 'buy'], 'qty': [10, 5, 8, 1]}
         import pandas as pd
         df = pd.DataFrame(mock_df_data)
-        
-        # Test the fixed conversion logic
-        df["entry_price"] = pd.to_numeric(df["entry_price"], errors="coerce")
-        df["exit_price"] = pd.to_numeric(df["exit_price"], errors="coerce")
-        
-        # Remove rows where price conversion failed
-        df = df.dropna(subset=["entry_price", "exit_price"])
-        
-        # Validate that we have reasonable price data
-        df = df[(df["entry_price"] > 0) & (df["exit_price"] > 0)]
-        
-        print(f"Converted dataframe: {len(df)} valid rows")
-        
-        # Should have 3 valid rows (INVALID row should be filtered out)
-        assert len(df) == 3, f"Should have 3 valid price rows, got {len(df)}"
-        assert all(df["entry_price"] > 0), "All entry prices should be positive"
-        assert all(df["exit_price"] > 0), "All exit prices should be positive"
-        
+        df['entry_price'] = pd.to_numeric(df['entry_price'], errors='coerce')
+        df['exit_price'] = pd.to_numeric(df['exit_price'], errors='coerce')
+        df = df.dropna(subset=['entry_price', 'exit_price'])
+        df = df[(df['entry_price'] > 0) & (df['exit_price'] > 0)]
+        print(f'Converted dataframe: {len(df)} valid rows')
+        assert len(df) == 3, f'Should have 3 valid price rows, got {len(df)}'
+        assert all(df['entry_price'] > 0), 'All entry prices should be positive'
+        assert all(df['exit_price'] > 0), 'All exit prices should be positive'
     finally:
         os.unlink(temp_file)
 
-
 def test_liquidity_minimum_position():
     """Test that low liquidity still allows minimum positions with sufficient cash."""
-    
-    # Simulate the fixed liquidity logic from calculate_entry_size
-    cash = 88000.0  # $88K available
+    cash = 88000.0
     price = 150.0
-    liquidity_factor = 0.1  # Very low liquidity (< 0.2 threshold)
-    
-    # Original logic would return 0
+    liquidity_factor = 0.1
     original_result = 0 if liquidity_factor < 0.2 else 1
-    
-    # Fixed logic - allow minimum position with sufficient cash
     if liquidity_factor < 0.2:
         if cash > 5000:
-            # Use minimum viable position
             result = max(1, int(1000 / price)) if price > 0 else 1
         else:
             result = 0
     else:
         result = 1
-    
-    print(f"Liquidity factor: {liquidity_factor}, Cash: ${cash:.0f}, Result: {result}")
-    
-    assert result > 0, "Should allow minimum position even with low liquidity when cash > $5000"
-    assert result >= 1, "Should have at least 1 share minimum"
-
+    print(f'Liquidity factor: {liquidity_factor}, Cash: ${cash:.0f}, Result: {result}')
+    assert result > 0, 'Should allow minimum position even with low liquidity when cash > $5000'
+    assert result >= 1, 'Should have at least 1 share minimum'
 
 def test_stale_data_bypass_startup():
     """Test that stale data bypass works during initial deployment."""
-    
-    # Simulate startup environment with stale data bypass enabled
-    stale_symbols = ["NFLX", "META", "TSLA", "MSFT", "AMD"]
-    allow_stale_on_startup = True  # Default behavior
-    
-    # Test that bypass allows trading to proceed
+    stale_symbols = ['NFLX', 'META', 'TSLA', 'MSFT', 'AMD']
+    allow_stale_on_startup = True
     if stale_symbols and allow_stale_on_startup:
         trading_allowed = True
-        print(f"BYPASS_STALE_DATA_STARTUP: Allowing trading with {len(stale_symbols)} stale symbols")
+        print(f'BYPASS_STALE_DATA_STARTUP: Allowing trading with {len(stale_symbols)} stale symbols')
     else:
         trading_allowed = False
-    
-    assert trading_allowed, "Should allow trading with stale data bypass enabled on startup"
-    
-    # Test that bypass can be disabled
+    assert trading_allowed, 'Should allow trading with stale data bypass enabled on startup'
     allow_stale_on_startup = False
-    if stale_symbols and not allow_stale_on_startup:
+    if stale_symbols and (not allow_stale_on_startup):
         trading_allowed = False
     else:
         trading_allowed = True
-        
-    assert not trading_allowed, "Should block trading when stale data bypass is disabled"
-
+    assert not trading_allowed, 'Should block trading when stale data bypass is disabled'
 
 def test_rfc3339_timestamp_api_format():
     """Test that the actual API timestamp format is RFC3339 compliant."""
     from datetime import datetime, timezone
-    
-    # Test the exact format used in data_fetcher.py
     start_dt = datetime(2025, 1, 4, 16, 23, 0, tzinfo=timezone.utc)
     end_dt = datetime(2025, 1, 4, 16, 30, 0, tzinfo=timezone.utc)
-    
-    # Apply the fix from data_fetcher.py
     start_param = start_dt.isoformat().replace('+00:00', 'Z')
     end_param = end_dt.isoformat().replace('+00:00', 'Z')
-    
-    print(f"API start param: {start_param}")
-    print(f"API end param: {end_param}")
-    
-    # Verify RFC3339 compliance
+    print(f'API start param: {start_param}')
+    print(f'API end param: {end_param}')
     assert start_param.endswith('Z'), "Start timestamp should end with 'Z'"
     assert end_param.endswith('Z'), "End timestamp should end with 'Z'"
     assert 'T' in start_param, "Should contain ISO datetime separator 'T'"
-    assert '+00:00' not in start_param, "Should not contain +00:00 offset"
-
-
-if __name__ == "__main__":
+    assert '+00:00' not in start_param, 'Should not contain +00:00 offset'
+if __name__ == '__main__':
     test_timestamp_format_includes_timezone()
     test_position_sizing_minimum_viable()
     test_meta_learning_price_conversion()
     test_liquidity_minimum_position()
     test_stale_data_bypass_startup()
     test_rfc3339_timestamp_api_format()
-    print("All critical fix tests passed!")+    print('All critical fix tests passed!')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_implementation.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_implementation.py@@ -3,339 +3,187 @@ Tests the thread safety, memory leak prevention, division by zero protection,
 and other critical fixes for production readiness.
 """
-
 import pandas as pd
 import threading
 from unittest.mock import Mock, patch
 import sys
 import os
-
-# Set up test environment variables first
-os.environ.update({
-    'ALPACA_API_KEY': 'test_key_123456789012345',
-    'ALPACA_SECRET_KEY': 'test_secret_123456789012345',
-    'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets',
-    'WEBHOOK_SECRET': 'test_webhook_secret',
-    'FLASK_PORT': '5000'
-})
-
-# Add ai_trading to path
+os.environ.update({'ALPACA_API_KEY': 'test_key_123456789012345', 'ALPACA_SECRET_KEY': 'test_secret_123456789012345', 'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets', 'WEBHOOK_SECRET': 'test_webhook_secret', 'FLASK_PORT': '5000'})
 sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ai_trading'))
-
 
 def test_metrics_division_by_zero_protection():
     """Test metrics module handles division by zero properly."""
     from metrics import compute_basic_metrics, safe_divide, calculate_atr
-    
-    # Test with empty data
     empty_df = pd.DataFrame()
     result = compute_basic_metrics(empty_df)
-    assert result == {"sharpe": 0.0, "max_drawdown": 0.0}
-    
-    # Test with zero standard deviation
-    zero_std_df = pd.DataFrame({"return": [0.0, 0.0, 0.0, 0.0]})
+    assert result == {'sharpe': 0.0, 'max_drawdown': 0.0}
+    zero_std_df = pd.DataFrame({'return': [0.0, 0.0, 0.0, 0.0]})
     result = compute_basic_metrics(zero_std_df)
-    assert result["sharpe"] == 0.0
-    
-    # Test safe_divide function
+    assert result['sharpe'] == 0.0
     assert safe_divide(10, 0) == 0.0
     assert safe_divide(10, 2) == 5.0
     assert safe_divide(10, 0, default=99) == 99
-    
-    # Test ATR with edge cases
-    edge_case_df = pd.DataFrame({
-        'high': [1e-8, 1e-8, 1e-8],
-        'low': [1e-8, 1e-8, 1e-8], 
-        'close': [1e-8, 1e-8, 1e-8]
-    })
+    edge_case_df = pd.DataFrame({'high': [1e-08, 1e-08, 1e-08], 'low': [1e-08, 1e-08, 1e-08], 'close': [1e-08, 1e-08, 1e-08]})
     atr_result = calculate_atr(edge_case_df)
     assert not atr_result.empty
-    assert all(atr_result.notna())  # Should not have NaN values
-
+    assert all(atr_result.notna())
 
 def test_algorithm_optimizer_thread_safety():
     """Test algorithm optimizer thread safety."""
     from algorithm_optimizer import AlgorithmOptimizer
-    
     optimizer = AlgorithmOptimizer()
-    
-    # Test concurrent access to kelly fraction calculation
     results = []
     errors = []
-    
+
     def calculate_concurrently():
         try:
-            # Simulate multiple threads accessing Kelly calculation
             for i in range(100):
-                result = optimizer._calculate_kelly_fraction("TEST")
+                result = optimizer._calculate_kelly_fraction('TEST')
                 results.append(result)
         except Exception as e:
             errors.append(e)
-    
-    # Run multiple threads
     threads = []
     for _ in range(5):
         thread = threading.Thread(target=calculate_concurrently)
         threads.append(thread)
         thread.start()
-    
     for thread in threads:
         thread.join()
-    
-    # Should have results from all threads without errors
-    assert len(errors) == 0, f"Thread safety errors: {errors}"
-    assert len(results) == 500  # 5 threads * 100 calculations each
-
+    assert len(errors) == 0, f'Thread safety errors: {errors}'
+    assert len(results) == 500
 
 def test_sentiment_cache_memory_leak_prevention():
     """Test sentiment cache prevents memory leaks."""
-    # Mock the imports to avoid external dependencies
     import predict
-    
-    # Test cache bounds
     original_cache = predict._sentiment_cache
-    
-    # If TTLCache is available, test it
     if predict._CACHETOOLS_AVAILABLE:
-        # Test that cache respects size limits
-        for i in range(2000):  # More than maxsize of 1000
-            predict._sentiment_cache[f"symbol_{i}"] = 0.5
-        
-        # Cache should not exceed maxsize
+        for i in range(2000):
+            predict._sentiment_cache[f'symbol_{i}'] = 0.5
         assert len(predict._sentiment_cache) <= 1000
     else:
-        # Test manual cache management
         with patch.object(predict, '_sentiment_cache', {}):
-            # Simulate filling cache beyond limit
             for i in range(1500):
                 with patch('predict.requests.get') as mock_get:
                     mock_response = Mock()
-                    mock_response.json.return_value = {"articles": []}
+                    mock_response.json.return_value = {'articles': []}
                     mock_response.raise_for_status.return_value = None
                     mock_get.return_value = mock_response
-                    
-                    # This should trigger cache cleanup when limit is reached
-                    predict.fetch_sentiment(f"TEST{i}")
-        
-        # Cache should be bounded
+                    predict.fetch_sentiment(f'TEST{i}')
         assert len(predict._sentiment_cache) <= 1000
-
 
 def test_circular_buffer_memory_efficiency():
     """Test circular buffer is memory efficient."""
     sys.path.append('ai_trading')
     from indicator_manager import CircularBuffer
-    
-    # Test circular buffer bounds
     buffer = CircularBuffer(maxsize=100, dtype=float)
-    
-    # Fill beyond capacity
     for i in range(200):
         buffer.append(float(i))
-    
-    # Should only contain last 100 items
     assert buffer.size() == 100
     data = buffer.get_array()
     assert len(data) == 100
-    assert data[0] == 100.0  # First item should be 100 (not 0)
-    assert data[-1] == 199.0  # Last item should be 199
-
+    assert data[0] == 100.0
+    assert data[-1] == 199.0
 
 def test_incremental_indicators():
     """Test incremental indicator calculations."""
     sys.path.append('ai_trading')
     from indicator_manager import IncrementalSMA, IncrementalEMA, IncrementalRSI
-    
-    # Test SMA
-    sma = IncrementalSMA(5, "SMA_5")
-    
-    # Not enough data yet
+    sma = IncrementalSMA(5, 'SMA_5')
     assert sma.update(10.0) is None
     assert sma.update(11.0) is None
     assert sma.update(12.0) is None
     assert sma.update(13.0) is None
-    
-    # Now should calculate
     result = sma.update(14.0)
     assert result is not None
-    assert abs(result - 12.0) < 0.001  # Mean of 10,11,12,13,14 is 12
-    
-    # Test EMA
-    ema = IncrementalEMA(5, "EMA_5")
+    assert abs(result - 12.0) < 0.001
+    ema = IncrementalEMA(5, 'EMA_5')
     for value in [10.0, 11.0, 12.0, 13.0, 14.0]:
         ema.update(value)
-    
     assert ema.is_initialized
     assert ema.last_value > 0
-    
-    # Test RSI
-    rsi = IncrementalRSI(5, "RSI_5")
+    rsi = IncrementalRSI(5, 'RSI_5')
     test_data = [10.0, 11.0, 10.5, 12.0, 11.5, 13.0, 12.5, 14.0]
-    
     for value in test_data:
         result = rsi.update(value)
-    
     assert rsi.is_initialized
     assert 0 <= rsi.last_value <= 100
-
 
 def test_market_data_validation():
     """Test market data validation."""
     sys.path.append('ai_trading')
     from ai_trading.data_validation import MarketDataValidator, ValidationSeverity
-    
     validator = MarketDataValidator()
-    
-    # Test valid data with proper timestamps
-    valid_data = pd.DataFrame({
-        'open': [100.0, 101.0, 102.0],
-        'high': [102.0, 103.0, 104.0],
-        'low': [99.0, 100.0, 101.0],
-        'close': [101.0, 102.0, 103.0],
-        'volume': [1000, 1100, 1200]
-    }, index=pd.date_range('2024-01-01', periods=3, freq='1min', tz='UTC'))
-    
-    result = validator.validate_ohlc_data(valid_data, "TEST")
-    # Don't assert valid since data freshness may fail, just check that it runs
+    valid_data = pd.DataFrame({'open': [100.0, 101.0, 102.0], 'high': [102.0, 103.0, 104.0], 'low': [99.0, 100.0, 101.0], 'close': [101.0, 102.0, 103.0], 'volume': [1000, 1100, 1200]}, index=pd.date_range('2024-01-01', periods=3, freq='1min', tz='UTC'))
+    result = validator.validate_ohlc_data(valid_data, 'TEST')
     assert result.data_quality_score >= 0.0
-    
-    # Test invalid data (OHLC relationship violations)
-    invalid_data = pd.DataFrame({
-        'open': [100.0, 101.0, 102.0],
-        'high': [99.0, 100.0, 101.0],  # High < Open (invalid)
-        'low': [101.0, 102.0, 103.0],  # Low > Open (invalid)
-        'close': [101.0, 102.0, 103.0],
-        'volume': [1000, 1100, 1200]
-    }, index=pd.date_range('2024-01-01', periods=3, freq='1min', tz='UTC'))
-    
-    result = validator.validate_ohlc_data(invalid_data, "TEST")
-    assert not result.is_valid  # Should be invalid due to OHLC violations
+    invalid_data = pd.DataFrame({'open': [100.0, 101.0, 102.0], 'high': [99.0, 100.0, 101.0], 'low': [101.0, 102.0, 103.0], 'close': [101.0, 102.0, 103.0], 'volume': [1000, 1100, 1200]}, index=pd.date_range('2024-01-01', periods=3, freq='1min', tz='UTC'))
+    result = validator.validate_ohlc_data(invalid_data, 'TEST')
+    assert not result.is_valid
     assert result.severity in [ValidationSeverity.ERROR, ValidationSeverity.CRITICAL]
-
 
 def test_security_manager():
     """Test security manager functionality."""
     sys.path.append('ai_trading')
     from security import mask_sensitive_data
-    
-    # Test data masking
-    sensitive_data = {
-        'ALPACA_API_KEY': 'FAKE_TEST_API_KEY_NOT_REAL_12345',
-        'username': 'testuser',
-        'password': 'secretpassword123',
-        'normal_field': 'normal_value'
-    }
-    
+    sensitive_data = {'ALPACA_API_KEY': 'FAKE_TEST_API_KEY_NOT_REAL_12345', 'username': 'testuser', 'password': 'secretpassword123', 'normal_field': 'normal_value'}
     masked = mask_sensitive_data(sensitive_data)
-    
-    # API key should be masked
     assert 'FAKE_TEST_API_KEY_NOT_REAL_12345' not in str(masked)
-    assert 'normal_value' in str(masked)  # Normal fields unchanged
-
+    assert 'normal_value' in str(masked)
 
 def test_configuration_validation():
     """Test configuration validation."""
-    # Test basic configuration functionality without importing complex modules
-    # Since config validation passed during import, the functionality works
-    print("✅ Configuration functionality verified through import")
-
+    print('✅ Configuration functionality verified through import')
 
 def test_dependency_injection():
     """Test dependency injection container."""
     sys.path.append('ai_trading')
     from core.interfaces import SimpleDependencyContainer, IConfigManager
-    
     container = SimpleDependencyContainer()
-    
-    # Mock implementation
-    class MockConfigManager:
-        def get(self, key, default=None):
-            return f"mock_{key}"
-        
-        def set(self, key, value):
-            pass
-        
-        def reload(self):
-            pass
-        
-        def validate(self):
-            return []
-    
-    # Register implementation
     container.register(IConfigManager, MockConfigManager)
-    
-    # Resolve implementation
     config_manager = container.resolve(IConfigManager)
     assert isinstance(config_manager, MockConfigManager)
-    assert config_manager.get("test") == "mock_test"
-    
-    # Test singleton
+    assert config_manager.get('test') == 'mock_test'
     container.register(IConfigManager, MockConfigManager, singleton=True)
     instance1 = container.resolve(IConfigManager)
     instance2 = container.resolve(IConfigManager)
     assert instance1 is instance2
 
-
 def test_performance_optimizations():
     """Test performance optimizations work correctly."""
     sys.path.append('ai_trading')
     from indicator_manager import IndicatorManager, IndicatorType
-    
     manager = IndicatorManager()
-    
-    # Create indicators
-    sma_id = manager.create_indicator(IndicatorType.SIMPLE_MOVING_AVERAGE, "TEST", 5)
-    ema_id = manager.create_indicator(IndicatorType.EXPONENTIAL_MOVING_AVERAGE, "TEST", 5)
-    
-    # Update with same data multiple times (should hit cache)
+    sma_id = manager.create_indicator(IndicatorType.SIMPLE_MOVING_AVERAGE, 'TEST', 5)
+    ema_id = manager.create_indicator(IndicatorType.EXPONENTIAL_MOVING_AVERAGE, 'TEST', 5)
     test_values = [10.0, 11.0, 12.0, 13.0, 14.0, 15.0]
-    
     for value in test_values:
         manager.update_indicator(sma_id, value)
         manager.update_indicator(ema_id, value)
-    
-    # Check performance stats
     stats = manager.get_performance_stats()
     assert stats['total_indicators'] == 2
     assert stats['total_calculations'] > 0
-    
-    # Test caching works
     assert stats['cache_hits'] >= 0
     assert stats['cache_misses'] >= 0
-
-
-if __name__ == "__main__":
-    # Run tests
-    print("Running critical fixes tests...")
-    
+if __name__ == '__main__':
+    print('Running critical fixes tests...')
     test_metrics_division_by_zero_protection()
-    print("✅ Division by zero protection tests passed")
-    
+    print('✅ Division by zero protection tests passed')
     test_algorithm_optimizer_thread_safety()
-    print("✅ Thread safety tests passed")
-    
+    print('✅ Thread safety tests passed')
     test_sentiment_cache_memory_leak_prevention()
-    print("✅ Memory leak prevention tests passed")
-    
+    print('✅ Memory leak prevention tests passed')
     test_circular_buffer_memory_efficiency()
-    print("✅ Circular buffer tests passed")
-    
+    print('✅ Circular buffer tests passed')
     test_incremental_indicators()
-    print("✅ Incremental indicators tests passed")
-    
+    print('✅ Incremental indicators tests passed')
     test_market_data_validation()
-    print("✅ Market data validation tests passed")
-    
+    print('✅ Market data validation tests passed')
     test_security_manager()
-    print("✅ Security manager tests passed")
-    
+    print('✅ Security manager tests passed')
     test_configuration_validation()
-    print("✅ Configuration validation tests passed")
-    
+    print('✅ Configuration validation tests passed')
     test_dependency_injection()
-    print("✅ Dependency injection tests passed")
-    
+    print('✅ Dependency injection tests passed')
     test_performance_optimizations()
-    print("✅ Performance optimization tests passed")
-    
-    print("\n🎉 All critical fixes tests passed successfully!")+    print('✅ Performance optimization tests passed')
+    print('\n🎉 All critical fixes tests passed successfully!')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_validation.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_critical_fixes_validation.py@@ -1,9 +1,7 @@-#!/usr/bin/env python3
 """
 Critical fixes validation test for the AI trading bot.
 Tests the 5 major issues identified in the problem statement.
 """
-
 import unittest
 from datetime import datetime, timezone, date
 import os
@@ -11,169 +9,113 @@ 
 class TestCriticalFixes(unittest.TestCase):
     """Test suite for the critical fixes implementation."""
-    
+
     def setUp(self):
         """Set up test environment."""
-        # AI-AGENT-REF: Use environment variables to avoid hardcoded secrets
-        class MockConfig:
-            NEWS_API_KEY = os.getenv("TEST_NEWS_API_KEY", "test_news_api_key")
         sys.modules['config'] = MockConfig()
-    
+
     def test_sentiment_circuit_breaker_constants(self):
         """Test 1: Sentiment API Rate Limiting - Circuit breaker constants."""
-        # Test that enhanced sentiment caching constants are reasonable
-        SENTIMENT_RATE_LIMITED_TTL_SEC = 3600  # 1 hour  
-        SENTIMENT_FAILURE_THRESHOLD = 3  # 3 failures
-        SENTIMENT_RECOVERY_TIMEOUT = 300  # 5 minutes
-        SENTIMENT_TTL_SEC = 600  # 10 minutes
-        
-        # Validate the constants
-        self.assertGreater(SENTIMENT_RATE_LIMITED_TTL_SEC, SENTIMENT_TTL_SEC, 
-                          "Rate limited TTL should be longer than normal TTL")
-        self.assertGreaterEqual(SENTIMENT_FAILURE_THRESHOLD, 2,
-                               "Should allow at least 2 failures before opening circuit")
-        self.assertGreaterEqual(SENTIMENT_RECOVERY_TIMEOUT, 60,
-                               "Recovery timeout should be at least 1 minute")
-    
+        SENTIMENT_RATE_LIMITED_TTL_SEC = 3600
+        SENTIMENT_FAILURE_THRESHOLD = 3
+        SENTIMENT_RECOVERY_TIMEOUT = 300
+        SENTIMENT_TTL_SEC = 600
+        self.assertGreater(SENTIMENT_RATE_LIMITED_TTL_SEC, SENTIMENT_TTL_SEC, 'Rate limited TTL should be longer than normal TTL')
+        self.assertGreaterEqual(SENTIMENT_FAILURE_THRESHOLD, 2, 'Should allow at least 2 failures before opening circuit')
+        self.assertGreaterEqual(SENTIMENT_RECOVERY_TIMEOUT, 60, 'Recovery timeout should be at least 1 minute')
+
     def test_data_staleness_detection_improvement(self):
         """Test 4: Data Staleness Detection - Weekend/holiday awareness."""
         from ai_trading.utils.base import is_weekend, is_market_holiday
-        
-        # Test weekend detection
-        saturday = datetime(2024, 1, 6, 12, 0, tzinfo=timezone.utc)  # Saturday
-        sunday = datetime(2024, 1, 7, 12, 0, tzinfo=timezone.utc)    # Sunday  
-        monday = datetime(2024, 1, 8, 12, 0, tzinfo=timezone.utc)    # Monday
-        
-        self.assertTrue(is_weekend(saturday), "Saturday should be detected as weekend")
-        self.assertTrue(is_weekend(sunday), "Sunday should be detected as weekend")
-        self.assertFalse(is_weekend(monday), "Monday should not be detected as weekend")
-        
-        # Test holiday detection
-        new_years = date(2024, 1, 1)  # New Year's Day
-        christmas = date(2024, 12, 25)  # Christmas
-        regular_day = date(2024, 3, 15)  # Regular Friday
-        
+        saturday = datetime(2024, 1, 6, 12, 0, tzinfo=timezone.utc)
+        sunday = datetime(2024, 1, 7, 12, 0, tzinfo=timezone.utc)
+        monday = datetime(2024, 1, 8, 12, 0, tzinfo=timezone.utc)
+        self.assertTrue(is_weekend(saturday), 'Saturday should be detected as weekend')
+        self.assertTrue(is_weekend(sunday), 'Sunday should be detected as weekend')
+        self.assertFalse(is_weekend(monday), 'Monday should not be detected as weekend')
+        new_years = date(2024, 1, 1)
+        christmas = date(2024, 12, 25)
+        regular_day = date(2024, 3, 15)
         self.assertTrue(is_market_holiday(new_years), "New Year's should be detected as holiday")
-        self.assertTrue(is_market_holiday(christmas), "Christmas should be detected as holiday")
-        self.assertFalse(is_market_holiday(regular_day), "Regular day should not be detected as holiday")
-    
+        self.assertTrue(is_market_holiday(christmas), 'Christmas should be detected as holiday')
+        self.assertFalse(is_market_holiday(regular_day), 'Regular day should not be detected as holiday')
+
     def test_meta_learning_price_validation(self):
         """Test 2: MetaLearning Data Validation - Price validation logic."""
-        # Mock pandas for testing
         try:
             import pandas as pd
-            
-            # Test data with mixed price types
-            test_data = {
-                'entry_price': ['100.50', '200', 'invalid', '50.25'],
-                'exit_price': ['105.75', '195', '0', '55.00'],
-                'side': ['buy', 'sell', 'buy', 'sell'],
-                'signal_tags': ['momentum', 'mean_revert', 'momentum', 'trend']
-            }
+            test_data = {'entry_price': ['100.50', '200', 'invalid', '50.25'], 'exit_price': ['105.75', '195', '0', '55.00'], 'side': ['buy', 'sell', 'buy', 'sell'], 'signal_tags': ['momentum', 'mean_revert', 'momentum', 'trend']}
             df = pd.DataFrame(test_data)
-            
-            # Apply the validation logic from meta_learning.py
-            df["entry_price"] = pd.to_numeric(df["entry_price"], errors="coerce")
-            df["exit_price"] = pd.to_numeric(df["exit_price"], errors="coerce")
-            df = df.dropna(subset=["entry_price", "exit_price"])
-            
-            # Filter out non-positive prices
-            df = df[(df["entry_price"] > 0) & (df["exit_price"] > 0)]
-            
-            # Should have 2 valid rows (first and last)
-            self.assertEqual(len(df), 2, "Should have 2 rows with valid positive prices")
-            self.assertTrue(all(df["entry_price"] > 0), "All entry prices should be positive")
-            self.assertTrue(all(df["exit_price"] > 0), "All exit prices should be positive")
-            
+            df['entry_price'] = pd.to_numeric(df['entry_price'], errors='coerce')
+            df['exit_price'] = pd.to_numeric(df['exit_price'], errors='coerce')
+            df = df.dropna(subset=['entry_price', 'exit_price'])
+            df = df[(df['entry_price'] > 0) & (df['exit_price'] > 0)]
+            self.assertEqual(len(df), 2, 'Should have 2 rows with valid positive prices')
+            self.assertTrue(all(df['entry_price'] > 0), 'All entry prices should be positive')
+            self.assertTrue(all(df['exit_price'] > 0), 'All exit prices should be positive')
         except ImportError:
-            # Skip if pandas not available
-            self.skipTest("pandas not available for meta learning test")
-    
+            self.skipTest('pandas not available for meta learning test')
+
     def test_systemd_service_configuration(self):
         """Test 3: Service Configuration - systemd service file."""
-        service_file = "/home/runner/work/ai-trading-bot/ai-trading-bot/ai-trading-bot.service"
-        self.assertTrue(os.path.exists(service_file), "systemd service file should exist")
-        
+        service_file = '/home/runner/work/ai-trading-bot/ai-trading-bot/ai-trading-bot.service'
+        self.assertTrue(os.path.exists(service_file), 'systemd service file should exist')
         with open(service_file, 'r') as f:
             content = f.read()
-        
-        # Check key configuration elements
-        self.assertIn("User=aiuser", content, "Service should run as aiuser")
-        self.assertIn("Group=aiuser", content, "Service should run as aiuser group")
-        self.assertIn("WorkingDirectory=/home/aiuser/ai-trading-bot", content, 
-                     "Should have correct working directory")
-        self.assertIn("NoNewPrivileges=true", content, "Should have security restrictions")
-        self.assertIn("ProtectSystem=strict", content, "Should protect system")
-        self.assertIn("Restart=always", content, "Should restart on failure")
-    
+        self.assertIn('User=aiuser', content, 'Service should run as aiuser')
+        self.assertIn('Group=aiuser', content, 'Service should run as aiuser group')
+        self.assertIn('WorkingDirectory=/home/aiuser/ai-trading-bot', content, 'Should have correct working directory')
+        self.assertIn('NoNewPrivileges=true', content, 'Should have security restrictions')
+        self.assertIn('ProtectSystem=strict', content, 'Should protect system')
+        self.assertIn('Restart=always', content, 'Should restart on failure')
+
     def test_error_handling_robustness(self):
         """Test 5: General Robustness - Error handling patterns."""
-        # Test that we have proper exception handling patterns
-        
-        # Example of how sentiment fallback should work
+
         def mock_sentiment_fallback(cached_data, default_score=0.0):
             """Mock sentiment fallback logic."""
             try:
-                if cached_data and isinstance(cached_data, (list, tuple)) and len(cached_data) > 0:
-                    return cached_data[-1]  # Use last cached value
+                if cached_data and isinstance(cached_data, (list, tuple)) and (len(cached_data) > 0):
+                    return cached_data[-1]
                 return default_score
             except (TypeError, IndexError, AttributeError):
-                return default_score  # Always return safe default
-        
-        # Test fallback scenarios
-        self.assertEqual(mock_sentiment_fallback(None), 0.0, "Should return neutral when no cache")
-        self.assertEqual(mock_sentiment_fallback([]), 0.0, "Should return neutral when empty cache")
-        self.assertEqual(mock_sentiment_fallback([0.5, 0.7]), 0.7, "Should return last cached value")
-        self.assertEqual(mock_sentiment_fallback("invalid"), 0.0, "Should handle invalid data gracefully")
-    
+                return default_score
+        self.assertEqual(mock_sentiment_fallback(None), 0.0, 'Should return neutral when no cache')
+        self.assertEqual(mock_sentiment_fallback([]), 0.0, 'Should return neutral when empty cache')
+        self.assertEqual(mock_sentiment_fallback([0.5, 0.7]), 0.7, 'Should return last cached value')
+        self.assertEqual(mock_sentiment_fallback('invalid'), 0.0, 'Should handle invalid data gracefully')
+
     def test_cache_behavior(self):
         """Test enhanced caching behavior."""
         import time
-        
-        # Mock cache structure
         cache = {}
-        normal_ttl = 600  # 10 minutes
-        extended_ttl = 3600  # 1 hour
-        
+        normal_ttl = 600
+        extended_ttl = 3600
+
         def is_cache_valid(cache_entry, ttl):
             if not cache_entry:
                 return False
             timestamp, value = cache_entry
             return time.time() - timestamp < ttl
-        
-        # Test normal cache behavior
         now = time.time()
-        cache["AAPL"] = (now - 300, 0.5)  # 5 minutes old
-        
-        self.assertTrue(is_cache_valid(cache["AAPL"], normal_ttl), 
-                       "Recent cache should be valid with normal TTL")
-        
-        # Test extended cache during rate limiting
-        cache["MSFT"] = (now - 1800, 0.3)  # 30 minutes old
-        
-        self.assertFalse(is_cache_valid(cache["MSFT"], normal_ttl),
-                        "Old cache should be invalid with normal TTL")
-        self.assertTrue(is_cache_valid(cache["MSFT"], extended_ttl),
-                       "Old cache should be valid with extended TTL")
-
+        cache['AAPL'] = (now - 300, 0.5)
+        self.assertTrue(is_cache_valid(cache['AAPL'], normal_ttl), 'Recent cache should be valid with normal TTL')
+        cache['MSFT'] = (now - 1800, 0.3)
+        self.assertFalse(is_cache_valid(cache['MSFT'], normal_ttl), 'Old cache should be invalid with normal TTL')
+        self.assertTrue(is_cache_valid(cache['MSFT'], extended_ttl), 'Old cache should be valid with extended TTL')
 if __name__ == '__main__':
-    # Run the tests
-    print("Running Critical Fixes Validation Tests...")
-    print("=" * 60)
-    
-    # Set up the test environment
+    print('Running Critical Fixes Validation Tests...')
+    print('=' * 60)
     loader = unittest.TestLoader()
     suite = loader.loadTestsFromTestCase(TestCriticalFixes)
     runner = unittest.TextTestRunner(verbosity=2)
     result = runner.run(suite)
-    
-    # Report results
-    print("\n" + "=" * 60)
+    print('\n' + '=' * 60)
     if result.wasSuccessful():
-        print("🎉 All critical fixes validation tests PASSED!")
-        print(f"   Ran {result.testsRun} tests successfully")
+        print('🎉 All critical fixes validation tests PASSED!')
+        print(f'   Ran {result.testsRun} tests successfully')
     else:
-        print("❌ Some tests FAILED!")
-        print(f"   Failures: {len(result.failures)}")
-        print(f"   Errors: {len(result.errors)}")
+        print('❌ Some tests FAILED!')
+        print(f'   Failures: {len(result.failures)}')
+        print(f'   Errors: {len(result.errors)}')
         sys.exit(1)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_fill_rate_calculation_fix.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_fill_rate_calculation_fix.py@@ -1,158 +1,60 @@ """Test fill rate calculation fixes and alert thresholds."""
-
 import pytest
 import os
 from unittest.mock import MagicMock, patch
-
-# Ensure test environment
-os.environ.update({
-    'ALPACA_API_KEY': 'test_key',
-    'ALPACA_SECRET_KEY': 'test_secret', 
-    'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets',
-    'WEBHOOK_SECRET': 'test_webhook',
-    'FLASK_PORT': '5000'
-})
-
+os.environ.update({'ALPACA_API_KEY': 'test_key', 'ALPACA_SECRET_KEY': 'test_secret', 'ALPACA_BASE_URL': 'https://paper-api.alpaca.markets', 'WEBHOOK_SECRET': 'test_webhook', 'FLASK_PORT': '5000'})
 from ai_trading import ExecutionEngine
-
-
-class MockOrder:
-    """Mock order object to simulate Alpaca order responses."""
-    
-    def __init__(self, filled_qty=None, status="filled", order_id="test-order-123"):
-        self.filled_qty = filled_qty
-        self.status = status
-        self.id = order_id
-        self.symbol = "TEST"
-
-
-class MockContext:
-    """Mock trading context for testing."""
-    
-    def __init__(self):
-        self.api = MagicMock()
-        self.data_client = MagicMock()
-        self.data_fetcher = MagicMock()
-        self.capital_band = "small"
-
 
 @pytest.mark.smoke
 def test_fill_rate_calculation_fix():
     """Test that fill rate calculation now works correctly when order.filled_qty is None."""
-    
     ctx = MockContext()
     engine = ExecutionEngine(ctx)
-    
     with patch.object(engine, 'logger') as mock_logger:
-        
-        # Test Case: Order with filled_qty=None (the bug condition that was fixed)
         order_without_filled_qty = MockOrder(filled_qty=None)
-        
-        engine._reconcile_partial_fills(
-            symbol="QQQ",
-            submitted_qty=100,
-            remaining_qty=50,  # 50 remaining = 50 filled out of 100
-            side="buy", 
-            last_order=order_without_filled_qty
-        )
-        
+        engine._reconcile_partial_fills(symbol='QQQ', submitted_qty=100, remaining_qty=50, side='buy', last_order=order_without_filled_qty)
         logged_calls = mock_logger.warning.call_args_list
-        partial_fill_logs = [call for call in logged_calls if call[0][0] == "PARTIAL_FILL_DETECTED"]
-        
-        assert len(partial_fill_logs) > 0, "Should have logged partial fill"
-        
+        partial_fill_logs = [call for call in logged_calls if call[0][0] == 'PARTIAL_FILL_DETECTED']
+        assert len(partial_fill_logs) > 0, 'Should have logged partial fill'
         log_extra = partial_fill_logs[0][1]['extra']
-        
-        # This should now be 50 filled out of 100 = 50%
         assert log_extra['filled_qty'] == 50, f"Expected filled_qty=50, got {log_extra['filled_qty']}"
         assert log_extra['fill_rate_pct'] == 50.0, f"Expected 50% fill rate, got {log_extra['fill_rate_pct']}"
 
-
-@pytest.mark.smoke 
+@pytest.mark.smoke
 def test_fill_rate_alert_thresholds_updated():
     """Test that fill rate alert thresholds are now more realistic for market conditions."""
-    
     ctx = MockContext()
     engine = ExecutionEngine(ctx)
-    
     with patch.object(engine, 'logger') as mock_logger:
-        
-        # Test 50% fill rate - should NOT trigger any error alerts now
         order_50pct = MockOrder(filled_qty=25)
-        
-        engine._reconcile_partial_fills(
-            symbol="SPY",
-            submitted_qty=50,
-            remaining_qty=25,
-            side="buy",
-            last_order=order_50pct
-        )
-        
+        engine._reconcile_partial_fills(symbol='SPY', submitted_qty=50, remaining_qty=25, side='buy', last_order=order_50pct)
         error_calls = mock_logger.error.call_args_list
-        low_fill_alerts = [call for call in error_calls if call[0][0] == "LOW_FILL_RATE_ALERT"]
-        
-        assert len(low_fill_alerts) == 0, "50% fill rate should not trigger LOW_FILL_RATE_ALERT"
-        
-        # Test 30% fill rate - should trigger moderate warning but not error
+        low_fill_alerts = [call for call in error_calls if call[0][0] == 'LOW_FILL_RATE_ALERT']
+        assert len(low_fill_alerts) == 0, '50% fill rate should not trigger LOW_FILL_RATE_ALERT'
         mock_logger.reset_mock()
         order_30pct = MockOrder(filled_qty=15)
-        
-        engine._reconcile_partial_fills(
-            symbol="AMZN",
-            submitted_qty=50, 
-            remaining_qty=35,
-            side="buy",
-            last_order=order_30pct
-        )
-        
+        engine._reconcile_partial_fills(symbol='AMZN', submitted_qty=50, remaining_qty=35, side='buy', last_order=order_30pct)
         warning_calls = mock_logger.warning.call_args_list
-        moderate_alerts = [call for call in warning_calls if any("MODERATE_FILL_RATE_ALERT" in str(arg) for arg in call)]
-        
-        assert len(moderate_alerts) > 0, "30% fill rate should trigger MODERATE_FILL_RATE_ALERT"
-        
-        # Test 20% fill rate - should now trigger error-level alert
-        mock_logger.reset_mock() 
+        moderate_alerts = [call for call in warning_calls if any(('MODERATE_FILL_RATE_ALERT' in str(arg) for arg in call))]
+        assert len(moderate_alerts) > 0, '30% fill rate should trigger MODERATE_FILL_RATE_ALERT'
+        mock_logger.reset_mock()
         order_20pct = MockOrder(filled_qty=10)
-        
-        engine._reconcile_partial_fills(
-            symbol="MSFT",
-            submitted_qty=50,
-            remaining_qty=40,
-            side="buy", 
-            last_order=order_20pct
-        )
-        
+        engine._reconcile_partial_fills(symbol='MSFT', submitted_qty=50, remaining_qty=40, side='buy', last_order=order_20pct)
         error_calls = mock_logger.error.call_args_list
-        low_fill_alerts = [call for call in error_calls if call[0][0] == "LOW_FILL_RATE_ALERT"]
-        
-        assert len(low_fill_alerts) > 0, "20% fill rate should trigger LOW_FILL_RATE_ALERT"
-
+        low_fill_alerts = [call for call in error_calls if call[0][0] == 'LOW_FILL_RATE_ALERT']
+        assert len(low_fill_alerts) > 0, '20% fill rate should trigger LOW_FILL_RATE_ALERT'
 
 @pytest.mark.smoke
 def test_fill_rate_calculation_with_valid_order_data():
     """Test that fill rate calculation still works when order.filled_qty is properly set."""
-    
     ctx = MockContext()
     engine = ExecutionEngine(ctx)
-    
     with patch.object(engine, 'logger') as mock_logger:
-        
-        # Test Case: Order with valid filled_qty
-        order_with_filled_qty = MockOrder(filled_qty=75)  # 75 out of 100 requested
-        
-        engine._reconcile_partial_fills(
-            symbol="TSLA", 
-            submitted_qty=100,
-            remaining_qty=25,  # This should be ignored since order has filled_qty
-            side="buy",
-            last_order=order_with_filled_qty
-        )
-        
+        order_with_filled_qty = MockOrder(filled_qty=75)
+        engine._reconcile_partial_fills(symbol='TSLA', submitted_qty=100, remaining_qty=25, side='buy', last_order=order_with_filled_qty)
         logged_calls = mock_logger.warning.call_args_list
-        partial_fill_logs = [call for call in logged_calls if call[0][0] == "PARTIAL_FILL_DETECTED"]
-        
+        partial_fill_logs = [call for call in logged_calls if call[0][0] == 'PARTIAL_FILL_DETECTED']
         if partial_fill_logs:
             log_extra = partial_fill_logs[0][1]['extra']
-            # Should use order.filled_qty (75) not calculated value (75)
             assert log_extra['filled_qty'] == 75, f"Expected filled_qty=75, got {log_extra['filled_qty']}"
             assert log_extra['fill_rate_pct'] == 75.0, f"Expected 75% fill rate, got {log_extra['fill_rate_pct']}"--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_intelligent_position_management.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_intelligent_position_management.py@@ -11,66 +11,33 @@ 
 AI-AGENT-REF: Comprehensive tests for intelligent position management
 """
-
 from unittest.mock import Mock
 from dataclasses import dataclass
-
-# Import the new position management components
 try:
-    from ai_trading.position import (
-        IntelligentPositionManager,
-        MarketRegimeDetector, MarketRegime,
-        TechnicalSignalAnalyzer, SignalStrength, DivergenceType,
-        TrailingStopManager, TrailingStopType,
-        ProfitTakingEngine, ProfitTakingStrategy,
-        PortfolioCorrelationAnalyzer, ConcentrationLevel
-    )
+    from ai_trading.position import IntelligentPositionManager, MarketRegimeDetector, MarketRegime, TechnicalSignalAnalyzer, SignalStrength, DivergenceType, TrailingStopManager, TrailingStopType, ProfitTakingEngine, ProfitTakingStrategy, PortfolioCorrelationAnalyzer, ConcentrationLevel
 except ImportError as e:
-    print(f"Import error: {e}")
-    print("Testing import fallback...")
-    
-    # Test basic imports
+    print(f'Import error: {e}')
+    print('Testing import fallback...')
     import sys
     sys.path.append('/home/runner/work/ai-trading-bot/ai-trading-bot')
-    
     from ai_trading.position.intelligent_manager import IntelligentPositionManager
-    from ai_trading.position.market_regime import MarketRegimeDetector, MarketRegime  
+    from ai_trading.position.market_regime import MarketRegimeDetector, MarketRegime
     from ai_trading.position.technical_analyzer import TechnicalSignalAnalyzer
     from ai_trading.position.trailing_stops import TrailingStopManager
     from ai_trading.position.profit_taking import ProfitTakingEngine
     from ai_trading.position.correlation_analyzer import PortfolioCorrelationAnalyzer, ConcentrationLevel
 
-
-@dataclass
-class MockPosition:
-    """Mock position data for testing."""
-    symbol: str
-    qty: int
-    avg_entry_price: float
-    market_value: float
-
-
 class TestIntelligentPositionManager:
     """Test the main IntelligentPositionManager orchestrator."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.mock_ctx.data_fetcher = Mock()
-        
-        # Create mock data
-        self.mock_daily_data = {
-            'close': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110],
-            'high': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111],
-            'low': [99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109],
-            'volume': [1000, 1100, 1200, 1000, 900, 1300, 1400, 1100, 1000, 1200, 1500]
-        }
-        
+        self.mock_daily_data = {'close': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], 'high': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111], 'low': [99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109], 'volume': [1000, 1100, 1200, 1000, 900, 1300, 1400, 1100, 1000, 1200, 1500]}
         self.mock_ctx.data_fetcher.get_daily_df.return_value = Mock()
         self.mock_ctx.data_fetcher.get_daily_df.return_value.empty = False
         self.mock_ctx.data_fetcher.get_daily_df.return_value.__len__ = lambda: 11
-        
-        # Mock data access
         for col, values in self.mock_daily_data.items():
             mock_series = Mock()
             mock_series.__len__ = lambda: len(values)
@@ -79,9 +46,8 @@             mock_series.tail.return_value = Mock()
             mock_series.tail.return_value.tolist.return_value = values[-5:]
             setattr(self.mock_ctx.data_fetcher.get_daily_df.return_value, col, mock_series)
-        
         self.manager = IntelligentPositionManager(self.mock_ctx)
-    
+
     def test_initialization(self):
         """Test that IntelligentPositionManager initializes correctly."""
         assert self.manager.ctx == self.mock_ctx
@@ -90,393 +56,234 @@         assert isinstance(self.manager.trailing_stop_manager, TrailingStopManager)
         assert isinstance(self.manager.profit_taking_engine, ProfitTakingEngine)
         assert isinstance(self.manager.correlation_analyzer, PortfolioCorrelationAnalyzer)
-    
+
     def test_should_hold_position_integration(self):
         """Test the enhanced should_hold_position method."""
-        # Create mock position
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Test with profitable position
-        result = self.manager.should_hold_position(
-            symbol='AAPL',
-            current_position=position,
-            unrealized_pnl_pct=10.0,
-            days_held=5,
-            current_positions=[position]
-        )
-        
-        # Should return a boolean
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
+        result = self.manager.should_hold_position(symbol='AAPL', current_position=position, unrealized_pnl_pct=10.0, days_held=5, current_positions=[position])
         assert isinstance(result, bool)
-    
+
     def test_analyze_position_basic(self):
         """Test basic position analysis."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Mock current price
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         self.mock_ctx.data_fetcher.get_minute_df.return_value = None
         close_series = Mock()
         close_series.iloc = [-1]
         close_series.__getitem__ = lambda x: 110.0 if x == -1 else 100.0
         self.mock_ctx.data_fetcher.get_daily_df.return_value.__getitem__ = lambda x: close_series if x == 'close' else Mock()
-        
         recommendation = self.manager.analyze_position('AAPL', position, [position])
-        
-        # Should return a recommendation
         assert hasattr(recommendation, 'symbol')
         assert hasattr(recommendation, 'action')
         assert hasattr(recommendation, 'confidence')
         assert recommendation.symbol == 'AAPL'
 
-
 class TestMarketRegimeDetector:
     """Test market regime detection functionality."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.detector = MarketRegimeDetector(self.mock_ctx)
-    
+
     def test_regime_classification(self):
         """Test regime classification logic."""
-        # Test trending bull classification
         trend_metrics = {'strength': 0.8, 'direction': 0.5}
         vol_metrics = {'percentile': 50.0}
         momentum_metrics = {'score': 0.8}
         mean_reversion_metrics = {'score': 0.3}
-        
-        regime = self.detector._classify_regime(
-            trend_metrics, vol_metrics, momentum_metrics, mean_reversion_metrics
-        )
-        
+        regime = self.detector._classify_regime(trend_metrics, vol_metrics, momentum_metrics, mean_reversion_metrics)
         assert regime == MarketRegime.TRENDING_BULL
-    
+
     def test_regime_parameters(self):
         """Test regime-specific parameters."""
         params = self.detector.get_regime_parameters(MarketRegime.TRENDING_BULL)
-        
         assert 'stop_distance_multiplier' in params
         assert 'profit_taking_patience' in params
-        assert params['profit_taking_patience'] > 1.0  # Should be patient in bull trends
-    
+        assert params['profit_taking_patience'] > 1.0
+
     def test_high_volatility_regime(self):
         """Test high volatility regime detection."""
         trend_metrics = {'strength': 0.3, 'direction': 0.1}
-        vol_metrics = {'percentile': 85.0}  # High volatility
+        vol_metrics = {'percentile': 85.0}
         momentum_metrics = {'score': 0.5}
         mean_reversion_metrics = {'score': 0.5}
-        
-        regime = self.detector._classify_regime(
-            trend_metrics, vol_metrics, momentum_metrics, mean_reversion_metrics
-        )
-        
+        regime = self.detector._classify_regime(trend_metrics, vol_metrics, momentum_metrics, mean_reversion_metrics)
         assert regime == MarketRegime.HIGH_VOLATILITY
-
 
 class TestTechnicalSignalAnalyzer:
     """Test technical signal analysis."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.analyzer = TechnicalSignalAnalyzer(self.mock_ctx)
-    
+
     def test_rsi_calculation(self):
         """Test RSI calculation."""
-        # Create mock price series
         prices = Mock()
         prices.__len__ = lambda: 20
         prices.diff.return_value = Mock()
         prices.diff.return_value.where = Mock(return_value=Mock())
-        
-        # Mock rolling calculations
         mock_rolling = Mock()
         mock_rolling.mean.return_value = Mock()
         mock_rolling.mean.return_value.iloc = [-1]
         mock_rolling.mean.return_value.__getitem__ = lambda x: 2.0 if x == -1 else 1.0
-        
         prices.diff.return_value.where.return_value.rolling = Mock(return_value=mock_rolling)
-        
-        # Should not crash and return reasonable RSI
         rsi = self.analyzer._calculate_rsi(prices, 14)
         assert isinstance(rsi, float)
         assert 0 <= rsi <= 100
-    
+
     def test_divergence_detection(self):
         """Test momentum divergence detection."""
-        # Create mock data with divergence pattern
         mock_data = Mock()
         mock_data.__len__ = lambda: 20
-        
         close_series = Mock()
         close_series.__len__ = lambda: 20
         close_series.tail.return_value = Mock()
-        close_series.tail.return_value.tolist.return_value = [100, 101, 102, 103, 104]  # Rising prices
+        close_series.tail.return_value.tolist.return_value = [100, 101, 102, 103, 104]
         close_series.iloc = Mock()
-        
         mock_data.__getitem__ = lambda x: close_series if x == 'close' else Mock()
-        
         result = self.analyzer._analyze_divergence(mock_data)
-        
         assert 'type' in result
         assert 'strength' in result
 
-
 class TestTrailingStopManager:
     """Test dynamic trailing stop functionality."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.manager = TrailingStopManager(self.mock_ctx)
-    
+
     def test_stop_initialization(self):
         """Test trailing stop initialization."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         stop_level = self.manager.update_trailing_stop('AAPL', position, 110.0)
-        
         assert stop_level is not None
         assert stop_level.symbol == 'AAPL'
         assert stop_level.current_price == 110.0
-        assert stop_level.stop_price < 110.0  # Stop should be below current price
-    
+        assert stop_level.stop_price < 110.0
+
     def test_stop_movement(self):
         """Test that stops move up with price for long positions."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Initialize stop
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         stop1 = self.manager.update_trailing_stop('AAPL', position, 110.0)
         initial_stop = stop1.stop_price
-        
-        # Price moves higher
         stop2 = self.manager.update_trailing_stop('AAPL', position, 115.0)
-        
-        # Stop should move up
         assert stop2.stop_price >= initial_stop
         assert stop2.current_price == 115.0
-    
+
     def test_stop_trigger_detection(self):
         """Test stop trigger detection."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Initialize stop at high price
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         self.manager.update_trailing_stop('AAPL', position, 115.0)
-        
-        # Price falls below stop
         stop_level = self.manager.update_trailing_stop('AAPL', position, 105.0)
-        
-        # Should detect trigger
         assert stop_level.is_triggered
-
 
 class TestProfitTakingEngine:
     """Test multi-tiered profit taking."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.engine = ProfitTakingEngine(self.mock_ctx)
-    
+
     def test_profit_plan_creation(self):
         """Test profit plan creation."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Mock current price
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         self.mock_ctx.data_fetcher.get_minute_df.return_value = None
         self.mock_ctx.data_fetcher.get_daily_df.return_value = None
-        
-        plan = self.engine.create_profit_plan('AAPL', position, 100.0, 300.0)  # $3 risk per share
-        
-        if plan:  # Plan creation might fail due to mocking
+        plan = self.engine.create_profit_plan('AAPL', position, 100.0, 300.0)
+        if plan:
             assert plan.symbol == 'AAPL'
             assert plan.entry_price == 100.0
             assert len(plan.targets) > 0
-    
+
     def test_target_triggering(self):
         """Test profit target triggering."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=11000.0
-        )
-        
-        # Create plan
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=11000.0)
         plan = self.engine.create_profit_plan('AAPL', position, 100.0, 300.0)
-        
         if plan:
-            # Simulate price increase
             triggered_targets = self.engine.update_profit_plan('AAPL', 110.0, position)
-            
-            # Should return list of triggered targets
             assert isinstance(triggered_targets, list)
-
 
 class TestPortfolioCorrelationAnalyzer:
     """Test portfolio correlation analysis."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.analyzer = PortfolioCorrelationAnalyzer(self.mock_ctx)
-    
+
     def test_position_data_extraction(self):
         """Test position data extraction."""
-        positions = [
-            MockPosition('AAPL', 100, 100.0, 11000.0),
-            MockPosition('MSFT', 50, 200.0, 10500.0),
-            MockPosition('GOOGL', 25, 150.0, 3750.0)
-        ]
-        
+        positions = [MockPosition('AAPL', 100, 100.0, 11000.0), MockPosition('MSFT', 50, 200.0, 10500.0), MockPosition('GOOGL', 25, 150.0, 3750.0)]
         position_data = self.analyzer._extract_position_data(positions)
-        
         assert len(position_data) == 3
         assert 'AAPL' in position_data
         assert position_data['AAPL']['market_value'] == 11000.0
-    
+
     def test_sector_classification(self):
         """Test sector classification."""
         sector = self.analyzer._get_symbol_sector('AAPL')
         assert sector == 'Technology'
-        
         sector = self.analyzer._get_symbol_sector('JPM')
         assert sector == 'Financials'
-    
+
     def test_concentration_analysis(self):
         """Test concentration level analysis."""
-        positions = [
-            MockPosition('AAPL', 100, 100.0, 50000.0),  # 50% of portfolio
-            MockPosition('MSFT', 50, 200.0, 25000.0),   # 25% of portfolio
-            MockPosition('GOOGL', 25, 150.0, 25000.0)   # 25% of portfolio
-        ]
-        
+        positions = [MockPosition('AAPL', 100, 100.0, 50000.0), MockPosition('MSFT', 50, 200.0, 25000.0), MockPosition('GOOGL', 25, 150.0, 25000.0)]
         analysis = self.analyzer.analyze_portfolio(positions)
-        
         assert analysis.total_positions == 3
         assert analysis.total_value == 100000.0
         assert analysis.concentration_level in [ConcentrationLevel.HIGH, ConcentrationLevel.EXTREME]
 
-
 class TestIntegrationScenarios:
     """Test integrated scenarios combining multiple components."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.mock_ctx = Mock()
         self.manager = IntelligentPositionManager(self.mock_ctx)
-    
+
     def test_profitable_position_scenario(self):
         """Test scenario with profitable position."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=12000.0  # 20% gain
-        )
-        
-        # Should lean towards holding profitable position
-        result = self.manager.should_hold_position(
-            'AAPL', position, 20.0, 10, [position]
-        )
-        
-        # With 20% gain and 10 days held, should generally hold
-        # (exact result depends on market conditions)
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=12000.0)
+        result = self.manager.should_hold_position('AAPL', position, 20.0, 10, [position])
         assert isinstance(result, bool)
-    
+
     def test_loss_position_scenario(self):
         """Test scenario with losing position."""
-        position = MockPosition(
-            symbol='AAPL',
-            qty=100,
-            avg_entry_price=100.0,
-            market_value=9000.0  # 10% loss
-        )
-        
-        result = self.manager.should_hold_position(
-            'AAPL', position, -10.0, 2, [position]
-        )
-        
-        # Should consider holding if only held for 2 days (min hold period)
+        position = MockPosition(symbol='AAPL', qty=100, avg_entry_price=100.0, market_value=9000.0)
+        result = self.manager.should_hold_position('AAPL', position, -10.0, 2, [position])
         assert isinstance(result, bool)
-    
+
     def test_portfolio_level_recommendations(self):
         """Test portfolio-level analysis and recommendations."""
-        positions = [
-            MockPosition('AAPL', 100, 100.0, 11000.0),
-            MockPosition('MSFT', 50, 200.0, 10500.0),
-            MockPosition('TSLA', 30, 150.0, 4800.0)
-        ]
-        
+        positions = [MockPosition('AAPL', 100, 100.0, 11000.0), MockPosition('MSFT', 50, 200.0, 10500.0), MockPosition('TSLA', 30, 150.0, 4800.0)]
         recommendations = self.manager.get_portfolio_recommendations(positions)
-        
-        # Should return recommendations for all positions
         assert isinstance(recommendations, list)
-        # Could be empty if mocked data doesn't trigger any actions
-
 
 def test_logging_configuration():
     """Test that logging is properly configured."""
-    # Components should use appropriate loggers
     manager = IntelligentPositionManager()
-    assert manager.logger.name.endswith("IntelligentPositionManager")
-    
+    assert manager.logger.name.endswith('IntelligentPositionManager')
     detector = MarketRegimeDetector()
-    assert detector.logger.name.endswith("MarketRegimeDetector")
-
-
-if __name__ == "__main__":
-    # Run tests manually if pytest not available
+    assert detector.logger.name.endswith('MarketRegimeDetector')
+if __name__ == '__main__':
     test_logging_configuration()
-    
-    # Basic smoke tests
     manager = IntelligentPositionManager()
-    print("✓ IntelligentPositionManager initialized successfully")
-    
+    print('✓ IntelligentPositionManager initialized successfully')
     detector = MarketRegimeDetector()
     regime_params = detector.get_regime_parameters(MarketRegime.TRENDING_BULL)
-    print(f"✓ Regime parameters: {regime_params}")
-    
+    print(f'✓ Regime parameters: {regime_params}')
     analyzer = TechnicalSignalAnalyzer()
-    print("✓ TechnicalSignalAnalyzer initialized successfully")
-    
+    print('✓ TechnicalSignalAnalyzer initialized successfully')
     trail_manager = TrailingStopManager()
-    print("✓ TrailingStopManager initialized successfully")
-    
+    print('✓ TrailingStopManager initialized successfully')
     profit_engine = ProfitTakingEngine()
-    print("✓ ProfitTakingEngine initialized successfully")
-    
+    print('✓ ProfitTakingEngine initialized successfully')
     corr_analyzer = PortfolioCorrelationAnalyzer()
-    print("✓ PortfolioCorrelationAnalyzer initialized successfully")
-    
-    print("\n✅ All advanced position management components initialized successfully!")
-    print("🎯 Ready for intelligent position holding strategies implementation!")+    print('✓ PortfolioCorrelationAnalyzer initialized successfully')
+    print('\n✅ All advanced position management components initialized successfully!')
+    print('🎯 Ready for intelligent position holding strategies implementation!')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_kelly_confidence_fix.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_kelly_confidence_fix.py@@ -6,110 +6,55 @@ """
 import pytest
 import math
-
-# Test the actual import and function from bot_engine
 import sys
 import os
 sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
 
 def test_kelly_confidence_normalization():
     """Test that high confidence values are properly normalized to probabilities."""
-    # Mock BotContext for testing
-    class MockBotContext:
-        def __init__(self):
-            self.kelly_fraction = 0.25
-            self.max_position_dollars = 10000
-            
-            # Mock capital scaler
-            class MockScaler:
-                def compression_factor(self, balance):
-                    return 1.0
-            self.capital_scaler = MockScaler()
-    
-    # Import the actual function (if available)
     try:
         from ai_trading.bot_engine import fractional_kelly_size
-        
         ctx = MockBotContext()
         balance = 10000.0
         price = 100.0
         atr = 2.0
-        
-        # Test cases that previously caused errors
-        problematic_confidences = [
-            3.315653439025116,
-            3.0650464264275152,
-            5.0,
-            2.5,
-        ]
-        
+        problematic_confidences = [3.315653439025116, 3.0650464264275152, 5.0, 2.5]
         for confidence in problematic_confidences:
-            # This should not raise an error and should return a valid position size
             result = fractional_kelly_size(ctx, balance, price, atr, confidence)
-            
-            # Verify result is reasonable
-            assert isinstance(result, int), f"Result should be integer, got {type(result)}"
-            assert result >= 0, f"Position size should be non-negative, got {result}"
-            assert result < 1000, f"Position size should be reasonable, got {result}"
-            
-        # Test edge cases
+            assert isinstance(result, int), f'Result should be integer, got {type(result)}'
+            assert result >= 0, f'Position size should be non-negative, got {result}'
+            assert result < 1000, f'Position size should be reasonable, got {result}'
         assert fractional_kelly_size(ctx, balance, price, atr, 0.0) >= 0
         assert fractional_kelly_size(ctx, balance, price, atr, 1.0) >= 0
         assert fractional_kelly_size(ctx, balance, price, atr, -0.5) >= 0
-        
     except ImportError:
-        # If we can't import, at least test our normalization logic
+
         def sigmoid_normalize(value):
             if value > 1.0:
                 return 1.0 / (1.0 + math.exp(-value + 1.0))
             elif value < 0:
                 return 0.0
             return value
-        
         test_values = [3.315653439025116, 3.0650464264275152, 5.0, 1.0, 0.5, 0.0, -0.5]
-        
         for value in test_values:
             normalized = sigmoid_normalize(value)
-            assert 0.0 <= normalized <= 1.0, f"Normalized value {normalized} should be in [0,1]"
-            
-            # Values > 1 should be mapped to something > 0.5
+            assert 0.0 <= normalized <= 1.0, f'Normalized value {normalized} should be in [0,1]'
             if value > 1.0:
-                assert normalized > 0.5, f"High confidence {value} should map to high probability"
-
+                assert normalized > 0.5, f'High confidence {value} should map to high probability'
 
 def test_kelly_input_validation():
     """Test that Kelly calculation properly validates all inputs."""
-    # Mock BotContext for testing
-    class MockBotContext:
-        def __init__(self):
-            self.kelly_fraction = 0.25
-            self.max_position_dollars = 10000
-            
-            class MockScaler:
-                def compression_factor(self, balance):
-                    return 1.0
-            self.capital_scaler = MockScaler()
-    
     try:
         from ai_trading.bot_engine import fractional_kelly_size
-        
         ctx = MockBotContext()
-        
-        # Test invalid inputs return 0 or minimal position
-        assert fractional_kelly_size(ctx, -1000, 100, 2.0, 0.6) == 0  # negative balance
-        assert fractional_kelly_size(ctx, 1000, -100, 2.0, 0.6) == 0  # negative price
-        assert fractional_kelly_size(ctx, 1000, 0, 2.0, 0.6) == 0     # zero price
-        
-        # Test that valid inputs work
+        assert fractional_kelly_size(ctx, -1000, 100, 2.0, 0.6) == 0
+        assert fractional_kelly_size(ctx, 1000, -100, 2.0, 0.6) == 0
+        assert fractional_kelly_size(ctx, 1000, 0, 2.0, 0.6) == 0
         result = fractional_kelly_size(ctx, 1000, 100, 2.0, 0.6)
-        assert result > 0, "Valid inputs should produce positive position size"
-        
+        assert result > 0, 'Valid inputs should produce positive position size'
     except ImportError:
-        # Skip if we can't import the actual function
-        pytest.skip("Cannot import fractional_kelly_size function")
-
-
-if __name__ == "__main__":
+        pytest.skip('Cannot import fractional_kelly_size function')
+if __name__ == '__main__':
     test_kelly_confidence_normalization()
     test_kelly_input_validation()
-    print("✅ All Kelly confidence fix tests passed!")+    print('✅ All Kelly confidence fix tests passed!')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_parameter_optimization.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_parameter_optimization.py@@ -4,152 +4,91 @@ Validates that optimized parameters maintain safety standards while
 improving profit potential.
 """
-
 import pytest
 import sys
 import os
-
-# Add the project root to Python path for imports
 sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
 
 def test_kelly_parameters_optimization():
     """Test that Kelly parameters are optimized correctly."""
     from ai_trading.core.constants import KELLY_PARAMETERS
-    
-    # Verify optimized Kelly parameters
-    assert KELLY_PARAMETERS["MAX_KELLY_FRACTION"] == 0.15, f"Expected 0.15, got {KELLY_PARAMETERS['MAX_KELLY_FRACTION']}"
-    assert KELLY_PARAMETERS["MIN_SAMPLE_SIZE"] == 20, f"Expected 20, got {KELLY_PARAMETERS['MIN_SAMPLE_SIZE']}"
-    assert KELLY_PARAMETERS["CONFIDENCE_LEVEL"] == 0.90, f"Expected 0.90, got {KELLY_PARAMETERS['CONFIDENCE_LEVEL']}"
-    
-    # Ensure parameters remain within safe bounds
-    assert 0.05 <= KELLY_PARAMETERS["MAX_KELLY_FRACTION"] <= 0.50, "Kelly fraction outside safe bounds"
-    assert 10 <= KELLY_PARAMETERS["MIN_SAMPLE_SIZE"] <= 100, "Sample size outside safe bounds"
-    assert 0.80 <= KELLY_PARAMETERS["CONFIDENCE_LEVEL"] <= 0.99, "Confidence level outside safe bounds"
-
+    assert KELLY_PARAMETERS['MAX_KELLY_FRACTION'] == 0.15, f"Expected 0.15, got {KELLY_PARAMETERS['MAX_KELLY_FRACTION']}"
+    assert KELLY_PARAMETERS['MIN_SAMPLE_SIZE'] == 20, f"Expected 20, got {KELLY_PARAMETERS['MIN_SAMPLE_SIZE']}"
+    assert KELLY_PARAMETERS['CONFIDENCE_LEVEL'] == 0.9, f"Expected 0.90, got {KELLY_PARAMETERS['CONFIDENCE_LEVEL']}"
+    assert 0.05 <= KELLY_PARAMETERS['MAX_KELLY_FRACTION'] <= 0.5, 'Kelly fraction outside safe bounds'
+    assert 10 <= KELLY_PARAMETERS['MIN_SAMPLE_SIZE'] <= 100, 'Sample size outside safe bounds'
+    assert 0.8 <= KELLY_PARAMETERS['CONFIDENCE_LEVEL'] <= 0.99, 'Confidence level outside safe bounds'
 
 def test_risk_parameters_optimization():
     """Test that risk parameters are optimized correctly."""
     from ai_trading.core.constants import RISK_PARAMETERS
-    
-    # Verify optimized risk parameters
-    assert RISK_PARAMETERS["MAX_PORTFOLIO_RISK"] == 0.025, f"Expected 0.025, got {RISK_PARAMETERS['MAX_PORTFOLIO_RISK']}"
-    assert RISK_PARAMETERS["MAX_POSITION_SIZE"] == 0.25, f"Expected 0.25, got {RISK_PARAMETERS['MAX_POSITION_SIZE']}"
-    assert RISK_PARAMETERS["STOP_LOSS_MULTIPLIER"] == 1.8, f"Expected 1.8, got {RISK_PARAMETERS['STOP_LOSS_MULTIPLIER']}"
-    assert RISK_PARAMETERS["TAKE_PROFIT_MULTIPLIER"] == 2.5, f"Expected 2.5, got {RISK_PARAMETERS['TAKE_PROFIT_MULTIPLIER']}"
-    assert RISK_PARAMETERS["MAX_CORRELATION_EXPOSURE"] == 0.15, f"Expected 0.15, got {RISK_PARAMETERS['MAX_CORRELATION_EXPOSURE']}"
-    
-    # Ensure parameters remain within safe bounds
-    assert 0.01 <= RISK_PARAMETERS["MAX_PORTFOLIO_RISK"] <= 0.05, "Portfolio risk outside safe bounds"
-    assert 0.05 <= RISK_PARAMETERS["MAX_POSITION_SIZE"] <= 0.30, "Position size outside safe bounds"
-    assert 1.0 <= RISK_PARAMETERS["STOP_LOSS_MULTIPLIER"] <= 3.0, "Stop loss multiplier outside safe bounds"
-    assert 1.5 <= RISK_PARAMETERS["TAKE_PROFIT_MULTIPLIER"] <= 5.0, "Take profit multiplier outside safe bounds"
-    assert 0.05 <= RISK_PARAMETERS["MAX_CORRELATION_EXPOSURE"] <= 0.30, "Correlation exposure outside safe bounds"
-
+    assert RISK_PARAMETERS['MAX_PORTFOLIO_RISK'] == 0.025, f"Expected 0.025, got {RISK_PARAMETERS['MAX_PORTFOLIO_RISK']}"
+    assert RISK_PARAMETERS['MAX_POSITION_SIZE'] == 0.25, f"Expected 0.25, got {RISK_PARAMETERS['MAX_POSITION_SIZE']}"
+    assert RISK_PARAMETERS['STOP_LOSS_MULTIPLIER'] == 1.8, f"Expected 1.8, got {RISK_PARAMETERS['STOP_LOSS_MULTIPLIER']}"
+    assert RISK_PARAMETERS['TAKE_PROFIT_MULTIPLIER'] == 2.5, f"Expected 2.5, got {RISK_PARAMETERS['TAKE_PROFIT_MULTIPLIER']}"
+    assert RISK_PARAMETERS['MAX_CORRELATION_EXPOSURE'] == 0.15, f"Expected 0.15, got {RISK_PARAMETERS['MAX_CORRELATION_EXPOSURE']}"
+    assert 0.01 <= RISK_PARAMETERS['MAX_PORTFOLIO_RISK'] <= 0.05, 'Portfolio risk outside safe bounds'
+    assert 0.05 <= RISK_PARAMETERS['MAX_POSITION_SIZE'] <= 0.3, 'Position size outside safe bounds'
+    assert 1.0 <= RISK_PARAMETERS['STOP_LOSS_MULTIPLIER'] <= 3.0, 'Stop loss multiplier outside safe bounds'
+    assert 1.5 <= RISK_PARAMETERS['TAKE_PROFIT_MULTIPLIER'] <= 5.0, 'Take profit multiplier outside safe bounds'
+    assert 0.05 <= RISK_PARAMETERS['MAX_CORRELATION_EXPOSURE'] <= 0.3, 'Correlation exposure outside safe bounds'
 
 def test_execution_parameters_optimization():
     """Test that execution parameters are optimized correctly."""
     from ai_trading.core.constants import EXECUTION_PARAMETERS
-    
-    # Verify optimized execution parameters
-    assert EXECUTION_PARAMETERS["PARTICIPATION_RATE"] == 0.15, f"Expected 0.15, got {EXECUTION_PARAMETERS['PARTICIPATION_RATE']}"
-    assert EXECUTION_PARAMETERS["MAX_SLIPPAGE_BPS"] == 15, f"Expected 15, got {EXECUTION_PARAMETERS['MAX_SLIPPAGE_BPS']}"
-    assert EXECUTION_PARAMETERS["ORDER_TIMEOUT_SECONDS"] == 180, f"Expected 180, got {EXECUTION_PARAMETERS['ORDER_TIMEOUT_SECONDS']}"
-    
-    # Ensure parameters remain within safe bounds
-    assert 0.05 <= EXECUTION_PARAMETERS["PARTICIPATION_RATE"] <= 0.25, "Participation rate outside safe bounds"
-    assert 5 <= EXECUTION_PARAMETERS["MAX_SLIPPAGE_BPS"] <= 50, "Slippage outside safe bounds"
-    assert 60 <= EXECUTION_PARAMETERS["ORDER_TIMEOUT_SECONDS"] <= 600, "Order timeout outside safe bounds"
-
+    assert EXECUTION_PARAMETERS['PARTICIPATION_RATE'] == 0.15, f"Expected 0.15, got {EXECUTION_PARAMETERS['PARTICIPATION_RATE']}"
+    assert EXECUTION_PARAMETERS['MAX_SLIPPAGE_BPS'] == 15, f"Expected 15, got {EXECUTION_PARAMETERS['MAX_SLIPPAGE_BPS']}"
+    assert EXECUTION_PARAMETERS['ORDER_TIMEOUT_SECONDS'] == 180, f"Expected 180, got {EXECUTION_PARAMETERS['ORDER_TIMEOUT_SECONDS']}"
+    assert 0.05 <= EXECUTION_PARAMETERS['PARTICIPATION_RATE'] <= 0.25, 'Participation rate outside safe bounds'
+    assert 5 <= EXECUTION_PARAMETERS['MAX_SLIPPAGE_BPS'] <= 50, 'Slippage outside safe bounds'
+    assert 60 <= EXECUTION_PARAMETERS['ORDER_TIMEOUT_SECONDS'] <= 600, 'Order timeout outside safe bounds'
 
 def test_performance_thresholds_optimization():
     """Test that performance thresholds are optimized correctly."""
     from ai_trading.core.constants import PERFORMANCE_THRESHOLDS
-    
-    # Verify optimized performance thresholds
-    assert PERFORMANCE_THRESHOLDS["MIN_SHARPE_RATIO"] == 1.2, f"Expected 1.2, got {PERFORMANCE_THRESHOLDS['MIN_SHARPE_RATIO']}"
-    assert PERFORMANCE_THRESHOLDS["MAX_DRAWDOWN"] == 0.15, f"Expected 0.15, got {PERFORMANCE_THRESHOLDS['MAX_DRAWDOWN']}"
-    assert PERFORMANCE_THRESHOLDS["MIN_WIN_RATE"] == 0.48, f"Expected 0.48, got {PERFORMANCE_THRESHOLDS['MIN_WIN_RATE']}"
-    
-    # Ensure parameters remain within safe bounds
-    assert 0.5 <= PERFORMANCE_THRESHOLDS["MIN_SHARPE_RATIO"] <= 2.0, "Sharpe ratio outside safe bounds"
-    assert 0.05 <= PERFORMANCE_THRESHOLDS["MAX_DRAWDOWN"] <= 0.30, "Drawdown outside safe bounds"
-    assert 0.30 <= PERFORMANCE_THRESHOLDS["MIN_WIN_RATE"] <= 0.70, "Win rate outside safe bounds"
-
+    assert PERFORMANCE_THRESHOLDS['MIN_SHARPE_RATIO'] == 1.2, f"Expected 1.2, got {PERFORMANCE_THRESHOLDS['MIN_SHARPE_RATIO']}"
+    assert PERFORMANCE_THRESHOLDS['MAX_DRAWDOWN'] == 0.15, f"Expected 0.15, got {PERFORMANCE_THRESHOLDS['MAX_DRAWDOWN']}"
+    assert PERFORMANCE_THRESHOLDS['MIN_WIN_RATE'] == 0.48, f"Expected 0.48, got {PERFORMANCE_THRESHOLDS['MIN_WIN_RATE']}"
+    assert 0.5 <= PERFORMANCE_THRESHOLDS['MIN_SHARPE_RATIO'] <= 2.0, 'Sharpe ratio outside safe bounds'
+    assert 0.05 <= PERFORMANCE_THRESHOLDS['MAX_DRAWDOWN'] <= 0.3, 'Drawdown outside safe bounds'
+    assert 0.3 <= PERFORMANCE_THRESHOLDS['MIN_WIN_RATE'] <= 0.7, 'Win rate outside safe bounds'
 
 def test_parameter_consistency():
     """Test that optimized parameters maintain internal consistency."""
     from ai_trading.core.constants import RISK_PARAMETERS, PERFORMANCE_THRESHOLDS
-    
-    # Stop loss should be lower than take profit
-    assert RISK_PARAMETERS["STOP_LOSS_MULTIPLIER"] < RISK_PARAMETERS["TAKE_PROFIT_MULTIPLIER"], \
-        "Stop loss should be lower than take profit"
-    
-    # Drawdown should be reasonable compared to position size
-    max_single_position_loss = RISK_PARAMETERS["MAX_POSITION_SIZE"] * 0.20  # Assume 20% worst case
-    assert PERFORMANCE_THRESHOLDS["MAX_DRAWDOWN"] > max_single_position_loss, \
-        "Max drawdown should account for potential single position losses"
-
+    assert RISK_PARAMETERS['STOP_LOSS_MULTIPLIER'] < RISK_PARAMETERS['TAKE_PROFIT_MULTIPLIER'], 'Stop loss should be lower than take profit'
+    max_single_position_loss = RISK_PARAMETERS['MAX_POSITION_SIZE'] * 0.2
+    assert PERFORMANCE_THRESHOLDS['MAX_DRAWDOWN'] > max_single_position_loss, 'Max drawdown should account for potential single position losses'
 
 def test_adaptive_sizing_optimization():
     """Test that adaptive sizing uses optimized parameters."""
     try:
         from ai_trading.risk.adaptive_sizing import AdaptivePositionSizer
         from ai_trading.core.enums import RiskLevel
-        
-        # Test that sizer can be instantiated with optimized parameters
         sizer = AdaptivePositionSizer(RiskLevel.MODERATE)
-        
-        # Verify regime multipliers are within reasonable bounds
         for regime, multiplier in sizer.regime_multipliers.items():
-            assert 0.1 <= multiplier <= 2.0, f"Regime multiplier {multiplier} for {regime} outside safe bounds"
-        
-        # Verify volatility adjustments are within reasonable bounds  
+            assert 0.1 <= multiplier <= 2.0, f'Regime multiplier {multiplier} for {regime} outside safe bounds'
         for vol_regime, adjustment in sizer.volatility_adjustments.items():
-            assert 0.2 <= adjustment <= 2.0, f"Volatility adjustment {adjustment} for {vol_regime} outside safe bounds"
-            
+            assert 0.2 <= adjustment <= 2.0, f'Volatility adjustment {adjustment} for {vol_regime} outside safe bounds'
     except ImportError as e:
-        # Skip test if dependencies not available
-        pytest.skip(f"Adaptive sizing test skipped due to import error: {e}")
-
+        pytest.skip(f'Adaptive sizing test skipped due to import error: {e}')
 
 def test_execution_algorithm_optimization():
     """Test that execution algorithms use optimized parameters."""
     try:
-        # Test VWAP participation rate
         from ai_trading.execution.algorithms import VWAPExecutor
-        
-        # Mock order manager for testing
-        class MockOrderManager:
-            def submit_order(self, order):
-                return True
-        
         vwap = VWAPExecutor(MockOrderManager())
-        
-        # Verify optimized participation rate
-        assert vwap.participation_rate == 0.15, f"Expected 0.15, got {vwap.participation_rate}"
-        assert 0.05 <= vwap.participation_rate <= 0.30, "VWAP participation rate outside safe bounds"
-        
+        assert vwap.participation_rate == 0.15, f'Expected 0.15, got {vwap.participation_rate}'
+        assert 0.05 <= vwap.participation_rate <= 0.3, 'VWAP participation rate outside safe bounds'
     except ImportError as e:
-        # Skip test if dependencies not available
-        pytest.skip(f"Execution algorithm test skipped due to import error: {e}")
-
+        pytest.skip(f'Execution algorithm test skipped due to import error: {e}')
 
 def test_constants_backward_compatibility():
     """Test that TRADING_CONSTANTS dictionary maintains backward compatibility."""
     from ai_trading.core.constants import TRADING_CONSTANTS
-    
-    # Verify all expected sections exist
-    required_sections = [
-        "MARKET_HOURS", "RISK_PARAMETERS", "KELLY_PARAMETERS", 
-        "EXECUTION_PARAMETERS", "DATA_PARAMETERS", "DATABASE_PARAMETERS", 
-        "PERFORMANCE_THRESHOLDS", "SYSTEM_LIMITS"
-    ]
-    
+    required_sections = ['MARKET_HOURS', 'RISK_PARAMETERS', 'KELLY_PARAMETERS', 'EXECUTION_PARAMETERS', 'DATA_PARAMETERS', 'DATABASE_PARAMETERS', 'PERFORMANCE_THRESHOLDS', 'SYSTEM_LIMITS']
     for section in required_sections:
-        assert section in TRADING_CONSTANTS, f"Missing required section: {section}"
-        assert isinstance(TRADING_CONSTANTS[section], dict), f"Section {section} should be a dictionary"
-
-
-if __name__ == "__main__":
-    # Run tests directly for validation
-    pytest.main([__file__, "-v"])+        assert section in TRADING_CONSTANTS, f'Missing required section: {section}'
+        assert isinstance(TRADING_CONSTANTS[section], dict), f'Section {section} should be a dictionary'
+if __name__ == '__main__':
+    pytest.main([__file__, '-v'])--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_portfolio_integration.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_portfolio_integration.py@@ -2,253 +2,84 @@ Integration test for portfolio-level signal filtering.
 Tests the complete workflow with realistic signal objects.
 """
-
 import pytest
 import os
 from datetime import datetime, timedelta, timezone
-
-# Set testing environment
 os.environ['TESTING'] = '1'
-
 from ai_trading.signals import filter_signals_with_portfolio_optimization
-
-
-class MockSignal:
-    """Mock signal object for testing."""
-    def __init__(self, symbol: str, side: str, quantity: float):
-        self.symbol = symbol
-        self.side = side
-        self.quantity = quantity
-
-
-class MockContext:
-    """Mock trading context for testing."""
-    def __init__(self):
-        self.portfolio_positions = {
-            'AAPL': 100.0,
-            'MSFT': 80.0,
-            'GOOGL': 60.0
-        }
-        self.data_fetcher = MockDataFetcher()
-
-
-class MockDataFetcher:
-    """Mock data fetcher for testing."""
-    
-    def get_daily_df(self, ctx, symbol):
-        """Return mock dataframe data."""
-        from datetime import datetime, timedelta
-        
-        # Generate mock price data
-        dates = [datetime.now(timezone.utc) - timedelta(days=i) for i in range(100, 0, -1)]  # AI-AGENT-REF: Use timezone-aware datetime
-        prices = [100.0 + (i % 20) - 10 for i in range(100)]  # Simulated price movement
-        volumes = [1000000 + (i % 100000) for i in range(100)]
-        
-        # Create a simple mock DataFrame-like object
-        class MockDataFrame:
-            def __init__(self, data):
-                self.data = data
-                self.columns = list(data.keys())
-            
-            def __len__(self):
-                return len(self.data['close'])
-            
-            def iloc(self):
-                return self
-            
-            def __getitem__(self, key):
-                if isinstance(key, str):
-                    return MockSeries(self.data[key])
-                elif key == -1:  # For iloc[-1]
-                    return {col: values[-1] for col, values in self.data.items()}
-                return self
-            
-            def tail(self, n):
-                return MockSeries([sum(self.data['volume'][-n:]) / n])  # Average volume
-            
-            @property  
-            def values(self):
-                return self.data['close']
-        
-        class MockSeries:
-            def __init__(self, data):
-                if isinstance(data, list):
-                    self.data = data
-                else:
-                    self.data = [data]
-            
-            def iloc(self, index):
-                return self.data[index]
-            
-            def __getitem__(self, index):
-                return self.data[index]
-            
-            def mean(self):
-                return sum(self.data) / len(self.data) if self.data else 0
-            
-            @property
-            def values(self):
-                return self.data
-            
-            def tail(self, n):
-                return MockSeries(self.data[-n:])
-        
-        if symbol in ['AAPL', 'MSFT', 'GOOGL', 'SPY']:
-            return MockDataFrame({
-                'close': prices,
-                'volume': volumes,
-                'date': dates
-            })
-        return None
-
 
 class TestPortfolioSignalFiltering:
     """Test portfolio-level signal filtering integration."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.ctx = MockContext()
-        
-        # Create test signals
-        self.test_signals = [
-            MockSignal('AAPL', 'buy', 20),   # Small increase
-            MockSignal('MSFT', 'sell', 10),  # Small decrease  
-            MockSignal('GOOGL', 'buy', 100), # Large increase
-            MockSignal('TSLA', 'buy', 50),   # New position
-        ]
-    
+        self.test_signals = [MockSignal('AAPL', 'buy', 20), MockSignal('MSFT', 'sell', 10), MockSignal('GOOGL', 'buy', 100), MockSignal('TSLA', 'buy', 50)]
+
     def test_portfolio_signal_filtering_basic(self):
         """Test basic portfolio signal filtering functionality."""
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            self.test_signals,
-            self.ctx
-        )
-        
-        # Should return a list (may be filtered)
+        filtered_signals = filter_signals_with_portfolio_optimization(self.test_signals, self.ctx)
         assert isinstance(filtered_signals, list)
-        
-        # Should not crash and should handle all signals
         assert len(filtered_signals) <= len(self.test_signals)
-        
-        # All returned signals should be from the original list
         for signal in filtered_signals:
             assert signal in self.test_signals
-    
+
     def test_portfolio_signal_filtering_with_positions(self):
         """Test portfolio signal filtering with explicit positions."""
-        current_positions = {
-            'AAPL': 100.0,
-            'MSFT': 80.0,
-            'GOOGL': 60.0
-        }
-        
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            self.test_signals,
-            self.ctx,
-            current_positions
-        )
-        
+        current_positions = {'AAPL': 100.0, 'MSFT': 80.0, 'GOOGL': 60.0}
+        filtered_signals = filter_signals_with_portfolio_optimization(self.test_signals, self.ctx, current_positions)
         assert isinstance(filtered_signals, list)
         assert len(filtered_signals) <= len(self.test_signals)
-    
+
     def test_portfolio_signal_filtering_empty_signals(self):
         """Test portfolio signal filtering with empty signal list."""
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            [],
-            self.ctx
-        )
-        
+        filtered_signals = filter_signals_with_portfolio_optimization([], self.ctx)
         assert filtered_signals == []
-    
+
     def test_portfolio_signal_filtering_invalid_signals(self):
         """Test portfolio signal filtering with invalid signals."""
-        invalid_signals = [
-            MockSignal('', 'buy', 20),     # Empty symbol
-            MockSignal('AAPL', '', 20),    # Empty side
-            MockSignal('MSFT', 'unknown', 20),  # Unknown side
-        ]
-        
-        # Should handle gracefully without crashing
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            invalid_signals,
-            self.ctx
-        )
-        
+        invalid_signals = [MockSignal('', 'buy', 20), MockSignal('AAPL', '', 20), MockSignal('MSFT', 'unknown', 20)]
+        filtered_signals = filter_signals_with_portfolio_optimization(invalid_signals, self.ctx)
         assert isinstance(filtered_signals, list)
-    
+
     def test_churn_reduction_effectiveness(self):
         """Test that portfolio filtering actually reduces churn."""
-        # Create many small trade signals (high churn scenario)
         high_churn_signals = []
         for i in range(20):
             symbol = ['AAPL', 'MSFT', 'GOOGL'][i % 3]
             side = 'buy' if i % 2 == 0 else 'sell'
-            quantity = 5 + (i % 10)  # Small quantities
+            quantity = 5 + i % 10
             high_churn_signals.append(MockSignal(symbol, side, quantity))
-        
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            high_churn_signals,
-            self.ctx
-        )
-        
-        # Should significantly reduce the number of signals
+        filtered_signals = filter_signals_with_portfolio_optimization(high_churn_signals, self.ctx)
         reduction_ratio = len(filtered_signals) / len(high_churn_signals)
-        
-        # Expect at least some reduction (not necessarily 60-80% in test environment)
         assert reduction_ratio <= 1.0
-        
-        # Log the results for verification
-        print(f"Churn reduction test: {len(high_churn_signals)} -> {len(filtered_signals)} "
-              f"({reduction_ratio:.1%} passed)")
-    
+        print(f'Churn reduction test: {len(high_churn_signals)} -> {len(filtered_signals)} ({reduction_ratio:.1%} passed)')
+
     def test_portfolio_optimization_fallback(self):
         """Test graceful fallback when portfolio optimization fails."""
-        # Test with minimal context that might cause issues
         minimal_ctx = type('MinimalContext', (), {})()
-        
-        # Should not crash and should return signals (potentially filtered)
-        filtered_signals = filter_signals_with_portfolio_optimization(
-            self.test_signals,
-            minimal_ctx
-        )
-        
+        filtered_signals = filter_signals_with_portfolio_optimization(self.test_signals, minimal_ctx)
         assert isinstance(filtered_signals, list)
-        # Portfolio optimization may filter signals even in limited environments
         assert len(filtered_signals) <= len(self.test_signals)
-
 
 class TestPortfolioRebalancingIntegration:
     """Test integration with portfolio rebalancing functionality."""
-    
+
     def setup_method(self):
         """Set up test fixtures."""
         self.ctx = MockContext()
-        # Add rebalancing-specific attributes
-        self.ctx.target_weights = {
-            'AAPL': 0.4,
-            'MSFT': 0.35,
-            'GOOGL': 0.25
-        }
+        self.ctx.target_weights = {'AAPL': 0.4, 'MSFT': 0.35, 'GOOGL': 0.25}
         self.ctx.last_portfolio_rebalance = None
-    
+
     def test_rebalancing_integration(self):
         """Test that portfolio optimization integrates with rebalancing logic."""
         from ai_trading.rebalancer import portfolio_first_rebalance, _get_current_positions_for_rebalancing
-        
-        # Test position extraction
         positions = _get_current_positions_for_rebalancing(self.ctx)
         assert isinstance(positions, dict)
-        
-        # Test portfolio-first rebalancing (should not crash)
         try:
             portfolio_first_rebalance(self.ctx)
-            # If it doesn't crash, that's a success in this test environment
             assert True
         except Exception as e:
-            # Some failures are expected due to limited test environment
-            # Just ensure it's handling errors gracefully
-            assert "Error" in str(e) or "not available" in str(e).lower()
-
-
+            assert 'Error' in str(e) or 'not available' in str(e).lower()
 if __name__ == '__main__':
     pytest.main([__file__, '-v'])--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_position_intelligence.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_position_intelligence.py@@ -2,256 +2,123 @@ Simple test for the enhanced position management system.
 Tests the integration without requiring full environment setup.
 """
-
 import os
 import sys
 import logging
-
-# Set up basic logging
 logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
 
 def test_intelligent_position_components():
     """Test the intelligent position management components directly."""
-    print("🧪 Testing Intelligent Position Management Components")
-    print("=" * 60)
-    
-    # Add position module to path
+    print('🧪 Testing Intelligent Position Management Components')
+    print('=' * 60)
     position_path = os.path.join(os.path.dirname(__file__), 'ai_trading', 'position')
     if position_path not in sys.path:
         sys.path.insert(0, position_path)
-    
     try:
-        # Test 1: Market Regime Detection
-        print("\n1. Testing Market Regime Detection...")
+        print('\n1. Testing Market Regime Detection...')
         from market_regime import MarketRegimeDetector, MarketRegime
-        
         detector = MarketRegimeDetector()
         params = detector.get_regime_parameters(MarketRegime.TRENDING_BULL)
-        
-        print(f"   ✓ Regime parameters for trending bull: {len(params)} parameters")
+        print(f'   ✓ Regime parameters for trending bull: {len(params)} parameters')
         print(f"   ✓ Profit taking patience: {params.get('profit_taking_patience', 'N/A')}")
         print(f"   ✓ Stop distance multiplier: {params.get('stop_distance_multiplier', 'N/A')}")
-        
-        # Test 2: Technical Signal Analysis
-        print("\n2. Testing Technical Signal Analysis...")
+        print('\n2. Testing Technical Signal Analysis...')
         from technical_analyzer import TechnicalSignalAnalyzer
-        
         analyzer = TechnicalSignalAnalyzer()
-        
-        # Test RSI calculation with mock data
-        class MockSeries:
-            def __init__(self, data):
-                self.data = data
-            def __len__(self):
-                return len(self.data)
-            def diff(self):
-                return MockSeries([self.data[i] - self.data[i-1] if i > 0 else 0 for i in range(len(self.data))])
-            def where(self, condition, other):
-                return MockSeries([x if x > 0 else other for x in self.data])
-            def rolling(self, window):
-                return MockRolling(self.data, window)
-            @property
-            def iloc(self):
-                return MockIloc(self.data)
-                
-        class MockRolling:
-            def __init__(self, data, window):
-                self.data = data
-                self.window = window
-            def mean(self):
-                result = []
-                for i in range(len(self.data)):
-                    if i < self.window - 1:
-                        result.append(float('nan'))
-                    else:
-                        window_data = self.data[i-self.window+1:i+1]
-                        result.append(sum(window_data) / len(window_data))
-                return MockSeries(result)
-                
-        class MockIloc:
-            def __init__(self, data):
-                self.data = data
-            def __getitem__(self, idx):
-                return self.data[idx]
-        
-        # Test with trending price data
         price_data = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]
         mock_prices = MockSeries(price_data)
-        
         rsi = analyzer._calculate_rsi(mock_prices, 14)
-        print(f"   ✓ RSI calculation: {rsi:.2f} (trending up)")
-        
-        # Test 3: Trailing Stop Management
-        print("\n3. Testing Trailing Stop Management...")
+        print(f'   ✓ RSI calculation: {rsi:.2f} (trending up)')
+        print('\n3. Testing Trailing Stop Management...')
         from trailing_stops import TrailingStopManager
-        
         stop_manager = TrailingStopManager()
-        
-        # Test stop distance calculation
         initial_distance = stop_manager.base_trail_percent
-        print(f"   ✓ Base trailing distance: {initial_distance}%")
-        
-        # Test momentum multiplier
+        print(f'   ✓ Base trailing distance: {initial_distance}%')
         multiplier = stop_manager._calculate_momentum_multiplier('AAPL', None)
-        print(f"   ✓ Momentum multiplier: {multiplier}")
-        
-        # Test time decay
+        print(f'   ✓ Momentum multiplier: {multiplier}')
         time_multiplier = stop_manager._calculate_time_decay_multiplier(10)
-        print(f"   ✓ Time decay multiplier (10 days): {time_multiplier}")
-        
-        # Test 4: Profit Taking Engine
-        print("\n4. Testing Profit Taking Engine...")
+        print(f'   ✓ Time decay multiplier (10 days): {time_multiplier}')
+        print('\n4. Testing Profit Taking Engine...')
         from profit_taking import ProfitTakingEngine
-        
         profit_engine = ProfitTakingEngine()
-        
-        # Test profit velocity calculation
-        velocity = profit_engine.calculate_profit_velocity('AAPL')  # Will return 0.0 without plan
-        print(f"   ✓ Profit velocity calculation: {velocity}")
-        
-        # Test percentage targets creation
+        velocity = profit_engine.calculate_profit_velocity('AAPL')
+        print(f'   ✓ Profit velocity calculation: {velocity}')
         targets = profit_engine._create_percentage_targets(100.0, 100)
-        print(f"   ✓ Created {len(targets)} percentage-based profit targets")
-        
-        # Test 5: Portfolio Correlation Analysis
-        print("\n5. Testing Portfolio Correlation Analysis...")
+        print(f'   ✓ Created {len(targets)} percentage-based profit targets')
+        print('\n5. Testing Portfolio Correlation Analysis...')
         from correlation_analyzer import PortfolioCorrelationAnalyzer
-        
         corr_analyzer = PortfolioCorrelationAnalyzer()
-        
-        # Test sector classification
         sector = corr_analyzer._get_symbol_sector('AAPL')
-        print(f"   ✓ AAPL sector classification: {sector}")
-        
+        print(f'   ✓ AAPL sector classification: {sector}')
         sector = corr_analyzer._get_symbol_sector('JPM')
-        print(f"   ✓ JPM sector classification: {sector}")
-        
-        # Test concentration classification
+        print(f'   ✓ JPM sector classification: {sector}')
         level = corr_analyzer._classify_position_concentration(45.0)
-        print(f"   ✓ 45% position concentration level: {level.value}")
-        
-        # Test 6: Intelligent Position Manager
-        print("\n6. Testing Intelligent Position Manager...")
+        print(f'   ✓ 45% position concentration level: {level.value}')
+        print('\n6. Testing Intelligent Position Manager...')
         from intelligent_manager import IntelligentPositionManager
-        
         manager = IntelligentPositionManager()
-        print(f"   ✓ Initialized with {len(manager.analysis_weights)} analysis components")
-        
-        # Test action determination
+        print(f'   ✓ Initialized with {len(manager.analysis_weights)} analysis components')
         action, confidence, urgency = manager._determine_action_from_scores(0.8, 0.2, 0.1)
-        print(f"   ✓ Action determination: {action.value} (confidence: {confidence:.2f})")
-        
-        print("\n" + "=" * 60)
-        print("🎉 ALL INTELLIGENT POSITION MANAGEMENT COMPONENTS WORKING!")
-        print("🚀 Ready for advanced position holding strategies!")
-        
+        print(f'   ✓ Action determination: {action.value} (confidence: {confidence:.2f})')
+        print('\n' + '=' * 60)
+        print('🎉 ALL INTELLIGENT POSITION MANAGEMENT COMPONENTS WORKING!')
+        print('🚀 Ready for advanced position holding strategies!')
         return True
-        
     except Exception as e:
-        print(f"\n❌ Test failed: {e}")
+        print(f'\n❌ Test failed: {e}')
         import traceback
         traceback.print_exc()
         return False
 
 def test_integration_scenarios():
     """Test integration scenarios."""
-    print("\n🔗 Testing Integration Scenarios")
-    print("=" * 40)
-    
+    print('\n🔗 Testing Integration Scenarios')
+    print('=' * 40)
     try:
         position_path = os.path.join(os.path.dirname(__file__), 'ai_trading', 'position')
         if position_path not in sys.path:
             sys.path.insert(0, position_path)
-        
         from intelligent_manager import IntelligentPositionManager
         from market_regime import MarketRegime
-        
         manager = IntelligentPositionManager()
-        
-        # Test scenario: Profitable position in trending market
-        print("\n📈 Scenario 1: Profitable position in bull trend")
-        
-        # Mock analyses
-        regime_analysis = {
-            'regime': MarketRegime.TRENDING_BULL,
-            'confidence': 0.8,
-            'parameters': {'profit_taking_patience': 2.0, 'stop_distance_multiplier': 1.5}
-        }
-        
-        technical_analysis = {
-            'signals': None,
-            'hold_strength': 'STRONG',
-            'exit_urgency': 0.2,
-            'divergence': 'NONE',
-            'momentum': 0.8
-        }
-        
-        profit_analysis = {
-            'triggered_targets': [],
-            'profit_plan': None,
-            'velocity': 2.0,
-            'has_targets': False
-        }
-        
-        stop_analysis = {
-            'stop_level': None,
-            'is_triggered': False,
-            'stop_price': 0.0,
-            'trail_distance': 0.0
-        }
-        
-        correlation_analysis = {
-            'portfolio_analysis': None,
-            'should_reduce': False,
-            'reduce_reason': '',
-            'correlation_factor': 1.0
-        }
-        
-        # Test action determination
+        print('\n📈 Scenario 1: Profitable position in bull trend')
+        regime_analysis = {'regime': MarketRegime.TRENDING_BULL, 'confidence': 0.8, 'parameters': {'profit_taking_patience': 2.0, 'stop_distance_multiplier': 1.5}}
+        technical_analysis = {'signals': None, 'hold_strength': 'STRONG', 'exit_urgency': 0.2, 'divergence': 'NONE', 'momentum': 0.8}
+        profit_analysis = {'triggered_targets': [], 'profit_plan': None, 'velocity': 2.0, 'has_targets': False}
+        stop_analysis = {'stop_level': None, 'is_triggered': False, 'stop_price': 0.0, 'trail_distance': 0.0}
+        correlation_analysis = {'portfolio_analysis': None, 'should_reduce': False, 'reduce_reason': '', 'correlation_factor': 1.0}
         action, confidence, urgency = manager._determine_action_from_scores(0.7, 0.2, 0.1)
-        print(f"   ✓ Recommended action: {action.value}")
-        print(f"   ✓ Confidence: {confidence:.2f}")
-        print(f"   ✓ Urgency: {urgency:.2f}")
-        
-        # Test scenario: Loss position with bearish signals
-        print("\n📉 Scenario 2: Loss position with bearish signals")
+        print(f'   ✓ Recommended action: {action.value}')
+        print(f'   ✓ Confidence: {confidence:.2f}')
+        print(f'   ✓ Urgency: {urgency:.2f}')
+        print('\n📉 Scenario 2: Loss position with bearish signals')
         action, confidence, urgency = manager._determine_action_from_scores(0.1, 0.8, 0.2)
-        print(f"   ✓ Recommended action: {action.value}")
-        print(f"   ✓ Confidence: {confidence:.2f}")
-        print(f"   ✓ Urgency: {urgency:.2f}")
-        
-        print("\n✅ Integration scenarios completed successfully!")
+        print(f'   ✓ Recommended action: {action.value}')
+        print(f'   ✓ Confidence: {confidence:.2f}')
+        print(f'   ✓ Urgency: {urgency:.2f}')
+        print('\n✅ Integration scenarios completed successfully!')
         return True
-        
     except Exception as e:
-        print(f"\n❌ Integration test failed: {e}")
+        print(f'\n❌ Integration test failed: {e}')
         return False
-
-if __name__ == "__main__":
-    print("🧪 ADVANCED INTELLIGENT POSITION MANAGEMENT TESTING")
-    print("=" * 80)
-    
+if __name__ == '__main__':
+    print('🧪 ADVANCED INTELLIGENT POSITION MANAGEMENT TESTING')
+    print('=' * 80)
     success = True
-    
-    # Test individual components
     success &= test_intelligent_position_components()
-    
-    # Test integration scenarios
     success &= test_integration_scenarios()
-    
-    print("\n" + "=" * 80)
+    print('\n' + '=' * 80)
     if success:
-        print("🎉 ALL TESTS PASSED!")
-        print("✅ Advanced intelligent position holding strategies are ready!")
-        print("🚀 The system can now:")
-        print("   • Detect market regimes and adapt strategies")
-        print("   • Analyze technical signals for exit timing")
-        print("   • Manage dynamic trailing stops")
-        print("   • Execute multi-tiered profit taking")
-        print("   • Monitor portfolio correlations")
-        print("   • Make intelligent hold/sell decisions")
+        print('🎉 ALL TESTS PASSED!')
+        print('✅ Advanced intelligent position holding strategies are ready!')
+        print('🚀 The system can now:')
+        print('   • Detect market regimes and adapt strategies')
+        print('   • Analyze technical signals for exit timing')
+        print('   • Manage dynamic trailing stops')
+        print('   • Execute multi-tiered profit taking')
+        print('   • Monitor portfolio correlations')
+        print('   • Make intelligent hold/sell decisions')
     else:
-        print("❌ SOME TESTS FAILED")
-        print("🔧 Please review the errors above")
-    
-    print("=" * 80)+        print('❌ SOME TESTS FAILED')
+        print('🔧 Please review the errors above')
+    print('=' * 80)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_retry_idempotency_integration.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_retry_idempotency_integration.py@@ -1,128 +1,66 @@ """Integration test for retry/backoff and idempotency in order submission."""
-
 import time
 from datetime import datetime, timezone
-
-# Set PYTHONPATH to include our tenacity mock
 import sys
 sys.path.insert(0, '/tmp')
-
 from tenacity import retry, stop_after_attempt, wait_exponential
-
-
-class MockBrokerAPI:
-    """Mock broker API that fails N times then succeeds."""
-    
-    def __init__(self, fail_count=2):
-        self.fail_count = fail_count
-        self.call_count = 0
-        self.submitted_orders = []
-    
-    def submit_order(self, order_data):
-        """Mock order submission that fails first N times."""
-        self.call_count += 1
-        
-        if self.call_count <= self.fail_count:
-            raise ConnectionError(f"Network error on attempt {self.call_count}")
-        
-        # Success case - record the order
-        order_id = f"order_{len(self.submitted_orders) + 1}"
-        order_record = {
-            "id": order_id,
-            "symbol": order_data["symbol"],
-            "quantity": order_data["quantity"],
-            "submitted_at": datetime.now(timezone.utc),
-            "status": "submitted"
-        }
-        self.submitted_orders.append(order_record)
-        return order_record
-
 
 class OrderIdempotencyManager:
     """Mock idempotency manager to prevent duplicate orders."""
-    
+
     def __init__(self):
         self.submitted_orders = set()
-    
+
     def mark_submitted(self, order_id):
         """Mark an order as submitted to prevent duplicates."""
         if order_id in self.submitted_orders:
-            raise ValueError(f"Order {order_id} already submitted")
+            raise ValueError(f'Order {order_id} already submitted')
         self.submitted_orders.add(order_id)
-    
+
     def is_submitted(self, order_id):
         """Check if order was already submitted."""
         return order_id in self.submitted_orders
 
-
 class PositionReconciler:
     """Mock position reconciler."""
-    
+
     def __init__(self):
         self.local_positions = {}
         self.broker_positions = {}
         self.reconciliation_calls = 0
-    
+
     def reconcile_positions_and_orders(self):
         """Mock reconciliation between local and broker state."""
         self.reconciliation_calls += 1
-        # In real implementation, this would sync local state with broker
-        return {"reconciled": True, "call_count": self.reconciliation_calls}
+        return {'reconciled': True, 'call_count': self.reconciliation_calls}
 
-
-@retry(
-    stop=stop_after_attempt(3),
-    wait=wait_exponential(multiplier=1, min=0.1, max=1),
-    reraise=True
-)
+@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=0.1, max=1), reraise=True)
 def submit_order_with_retry(broker, idempotency_mgr, order_data):
     """Submit order with retry logic and idempotency protection."""
-    order_id = order_data["client_order_id"]
-    
-    # Check idempotency first - if already submitted, return early
+    order_id = order_data['client_order_id']
     if idempotency_mgr.is_submitted(order_id):
-        raise ValueError(f"Order {order_id} already submitted")
-    
-    # Use a local flag to track if we've marked it submitted
+        raise ValueError(f'Order {order_id} already submitted')
     marked_submitted = False
-    
     try:
-        # Mark as submitted before actual submission attempts
         if not marked_submitted:
             idempotency_mgr.mark_submitted(order_id)
             marked_submitted = True
-        
-        # Attempt broker submission
         result = broker.submit_order(order_data)
         return result
     except Exception:
-        # If submission fails, we keep the idempotency mark
-        # This prevents retry storms from causing duplicate orders
         raise
-
 
 def test_retry_idempotency_integration():
     """Test that retry mechanism works with idempotency protection."""
-    broker = MockBrokerAPI(fail_count=2)  # Fail 2 times, succeed on 3rd
+    broker = MockBrokerAPI(fail_count=2)
     idempotency_mgr = OrderIdempotencyManager()
     reconciler = PositionReconciler()
-    
-    order_data = {
-        "client_order_id": "test_order_123",
-        "symbol": "AAPL",
-        "quantity": 100,
-        "side": "buy"
-    }
-    
-    # Manually apply retry logic since we can't easily test the decorator
+    order_data = {'client_order_id': 'test_order_123', 'symbol': 'AAPL', 'quantity': 100, 'side': 'buy'}
     attempt = 0
     max_attempts = 3
     result = None
-    order_id = order_data["client_order_id"]
-    
-    # Mark as submitted for idempotency
+    order_id = order_data['client_order_id']
     idempotency_mgr.mark_submitted(order_id)
-    
     while attempt < max_attempts:
         try:
             result = broker.submit_order(order_data)
@@ -131,70 +69,37 @@             attempt += 1
             if attempt >= max_attempts:
                 raise
-            time.sleep(0.1)  # Small delay
-    
-    # Verify retry behavior
-    assert broker.call_count == 3, f"Expected 3 calls, got {broker.call_count}"
-    assert len(broker.submitted_orders) == 1, "Should have exactly one order submitted"
-    
-    # Verify order details
+            time.sleep(0.1)
+    assert broker.call_count == 3, f'Expected 3 calls, got {broker.call_count}'
+    assert len(broker.submitted_orders) == 1, 'Should have exactly one order submitted'
     submitted_order = broker.submitted_orders[0]
-    assert submitted_order["symbol"] == "AAPL"
-    assert submitted_order["quantity"] == 100
-    
-    # Verify idempotency protection
-    assert idempotency_mgr.is_submitted("test_order_123")
-
+    assert submitted_order['symbol'] == 'AAPL'
+    assert submitted_order['quantity'] == 100
+    assert idempotency_mgr.is_submitted('test_order_123')
 
 def test_reconciliation_heals_state():
     """Test that reconciliation heals local/broker state after submission."""
-    broker = MockBrokerAPI(fail_count=0)  # No failures
+    broker = MockBrokerAPI(fail_count=0)
     idempotency_mgr = OrderIdempotencyManager()
     reconciler = PositionReconciler()
-    
-    order_data = {
-        "client_order_id": "test_order_reconcile",
-        "symbol": "TSLA", 
-        "quantity": 50,
-        "side": "buy"
-    }
-    
-    # Submit order successfully (no retries needed)
-    idempotency_mgr.mark_submitted(order_data["client_order_id"])
+    order_data = {'client_order_id': 'test_order_reconcile', 'symbol': 'TSLA', 'quantity': 50, 'side': 'buy'}
+    idempotency_mgr.mark_submitted(order_data['client_order_id'])
     result = broker.submit_order(order_data)
-    
-    # Simulate reconciliation after order submission
     reconciliation_result = reconciler.reconcile_positions_and_orders()
-    
-    # Verify reconciliation was called
-    assert reconciliation_result["reconciled"] is True
+    assert reconciliation_result['reconciled'] is True
     assert reconciler.reconciliation_calls == 1
-    
-    # Verify order is tracked correctly
     assert len(broker.submitted_orders) == 1
-    assert idempotency_mgr.is_submitted("test_order_reconcile")
-
+    assert idempotency_mgr.is_submitted('test_order_reconcile')
 
 def test_retry_exhaustion_with_idempotency():
     """Test behavior when all retries are exhausted."""
-    broker = MockBrokerAPI(fail_count=5)  # Fail more times than retry limit
+    broker = MockBrokerAPI(fail_count=5)
     idempotency_mgr = OrderIdempotencyManager()
-    
-    order_data = {
-        "client_order_id": "test_order_fail",
-        "symbol": "NVDA",
-        "quantity": 25,
-        "side": "sell"
-    }
-    
-    # Manual retry logic
+    order_data = {'client_order_id': 'test_order_fail', 'symbol': 'NVDA', 'quantity': 25, 'side': 'sell'}
     attempt = 0
     max_attempts = 3
-    order_id = order_data["client_order_id"]
-    
-    # Mark as submitted for idempotency
+    order_id = order_data['client_order_id']
     idempotency_mgr.mark_submitted(order_id)
-    
     last_exception = None
     while attempt < max_attempts:
         try:
@@ -206,22 +111,13 @@             if attempt >= max_attempts:
                 break
             time.sleep(0.1)
-    
-    # Should have exhausted retries
     if last_exception:
-        pass  # Expected to fail
-    
-    # Verify retries occurred but no order was submitted
-    assert broker.call_count == 3, f"Should have attempted 3 times, got {broker.call_count}"
-    assert len(broker.submitted_orders) == 0, "No orders should be submitted on failure"
-    
-    # Verify idempotency mark still exists (prevents retry storms)
-    assert idempotency_mgr.is_submitted("test_order_fail")
-
-
-if __name__ == "__main__":
-    # Run tests manually if executed directly
+        pass
+    assert broker.call_count == 3, f'Should have attempted 3 times, got {broker.call_count}'
+    assert len(broker.submitted_orders) == 0, 'No orders should be submitted on failure'
+    assert idempotency_mgr.is_submitted('test_order_fail')
+if __name__ == '__main__':
     test_retry_idempotency_integration()
     test_reconciliation_heals_state()
     test_retry_exhaustion_with_idempotency()
-    print("All integration tests passed!")+    print('All integration tests passed!')--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_talib_enforcement.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_talib_enforcement.py@@ -1,104 +1,57 @@ """Test TA-Lib enforcement and audit file creation improvements."""
-
 import csv
 from pathlib import Path
-
 import pytest
-
 
 def test_talib_import_enforcement():
     """Test that TA library import gracefully handles missing dependency."""
-    # Read the imports file to test the TA library section
-    imports_file = Path(__file__).parent.parent / "ai_trading" / "strategies" / "imports.py"
-    
+    imports_file = Path(__file__).parent.parent / 'ai_trading' / 'strategies' / 'imports.py'
     with open(imports_file, 'r') as f:
         content = f.read()
-    
-    # Find the TA library section
     lines = content.split('\n')
     ta_start = None
     ta_end = None
-    
     for i, line in enumerate(lines):
         if '# TA library for optimized technical analysis' in line:
             ta_start = i
         elif ta_start is not None and 'ta = MockTa()' in line:
             ta_end = i + 1
             break
-    
-    assert ta_start is not None, "Could not find TA library section"
-    assert ta_end is not None, "Could not find end of TA library section"
-    
-    # Verify the fallback implementation exists
+    assert ta_start is not None, 'Could not find TA library section'
+    assert ta_end is not None, 'Could not find end of TA library section'
     assert 'TA_AVAILABLE = False' in content
     assert 'class MockTa:' in content
     assert 'TA library not available - using fallback implementation' in content
-    
-    # Test that the import works without raising an error
     try:
         from ai_trading.strategies.imports import ta, TA_AVAILABLE
-        assert TA_AVAILABLE is True, "Expected TA_AVAILABLE to be True since ta library is installed"
-        assert hasattr(ta, 'trend'), "Expected ta to have trend module"
-        assert hasattr(ta, 'momentum'), "Expected ta to have momentum module"
-        assert hasattr(ta, 'volatility'), "Expected ta to have volatility module"
-        print("✅ TA library import successful")
+        assert TA_AVAILABLE is True, 'Expected TA_AVAILABLE to be True since ta library is installed'
+        assert hasattr(ta, 'trend'), 'Expected ta to have trend module'
+        assert hasattr(ta, 'momentum'), 'Expected ta to have momentum module'
+        assert hasattr(ta, 'volatility'), 'Expected ta to have volatility module'
+        print('✅ TA library import successful')
     except ImportError as e:
-        pytest.fail(f"TA library import should not raise ImportError with fallback: {e}")
-
+        pytest.fail(f'TA library import should not raise ImportError with fallback: {e}')
 
 def test_audit_file_creation_and_permissions(tmp_path, monkeypatch):
     """Test that audit.py creates trade log file with proper permissions."""
     import sys
-    
-    # Mock config to use temporary path
-    trade_log_path = tmp_path / "data" / "trades.csv"
-    
-    # Create mock config module
-    class MockConfig:
-        TRADE_LOG_FILE = str(trade_log_path)
-        TRADE_AUDIT_DIR = str(tmp_path / "audit")
-        NEWS_API_KEY = "fake_test_news_api_not_real"  # Guard: test-only fake value
-    
-    # Temporarily replace config module
+    trade_log_path = tmp_path / 'data' / 'trades.csv'
     original_config = sys.modules.get('config')
     sys.modules['config'] = MockConfig()
-    
     try:
-        # Import audit after mocking config
         if 'audit' in sys.modules:
             del sys.modules['audit']
         import audit
-        
-        # Ensure the file doesn't exist initially
         assert not trade_log_path.exists()
         assert not trade_log_path.parent.exists()
-        
-        # Call log_trade which should create the directory and file
-        audit.log_trade(
-            symbol="TEST",
-            qty=10,
-            side="buy", 
-            fill_price=100.0,
-            timestamp="2024-01-01T10:00:00Z",
-            extra_info="TEST_MODE",
-            exposure=0.1
-        )
-        
-        # Verify directory was created
+        audit.log_trade(symbol='TEST', qty=10, side='buy', fill_price=100.0, timestamp='2024-01-01T10:00:00Z', extra_info='TEST_MODE', exposure=0.1)
         assert trade_log_path.parent.exists()
-        
-        # Verify file was created
         assert trade_log_path.exists()
-        
-        # Verify file permissions (0o664)
         file_stat = trade_log_path.stat()
         file_mode = oct(file_stat.st_mode)[-3:]
-        assert file_mode == "664", f"Expected file permissions 664, got {file_mode}"
-        
-        # Verify file contents
+        assert file_mode == '664', f'Expected file permissions 664, got {file_mode}'
         with open(trade_log_path, 'r') as f:
             rows = list(csv.DictReader(f))
-            
         assert len(rows) == 1
         assert rows[0]['symbol'] == 'TEST'
         assert rows[0]['side'] == 'buy'
@@ -106,70 +59,41 @@         assert rows[0]['price'] == '100.0'
         assert rows[0]['exposure'] == '0.1'
         assert rows[0]['mode'] == 'TEST_MODE'
-        
-        # Verify CSV header exists
         with open(trade_log_path, 'r') as f:
             first_line = f.readline().strip()
-            expected_headers = "id,timestamp,symbol,side,qty,price,exposure,mode,result"
+            expected_headers = 'id,timestamp,symbol,side,qty,price,exposure,mode,result'
             assert first_line == expected_headers
-            
     finally:
-        # Restore original config module
         if original_config is not None:
             sys.modules['config'] = original_config
         elif 'config' in sys.modules:
             del sys.modules['config']
-        
-        # Clean up audit module
         if 'audit' in sys.modules:
             del sys.modules['audit']
-
 
 def test_audit_file_multiple_trades(tmp_path, monkeypatch):
     """Test that multiple trades are appended correctly without duplicate headers."""
     import sys
-    
-    trade_log_path = tmp_path / "trades.csv"
-    
-    class MockConfig:
-        TRADE_LOG_FILE = str(trade_log_path)
-        TRADE_AUDIT_DIR = str(tmp_path)
-        NEWS_API_KEY = "fake_test_news_api_not_real"  # Guard: test-only fake value
-    
+    trade_log_path = tmp_path / 'trades.csv'
     original_config = sys.modules.get('config')
     sys.modules['config'] = MockConfig()
-    
     try:
         if 'audit' in sys.modules:
             del sys.modules['audit']
         import audit
-        
-        # Log first trade
-        audit.log_trade("AAPL", 5, "buy", 150.0, "2024-01-01T10:00:00Z", "TEST_MODE")
-        
-        # Log second trade  
-        audit.log_trade("MSFT", 3, "sell", 250.0, "2024-01-01T11:00:00Z", "TEST_MODE")
-        
-        # Verify both trades are in file
+        audit.log_trade('AAPL', 5, 'buy', 150.0, '2024-01-01T10:00:00Z', 'TEST_MODE')
+        audit.log_trade('MSFT', 3, 'sell', 250.0, '2024-01-01T11:00:00Z', 'TEST_MODE')
         with open(trade_log_path, 'r') as f:
             content = f.read()
-            
-        # Should have header + 2 data rows
         lines = content.strip().split('\n')
-        assert len(lines) == 3, f"Expected 3 lines (header + 2 trades), got {len(lines)}"
-        
-        # Verify header appears only once
-        header_count = content.count("id,timestamp,symbol,side,qty,price,exposure,mode,result")
-        assert header_count == 1, f"Expected 1 header, found {header_count}"
-        
-        # Verify trade data
+        assert len(lines) == 3, f'Expected 3 lines (header + 2 trades), got {len(lines)}'
+        header_count = content.count('id,timestamp,symbol,side,qty,price,exposure,mode,result')
+        assert header_count == 1, f'Expected 1 header, found {header_count}'
         with open(trade_log_path, 'r') as f:
             rows = list(csv.DictReader(f))
-            
         assert len(rows) == 2
         assert rows[0]['symbol'] == 'AAPL'
         assert rows[1]['symbol'] == 'MSFT'
-        
     finally:
         if original_config is not None:
             sys.modules['config'] = original_config
--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_trigger_meta_learning_conversion.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/test_trigger_meta_learning_conversion.py@@ -2,12 +2,8 @@ import tempfile
 import os
 
-# Mock the config module to avoid environment variable requirements
-# Create a simple class-based approach that avoids singleton complexity
-
 class TradingConfig:
     """Mock TradingConfig class for testing."""
-    # Risk Management Parameters
     max_drawdown_threshold = 0.15
     daily_loss_limit = 0.03
     dollar_risk_limit = 0.05
@@ -18,16 +14,12 @@     position_size_min_usd = 100.0
     max_position_size = 8000
     max_position_size_pct = 0.25
-    
-    # Kelly Criterion Parameters
     kelly_fraction = 0.6
     kelly_fraction_max = 0.25
     min_sample_size = 20
-    confidence_level = 0.90
+    confidence_level = 0.9
     lookback_periods = 252
     rebalance_frequency = 21
-    
-    # Trading Mode Parameters  
     conf_threshold = 0.75
     buy_threshold = 0.1
     min_confidence = 0.6
@@ -35,310 +27,102 @@     take_profit_factor = 1.8
     trailing_factor = 1.2
     scaling_factor = 0.3
-    
+
     @classmethod
-    def from_env(cls, mode="balanced"):
+    def from_env(cls, mode='balanced'):
         return cls()
-    
+
     def get_legacy_params(self):
         """Return legacy parameters for backward compatibility."""
-        return {
-            'conf_threshold': self.conf_threshold,
-            'buy_threshold': self.buy_threshold,
-            'min_confidence': self.min_confidence,
-            'confirmation_count': self.confirmation_count,
-            'take_profit_factor': self.take_profit_factor,
-            'trailing_factor': self.trailing_factor,
-            'scaling_factor': self.scaling_factor,
-        }
-
-class MockConfig:
-    """Simple mock config that supports both attribute and import access."""
-    # Class attributes that can be modified by tests
-    TRADE_LOG_FILE = 'logs/trades.csv'
-    VERBOSE_LOGGING = True
-    SCHEDULER_SLEEP_SECONDS = 30.0
-    NEWS_API_KEY = "fake_test_news_api_not_real"  # Guard: test-only fake value
-    TESTING = True
-    REQUIRED_ENV_VARS = []
-    SEED = 42
-    ALPACA_DATA_FEED = "iex"
-    ALPACA_BASE_URL = "https://paper-api.alpaca.markets"
-    RATE_LIMIT_BUDGET = 190
-    
-    # Set TradingConfig as a class attribute for imports
-    TradingConfig = TradingConfig
-    
-    # SGD Parameters from config.py
-    SGD_PARAMS = {
-        "loss": "squared_error",
-        "learning_rate": "constant", 
-        "eta0": 0.01,
-        "penalty": "l2",
-        "alpha": 0.0001,
-        "random_state": 42,
-        "max_iter": 1000,
-        "tol": 1e-3
-    }
-    
-    @classmethod
-    def __getattr__(cls, name):
-        """Return a default value for any missing attribute."""
-        # Common default values for config attributes
-        defaults = {
-            'ALPACA_API_KEY': 'FAKE_TEST_API_KEY_NOT_REAL_123456789',
-            'ALPACA_SECRET_KEY': 'FAKE_TEST_SECRET_KEY_NOT_REAL_123456789',
-            'BOT_MODE': 'balanced',
-            'MODEL_PATH': 'trained_model.pkl',
-            'WEBHOOK_SECRET': 'fake-test-webhook-not-real',
-            'FLASK_PORT': 9000,
-            'DISABLE_DAILY_RETRAIN': False,
-            'SHADOW_MODE': False,
-            'DRY_RUN': False,
-        }
-        
-        if name in defaults:
-            return defaults[name]
-        
-        # For any other attribute, return a reasonable default based on the name
-        if name.endswith('_LIMIT') or name.endswith('_THRESHOLD'):
-            return 0.05
-        elif name.endswith('_PORT'):
-            return 9000
-        elif name.endswith('_BUDGET') or name.endswith('_SIZE'):
-            return 100
-        elif name.endswith('_MODE') or name.endswith('_FLAG'):
-            return False
-        elif name.endswith('_PATH') or name.endswith('_FILE'):
-            return f"test_{name.lower()}"
-        else:
-            return None
-    
-    @staticmethod
-    def reload_env():
-        """Mock reload_env method."""
-        pass
-    
-    @staticmethod
-    def validate_env_vars():
-        """Mock validate_env_vars method."""
-        pass
-    
-    @staticmethod
-    def validate_alpaca_credentials():
-        """Mock validate_alpaca_credentials method."""
-        pass
-    
-    @staticmethod
-    def log_config(env_vars):
-        """Mock log_config method."""
-        pass
-    
-    @staticmethod
-    def mask_secret(value: str, show_last: int = 4) -> str:
-        """Return value with all but the last show_last characters masked."""
-        if value is None:
-            return ""
-        return "*" * max(0, len(value) - show_last) + value[-show_last:]
-    
-    @staticmethod
-    def get_env(key: str, default=None, required=False, reload=False):
-        """Mock get_env method."""
-        import os
-        if reload:
-            MockConfig.reload_env()
-        
-        # Mock common environment variables used in tests
-        defaults = {
-            "MODELS_DIR": "models",
-            "MODEL_PATH": "trained_model.pkl",
-            "ALPACA_API_KEY": "FAKE_TEST_API_KEY_NOT_REAL_123456789",
-            "ALPACA_SECRET_KEY": "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789",
-            "ALPACA_BASE_URL": "https://paper-api.alpaca.markets",
-            "BOT_MODE": "balanced",
-        }
-        
-        value = os.getenv(key, defaults.get(key, default))
-        if required and not value:
-            raise RuntimeError(f"Required environment variable {key} not set")
-        return value
-
-# Replace the config module with our mock
+        return {'conf_threshold': self.conf_threshold, 'buy_threshold': self.buy_threshold, 'min_confidence': self.min_confidence, 'confirmation_count': self.confirmation_count, 'take_profit_factor': self.take_profit_factor, 'trailing_factor': self.trailing_factor, 'scaling_factor': self.scaling_factor}
 sys.modules['config'] = MockConfig
-
 from ai_trading import meta_learning
 
 def test_trigger_meta_learning_conversion_pure_meta_format():
     """Test trigger function with pure meta-learning format - should return True immediately."""
     with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
-        # Write meta-learning format data
-        f.write("symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n")
-        f.write("TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n")
-        f.write("AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n")
-        f.write("MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n")
+        f.write('symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n')
+        f.write('TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n')
+        f.write('AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n')
+        f.write('MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n')
         test_file = f.name
-    
     try:
-        # Set the trade log file path
         MockConfig.TRADE_LOG_FILE = test_file
-        
-        # Test trade data
-        test_trade = {
-            'symbol': 'TEST', 
-            'qty': 10, 
-            'side': 'buy', 
-            'price': 100.0, 
-            'timestamp': '2025-08-05T23:17:35Z', 
-            'order_id': 'test-001', 
-            'status': 'filled'
-        }
-        
-        # Verify quality report shows pure meta format
+        test_trade = {'symbol': 'TEST', 'qty': 10, 'side': 'buy', 'price': 100.0, 'timestamp': '2025-08-05T23:17:35Z', 'order_id': 'test-001', 'status': 'filled'}
         quality_report = meta_learning.validate_trade_data_quality(test_file)
         assert quality_report['mixed_format_detected'] is False
         assert quality_report['audit_format_rows'] == 0
         assert quality_report['meta_format_rows'] > 0
-        
-        # Test the trigger function - should return True immediately (no conversion needed)
         result = meta_learning.trigger_meta_learning_conversion(test_trade)
         assert result is True
-        
     finally:
         if os.path.exists(test_file):
             os.unlink(test_file)
 
-
 def test_trigger_meta_learning_conversion_pure_audit_format():
     """Test trigger function with pure audit format - should attempt conversion."""
     with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
-        # Write audit format data
-        f.write("order_id,timestamp,symbol,side,qty,price,mode,status\n")
-        f.write("123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:17:35Z,TEST,buy,10,100.0,live,filled\n")
-        f.write("234e5678-e89b-12d3-a456-426614174001,2025-08-05T23:18:35Z,TEST,sell,10,105.0,live,filled\n")
-        f.write("345e6789-e89b-12d3-a456-426614174002,2025-08-05T23:19:35Z,AAPL,buy,5,150.0,live,filled\n")
+        f.write('order_id,timestamp,symbol,side,qty,price,mode,status\n')
+        f.write('123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:17:35Z,TEST,buy,10,100.0,live,filled\n')
+        f.write('234e5678-e89b-12d3-a456-426614174001,2025-08-05T23:18:35Z,TEST,sell,10,105.0,live,filled\n')
+        f.write('345e6789-e89b-12d3-a456-426614174002,2025-08-05T23:19:35Z,AAPL,buy,5,150.0,live,filled\n')
         test_file = f.name
-    
     try:
-        # Set the trade log file path
         MockConfig.TRADE_LOG_FILE = test_file
-        
-        # Test trade data
-        test_trade = {
-            'symbol': 'TEST', 
-            'qty': 10, 
-            'side': 'buy', 
-            'price': 100.0, 
-            'timestamp': '2025-08-05T23:17:35Z', 
-            'order_id': 'test-001', 
-            'status': 'filled'
-        }
-        
-        # Verify quality report shows pure audit format
+        test_trade = {'symbol': 'TEST', 'qty': 10, 'side': 'buy', 'price': 100.0, 'timestamp': '2025-08-05T23:17:35Z', 'order_id': 'test-001', 'status': 'filled'}
         quality_report = meta_learning.validate_trade_data_quality(test_file)
         assert quality_report['mixed_format_detected'] is False
         assert quality_report['audit_format_rows'] > 0
         assert quality_report['meta_format_rows'] == 0
-        
-        # Test the trigger function - should attempt conversion and return True if successful
         result = meta_learning.trigger_meta_learning_conversion(test_trade)
-        assert result is True  # Should succeed in conversion
-        
+        assert result is True
     finally:
         if os.path.exists(test_file):
             os.unlink(test_file)
 
-
 def test_trigger_meta_learning_conversion_mixed_format():
     """Test trigger function with mixed format - should attempt conversion."""
     with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
-        # Write mixed format data (meta headers with audit data)
-        f.write("symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n")
-        f.write("123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:17:35Z,TEST,buy,10,100.0,live,filled\n")
-        f.write("234e5678-e89b-12d3-a456-426614174001,2025-08-05T23:18:35Z,TEST,sell,10,105.0,live,filled\n")
+        f.write('symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n')
+        f.write('123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:17:35Z,TEST,buy,10,100.0,live,filled\n')
+        f.write('234e5678-e89b-12d3-a456-426614174001,2025-08-05T23:18:35Z,TEST,sell,10,105.0,live,filled\n')
         test_file = f.name
-    
     try:
-        # Set the trade log file path
         MockConfig.TRADE_LOG_FILE = test_file
-        
-        # Test trade data
-        test_trade = {
-            'symbol': 'TEST', 
-            'qty': 10, 
-            'side': 'buy', 
-            'price': 100.0, 
-            'timestamp': '2025-08-05T23:17:35Z', 
-            'order_id': 'test-001', 
-            'status': 'filled'
-        }
-        
-        # Verify quality report shows mixed format
+        test_trade = {'symbol': 'TEST', 'qty': 10, 'side': 'buy', 'price': 100.0, 'timestamp': '2025-08-05T23:17:35Z', 'order_id': 'test-001', 'status': 'filled'}
         quality_report = meta_learning.validate_trade_data_quality(test_file)
         assert quality_report['mixed_format_detected'] is True
-        
-        # Test the trigger function - should attempt conversion and return True if successful
         result = meta_learning.trigger_meta_learning_conversion(test_trade)
-        assert result is True  # Should succeed in conversion
-        
+        assert result is True
     finally:
         if os.path.exists(test_file):
             os.unlink(test_file)
 
-
 def test_trigger_meta_learning_conversion_missing_file():
     """Test trigger function with missing file - should return False."""
-    # Set a non-existent file path
     MockConfig.TRADE_LOG_FILE = '/tmp/non_existent_file.csv'
-    
-    test_trade = {
-        'symbol': 'TEST', 
-        'qty': 10, 
-        'side': 'buy', 
-        'price': 100.0, 
-        'timestamp': '2025-08-05T23:17:35Z', 
-        'order_id': 'test-001', 
-        'status': 'filled'
-    }
-    
-    # Test the trigger function - should return False for missing file
+    test_trade = {'symbol': 'TEST', 'qty': 10, 'side': 'buy', 'price': 100.0, 'timestamp': '2025-08-05T23:17:35Z', 'order_id': 'test-001', 'status': 'filled'}
     result = meta_learning.trigger_meta_learning_conversion(test_trade)
     assert result is False
-
 
 def test_trigger_meta_learning_conversion_problem_statement_exact():
     """Test the exact scenario from the problem statement."""
     with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
-        # Create exactly the scenario: mixed_format_detected=False, audit_format_rows=0, meta_format_rows=4
-        f.write("symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n")
-        f.write("TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n")
-        f.write("AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n")
-        f.write("MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n")
-        f.write("GOOGL,2025-08-05T23:23:35Z,2500.0,2025-08-05T23:24:35Z,2505.0,1,buy,test_strategy,test,signal5,0.9,5.0\n")
+        f.write('symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n')
+        f.write('TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n')
+        f.write('AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n')
+        f.write('MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n')
+        f.write('GOOGL,2025-08-05T23:23:35Z,2500.0,2025-08-05T23:24:35Z,2505.0,1,buy,test_strategy,test,signal5,0.9,5.0\n')
         test_file = f.name
-    
     try:
         MockConfig.TRADE_LOG_FILE = test_file
-        
-        test_trade = {
-            'symbol': 'TEST', 
-            'qty': 10, 
-            'side': 'buy', 
-            'price': 100.0, 
-            'timestamp': '2025-08-05T23:17:35Z', 
-            'order_id': 'test-001', 
-            'status': 'filled'
-        }
-        
-        # Verify we have the exact scenario from problem statement
+        test_trade = {'symbol': 'TEST', 'qty': 10, 'side': 'buy', 'price': 100.0, 'timestamp': '2025-08-05T23:17:35Z', 'order_id': 'test-001', 'status': 'filled'}
         quality_report = meta_learning.validate_trade_data_quality(test_file)
         assert quality_report['mixed_format_detected'] is False
         assert quality_report['audit_format_rows'] == 0
-        assert quality_report['meta_format_rows'] > 0  # Should be 5 (4 data + 1 header)
-        
-        # This should return True immediately (no conversion needed)
+        assert quality_report['meta_format_rows'] > 0
         result = meta_learning.trigger_meta_learning_conversion(test_trade)
-        assert result is True, "Should return True for properly formatted meta-learning files"
-        
+        assert result is True, 'Should return True for properly formatted meta-learning files'
     finally:
         if os.path.exists(test_file):
             os.unlink(test_file)--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/institutional/framework.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/institutional/framework.py@@ -7,129 +7,12 @@ - Risk management scenario testing
 - Performance and compliance testing
 """
-
 import asyncio
 import time
 from datetime import datetime, timezone
 from typing import Dict, List, Any, Optional
 import logging
-
-# Use the centralized logger as per AGENTS.md
 from ai_trading.logging import logger
-
-
-class MockMarketDataProvider:
-    """
-    Mock market data provider for out-of-hours testing.
-    
-    Provides realistic market data scenarios for comprehensive testing
-    without requiring live market data feeds.
-    """
-    
-    def __init__(self):
-        """Initialize mock market data provider."""
-        self.symbols = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "SPY", "QQQ"]
-        self.current_prices = {}
-        self.price_history = {}
-        self._initialize_prices()
-    
-    def _initialize_prices(self):
-        """Initialize realistic starting prices for test symbols."""
-        base_prices = {
-            "AAPL": 175.00,
-            "MSFT": 350.00,
-            "GOOGL": 140.00,
-            "AMZN": 145.00,
-            "TSLA": 250.00,
-            "SPY": 450.00,
-            "QQQ": 380.00
-        }
-        
-        for symbol, price in base_prices.items():
-            self.current_prices[symbol] = price
-            self.price_history[symbol] = [price]
-    
-    def get_current_price(self, symbol: str) -> Optional[float]:
-        """Get current price for a symbol."""
-        return self.current_prices.get(symbol)
-    
-    def get_price_history(self, symbol: str, periods: int = 100) -> List[float]:
-        """Get price history for a symbol."""
-        history = self.price_history.get(symbol, [])
-        return history[-periods:] if len(history) > periods else history
-    
-    def simulate_market_movement(self, volatility: float = 0.01):
-        """Simulate realistic market price movements."""
-        import random
-        
-        for symbol in self.symbols:
-            current_price = self.current_prices[symbol]
-            
-            # Generate realistic price movement
-            change_pct = random.normalvariate(0, volatility)
-            new_price = current_price * (1 + change_pct)
-            
-            # Ensure price stays positive
-            new_price = max(new_price, current_price * 0.5)
-            
-            self.current_prices[symbol] = new_price
-            self.price_history[symbol].append(new_price)
-            
-            # Keep history reasonable size
-            if len(self.price_history[symbol]) > 1000:
-                self.price_history[symbol] = self.price_history[symbol][-500:]
-    
-    def create_market_scenario(self, scenario_type: str) -> None:
-        """Create specific market scenarios for testing."""
-        scenarios = {
-            "bull_market": self._create_bull_market,
-            "bear_market": self._create_bear_market,
-            "high_volatility": self._create_high_volatility,
-            "flash_crash": self._create_flash_crash,
-            "sideways": self._create_sideways_market,
-            "gap_up": self._create_gap_up,
-            "gap_down": self._create_gap_down
-        }
-        
-        if scenario_type in scenarios:
-            scenarios[scenario_type]()
-            logger.info(f"Created market scenario: {scenario_type}")
-        else:
-            logger.warning(f"Unknown market scenario: {scenario_type}")
-    
-    def _create_bull_market(self):
-        """Simulate bull market conditions."""
-        for symbol in self.symbols:
-            self.current_prices[symbol] *= 1.02  # 2% up
-    
-    def _create_bear_market(self):
-        """Simulate bear market conditions."""
-        for symbol in self.symbols:
-            self.current_prices[symbol] *= 0.98  # 2% down
-    
-    def _create_high_volatility(self):
-        """Simulate high volatility market."""
-        self.simulate_market_movement(volatility=0.05)  # 5% volatility
-    
-    def _create_flash_crash(self):
-        """Simulate flash crash scenario."""
-        for symbol in self.symbols:
-            self.current_prices[symbol] *= 0.85  # 15% sudden drop
-    
-    def _create_sideways_market(self):
-        """Simulate sideways market with minimal movement."""
-        self.simulate_market_movement(volatility=0.001)  # 0.1% volatility
-    
-    def _create_gap_up(self):
-        """Simulate gap up scenario."""
-        for symbol in self.symbols:
-            self.current_prices[symbol] *= 1.05  # 5% gap up
-    
-    def _create_gap_down(self):
-        """Simulate gap down scenario."""
-        for symbol in self.symbols:
-            self.current_prices[symbol] *= 0.95  # 5% gap down
-
 
 class TradingScenarioRunner:
     """
@@ -138,330 +21,187 @@     Executes various trading scenarios to validate bot behavior
     under different market conditions and operational scenarios.
     """
-    
+
     def __init__(self, execution_engine=None):
         """Initialize scenario runner."""
         self.execution_engine = execution_engine
         self.market_data = MockMarketDataProvider()
         self.test_results = []
-        
+
     async def run_end_to_end_test(self) -> Dict[str, Any]:
         """Run comprehensive end-to-end trading test."""
-        logger.info("Starting end-to-end trading test")
+        logger.info('Starting end-to-end trading test')
         start_time = time.time()
-        
-        results = {
-            "test_name": "end_to_end_trading",
-            "start_time": datetime.now(timezone.utc),
-            "scenarios": [],
-            "overall_status": "unknown",
-            "duration": 0.0
-        }
-        
-        scenarios = [
-            ("initialization", self._test_initialization),
-            ("market_order", self._test_market_order),
-            ("limit_order", self._test_limit_order),
-            ("order_cancellation", self._test_order_cancellation),
-            ("multiple_orders", self._test_multiple_orders),
-            ("error_handling", self._test_error_handling),
-            ("circuit_breaker", self._test_circuit_breaker)
-        ]
-        
+        results = {'test_name': 'end_to_end_trading', 'start_time': datetime.now(timezone.utc), 'scenarios': [], 'overall_status': 'unknown', 'duration': 0.0}
+        scenarios = [('initialization', self._test_initialization), ('market_order', self._test_market_order), ('limit_order', self._test_limit_order), ('order_cancellation', self._test_order_cancellation), ('multiple_orders', self._test_multiple_orders), ('error_handling', self._test_error_handling), ('circuit_breaker', self._test_circuit_breaker)]
         passed = 0
         total = len(scenarios)
-        
         for scenario_name, test_func in scenarios:
             try:
-                logger.info(f"Running scenario: {scenario_name}")
+                logger.info(f'Running scenario: {scenario_name}')
                 scenario_result = await test_func()
-                scenario_result["name"] = scenario_name
-                results["scenarios"].append(scenario_result)
-                
-                if scenario_result["status"] == "passed":
+                scenario_result['name'] = scenario_name
+                results['scenarios'].append(scenario_result)
+                if scenario_result['status'] == 'passed':
                     passed += 1
-                    logger.info(f"✅ {scenario_name} passed")
+                    logger.info(f'✅ {scenario_name} passed')
                 else:
                     logger.error(f"❌ {scenario_name} failed: {scenario_result.get('error', 'Unknown error')}")
-                    
             except Exception as e:
-                logger.error(f"❌ {scenario_name} failed with exception: {e}")
-                results["scenarios"].append({
-                    "name": scenario_name,
-                    "status": "failed",
-                    "error": str(e)
-                })
-        
-        results["duration"] = time.time() - start_time
-        results["overall_status"] = "passed" if passed == total else "failed"
-        results["pass_rate"] = passed / total if total > 0 else 0
-        
-        logger.info(f"End-to-end test completed: {passed}/{total} scenarios passed")
-        return results
-    
+                logger.error(f'❌ {scenario_name} failed with exception: {e}')
+                results['scenarios'].append({'name': scenario_name, 'status': 'failed', 'error': str(e)})
+        results['duration'] = time.time() - start_time
+        results['overall_status'] = 'passed' if passed == total else 'failed'
+        results['pass_rate'] = passed / total if total > 0 else 0
+        logger.info(f'End-to-end test completed: {passed}/{total} scenarios passed')
+        return results
+
     async def run_risk_scenario_tests(self) -> Dict[str, Any]:
         """Run risk management scenario tests."""
-        logger.info("Starting risk scenario tests")
-        
-        results = {
-            "test_name": "risk_scenarios",
-            "start_time": datetime.now(timezone.utc),
-            "scenarios": [],
-            "overall_status": "unknown"
-        }
-        
-        risk_scenarios = [
-            ("position_sizing", self._test_position_sizing),
-            ("max_drawdown", self._test_max_drawdown),
-            ("sector_exposure", self._test_sector_exposure),
-            ("leverage_limits", self._test_leverage_limits),
-            ("volatility_adjustment", self._test_volatility_adjustment)
-        ]
-        
+        logger.info('Starting risk scenario tests')
+        results = {'test_name': 'risk_scenarios', 'start_time': datetime.now(timezone.utc), 'scenarios': [], 'overall_status': 'unknown'}
+        risk_scenarios = [('position_sizing', self._test_position_sizing), ('max_drawdown', self._test_max_drawdown), ('sector_exposure', self._test_sector_exposure), ('leverage_limits', self._test_leverage_limits), ('volatility_adjustment', self._test_volatility_adjustment)]
         passed = 0
         total = len(risk_scenarios)
-        
         for scenario_name, test_func in risk_scenarios:
             try:
                 scenario_result = await test_func()
-                scenario_result["name"] = scenario_name
-                results["scenarios"].append(scenario_result)
-                
-                if scenario_result["status"] == "passed":
+                scenario_result['name'] = scenario_name
+                results['scenarios'].append(scenario_result)
+                if scenario_result['status'] == 'passed':
                     passed += 1
-                    
             except Exception as e:
-                results["scenarios"].append({
-                    "name": scenario_name,
-                    "status": "failed",
-                    "error": str(e)
-                })
-        
-        results["overall_status"] = "passed" if passed == total else "failed"
-        results["pass_rate"] = passed / total if total > 0 else 0
-        
-        return results
-    
+                results['scenarios'].append({'name': scenario_name, 'status': 'failed', 'error': str(e)})
+        results['overall_status'] = 'passed' if passed == total else 'failed'
+        results['pass_rate'] = passed / total if total > 0 else 0
+        return results
+
     async def run_performance_tests(self) -> Dict[str, Any]:
         """Run performance and latency tests."""
-        logger.info("Starting performance tests")
-        
-        results = {
-            "test_name": "performance",
-            "start_time": datetime.now(timezone.utc),
-            "metrics": {},
-            "overall_status": "unknown"
-        }
-        
-        # Order execution latency test
+        logger.info('Starting performance tests')
+        results = {'test_name': 'performance', 'start_time': datetime.now(timezone.utc), 'metrics': {}, 'overall_status': 'unknown'}
         latencies = []
         for i in range(10):
             start = time.time()
-            # Simulate order execution
-            await asyncio.sleep(0.01)  # Mock execution delay
-            latency = (time.time() - start) * 1000  # Convert to ms
+            await asyncio.sleep(0.01)
+            latency = (time.time() - start) * 1000
             latencies.append(latency)
-        
-        results["metrics"]["average_latency_ms"] = sum(latencies) / len(latencies)
-        results["metrics"]["max_latency_ms"] = max(latencies)
-        results["metrics"]["min_latency_ms"] = min(latencies)
-        
-        # Throughput test
+        results['metrics']['average_latency_ms'] = sum(latencies) / len(latencies)
+        results['metrics']['max_latency_ms'] = max(latencies)
+        results['metrics']['min_latency_ms'] = min(latencies)
         start_time = time.time()
         orders_processed = 0
-        
         for i in range(100):
-            # Simulate order processing
             await asyncio.sleep(0.001)
             orders_processed += 1
-        
         duration = time.time() - start_time
-        results["metrics"]["orders_per_second"] = orders_processed / duration
-        
-        # Memory usage simulation
-        results["metrics"]["memory_usage_mb"] = 50.0  # Mock value
-        
-        # Determine pass/fail based on thresholds
-        performance_ok = (
-            results["metrics"]["average_latency_ms"] < 100 and
-            results["metrics"]["orders_per_second"] > 50 and
-            results["metrics"]["memory_usage_mb"] < 100
-        )
-        
-        results["overall_status"] = "passed" if performance_ok else "failed"
-        return results
-    
-    # Individual test scenarios
+        results['metrics']['orders_per_second'] = orders_processed / duration
+        results['metrics']['memory_usage_mb'] = 50.0
+        performance_ok = results['metrics']['average_latency_ms'] < 100 and results['metrics']['orders_per_second'] > 50 and (results['metrics']['memory_usage_mb'] < 100)
+        results['overall_status'] = 'passed' if performance_ok else 'failed'
+        return results
+
     async def _test_initialization(self) -> Dict[str, Any]:
         """Test system initialization."""
         if self.execution_engine:
             success = self.execution_engine.initialize()
-            return {
-                "status": "passed" if success else "failed",
-                "details": "Execution engine initialization"
-            }
+            return {'status': 'passed' if success else 'failed', 'details': 'Execution engine initialization'}
         else:
-            return {
-                "status": "passed",
-                "details": "No execution engine to test"
-            }
-    
+            return {'status': 'passed', 'details': 'No execution engine to test'}
+
     async def _test_market_order(self) -> Dict[str, Any]:
         """Test market order execution."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
-        try:
-            result = self.execution_engine.submit_market_order("AAPL", "buy", 100)
-            return {
-                "status": "passed" if result else "failed",
-                "details": f"Market order result: {result}",
-                "order_id": result.get("id") if result else None
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
+            return {'status': 'skipped', 'details': 'No execution engine'}
+        try:
+            result = self.execution_engine.submit_market_order('AAPL', 'buy', 100)
+            return {'status': 'passed' if result else 'failed', 'details': f'Market order result: {result}', 'order_id': result.get('id') if result else None}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_limit_order(self) -> Dict[str, Any]:
         """Test limit order execution."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
-        try:
-            current_price = self.market_data.get_current_price("AAPL")
+            return {'status': 'skipped', 'details': 'No execution engine'}
+        try:
+            current_price = self.market_data.get_current_price('AAPL')
             limit_price = current_price * 0.99 if current_price else 170.0
-            
-            result = self.execution_engine.submit_limit_order("AAPL", "buy", 100, limit_price)
-            return {
-                "status": "passed" if result else "failed",
-                "details": f"Limit order result: {result}",
-                "order_id": result.get("id") if result else None
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
+            result = self.execution_engine.submit_limit_order('AAPL', 'buy', 100, limit_price)
+            return {'status': 'passed' if result else 'failed', 'details': f'Limit order result: {result}', 'order_id': result.get('id') if result else None}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_order_cancellation(self) -> Dict[str, Any]:
         """Test order cancellation."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
-        try:
-            # Submit order first
-            order_result = self.execution_engine.submit_limit_order("AAPL", "buy", 100, 150.0)
+            return {'status': 'skipped', 'details': 'No execution engine'}
+        try:
+            order_result = self.execution_engine.submit_limit_order('AAPL', 'buy', 100, 150.0)
             if not order_result:
-                return {"status": "failed", "details": "Could not create order to cancel"}
-            
-            # Cancel the order
-            order_id = order_result.get("id")
+                return {'status': 'failed', 'details': 'Could not create order to cancel'}
+            order_id = order_result.get('id')
             cancel_result = self.execution_engine.cancel_order(order_id)
-            
-            return {
-                "status": "passed" if cancel_result else "failed",
-                "details": f"Cancellation result: {cancel_result}"
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
+            return {'status': 'passed' if cancel_result else 'failed', 'details': f'Cancellation result: {cancel_result}'}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_multiple_orders(self) -> Dict[str, Any]:
         """Test multiple simultaneous orders."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
+            return {'status': 'skipped', 'details': 'No execution engine'}
         try:
             orders = []
-            symbols = ["AAPL", "MSFT", "GOOGL"]
-            
+            symbols = ['AAPL', 'MSFT', 'GOOGL']
             for symbol in symbols:
-                result = self.execution_engine.submit_market_order(symbol, "buy", 10)
+                result = self.execution_engine.submit_market_order(symbol, 'buy', 10)
                 orders.append(result)
-            
             successful_orders = [o for o in orders if o is not None]
-            
-            return {
-                "status": "passed" if len(successful_orders) >= 2 else "failed",
-                "details": f"Successfully submitted {len(successful_orders)}/{len(symbols)} orders"
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
+            return {'status': 'passed' if len(successful_orders) >= 2 else 'failed', 'details': f'Successfully submitted {len(successful_orders)}/{len(symbols)} orders'}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_error_handling(self) -> Dict[str, Any]:
         """Test error handling with invalid orders."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
-        try:
-            # Try invalid symbol
-            result1 = self.execution_engine.submit_market_order("INVALID", "buy", 100)
-            
-            # Try invalid quantity
-            result2 = self.execution_engine.submit_market_order("AAPL", "buy", -100)
-            
-            # Both should fail gracefully
-            errors_handled = (result1 is None) and (result2 is None)
-            
-            return {
-                "status": "passed" if errors_handled else "failed",
-                "details": "Error handling validation"
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
+            return {'status': 'skipped', 'details': 'No execution engine'}
+        try:
+            result1 = self.execution_engine.submit_market_order('INVALID', 'buy', 100)
+            result2 = self.execution_engine.submit_market_order('AAPL', 'buy', -100)
+            errors_handled = result1 is None and result2 is None
+            return {'status': 'passed' if errors_handled else 'failed', 'details': 'Error handling validation'}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_circuit_breaker(self) -> Dict[str, Any]:
         """Test circuit breaker functionality."""
         if not self.execution_engine:
-            return {"status": "skipped", "details": "No execution engine"}
-        
-        try:
-            # Get initial stats
+            return {'status': 'skipped', 'details': 'No execution engine'}
+        try:
             initial_stats = self.execution_engine.get_execution_stats()
-            
-            # Circuit breaker test would require actual failure simulation
-            # For now, just verify the circuit breaker status is available
-            has_circuit_breaker = "circuit_breaker_status" in initial_stats
-            
-            return {
-                "status": "passed" if has_circuit_breaker else "failed",
-                "details": "Circuit breaker status check"
-            }
-        except Exception as e:
-            return {"status": "failed", "error": str(e)}
-    
-    # Risk scenario tests
+            has_circuit_breaker = 'circuit_breaker_status' in initial_stats
+            return {'status': 'passed' if has_circuit_breaker else 'failed', 'details': 'Circuit breaker status check'}
+        except Exception as e:
+            return {'status': 'failed', 'error': str(e)}
+
     async def _test_position_sizing(self) -> Dict[str, Any]:
         """Test position sizing logic."""
-        # Mock position sizing test
-        return {
-            "status": "passed",
-            "details": "Position sizing validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Position sizing validation passed'}
+
     async def _test_max_drawdown(self) -> Dict[str, Any]:
         """Test maximum drawdown limits."""
-        return {
-            "status": "passed",
-            "details": "Drawdown limits validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Drawdown limits validation passed'}
+
     async def _test_sector_exposure(self) -> Dict[str, Any]:
         """Test sector exposure limits."""
-        return {
-            "status": "passed",
-            "details": "Sector exposure validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Sector exposure validation passed'}
+
     async def _test_leverage_limits(self) -> Dict[str, Any]:
         """Test leverage limit enforcement."""
-        return {
-            "status": "passed",
-            "details": "Leverage limits validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Leverage limits validation passed'}
+
     async def _test_volatility_adjustment(self) -> Dict[str, Any]:
         """Test volatility-based position adjustment."""
-        return {
-            "status": "passed",
-            "details": "Volatility adjustment validation passed"
-        }
-
+        return {'status': 'passed', 'details': 'Volatility adjustment validation passed'}
 
 class ComplianceTestSuite:
     """
@@ -470,85 +210,47 @@     Validates regulatory compliance, audit trail integrity,
     and risk management compliance.
     """
-    
+
     def __init__(self):
         """Initialize compliance test suite."""
         self.test_results = []
-    
+
     async def run_compliance_tests(self) -> Dict[str, Any]:
         """Run full compliance test suite."""
-        logger.info("Starting compliance tests")
-        
-        results = {
-            "test_name": "compliance",
-            "start_time": datetime.now(timezone.utc),
-            "tests": [],
-            "overall_status": "unknown"
-        }
-        
-        compliance_tests = [
-            ("audit_trail", self._test_audit_trail),
-            ("trade_logging", self._test_trade_logging),
-            ("risk_limits", self._test_risk_limits),
-            ("order_validation", self._test_order_validation),
-            ("data_retention", self._test_data_retention)
-        ]
-        
+        logger.info('Starting compliance tests')
+        results = {'test_name': 'compliance', 'start_time': datetime.now(timezone.utc), 'tests': [], 'overall_status': 'unknown'}
+        compliance_tests = [('audit_trail', self._test_audit_trail), ('trade_logging', self._test_trade_logging), ('risk_limits', self._test_risk_limits), ('order_validation', self._test_order_validation), ('data_retention', self._test_data_retention)]
         passed = 0
         total = len(compliance_tests)
-        
         for test_name, test_func in compliance_tests:
             try:
                 test_result = await test_func()
-                test_result["name"] = test_name
-                results["tests"].append(test_result)
-                
-                if test_result["status"] == "passed":
+                test_result['name'] = test_name
+                results['tests'].append(test_result)
+                if test_result['status'] == 'passed':
                     passed += 1
-                    
             except Exception as e:
-                results["tests"].append({
-                    "name": test_name,
-                    "status": "failed",
-                    "error": str(e)
-                })
-        
-        results["overall_status"] = "passed" if passed == total else "failed"
-        results["compliance_score"] = passed / total if total > 0 else 0
-        
-        return results
-    
+                results['tests'].append({'name': test_name, 'status': 'failed', 'error': str(e)})
+        results['overall_status'] = 'passed' if passed == total else 'failed'
+        results['compliance_score'] = passed / total if total > 0 else 0
+        return results
+
     async def _test_audit_trail(self) -> Dict[str, Any]:
         """Test audit trail completeness."""
-        return {
-            "status": "passed",
-            "details": "Audit trail validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Audit trail validation passed'}
+
     async def _test_trade_logging(self) -> Dict[str, Any]:
         """Test trade logging compliance."""
-        return {
-            "status": "passed",
-            "details": "Trade logging validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Trade logging validation passed'}
+
     async def _test_risk_limits(self) -> Dict[str, Any]:
         """Test risk limit enforcement."""
-        return {
-            "status": "passed",
-            "details": "Risk limits validation passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Risk limits validation passed'}
+
     async def _test_order_validation(self) -> Dict[str, Any]:
         """Test order validation compliance."""
-        return {
-            "status": "passed",
-            "details": "Order validation compliance passed"
-        }
-    
+        return {'status': 'passed', 'details': 'Order validation compliance passed'}
+
     async def _test_data_retention(self) -> Dict[str, Any]:
         """Test data retention compliance."""
-        return {
-            "status": "passed",
-            "details": "Data retention compliance passed"
-        }+        return {'status': 'passed', 'details': 'Data retention compliance passed'}--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/mocks/tenacity_mock.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/mocks/tenacity_mock.py@@ -1,12 +1,8 @@-# Mock for tenacity
 def retry(*args, **kwargs):
+
     def decorator(func):
         return func
     return decorator
-
-class MockWait:
-    def __add__(self, other):
-        return self
 
 class RetryError(Exception):
     """Mock RetryError exception for testing."""
@@ -22,7 +18,4 @@     return MockWait()
 
 def retry_if_exception_type(*args):
-    return None
-
-def __getattr__(name):
-    return lambda *args, **kwargs: lambda f: f
+    return None--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/support/mocks_runtime.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/support/mocks_runtime.py@@ -1,267 +1,4 @@-# tests/support/mocks_runtime.py
-# Placeholders for any legacy Mock* classes moved out of runtime.
-# Import and use within tests only.
-
 """
 Mock classes that were previously used for import fallbacks.
 These are now only for testing purposes.
-"""
-
-class MockNumpy:
-    """Mock NumPy implementation for testing environments."""
-    def __init__(self):
-        # Constants
-        self.nan = float("nan")
-        self.inf = float("inf")
-        self.pi = 3.141592653589793
-        self.e = 2.718281828459045
-
-        # Data types
-        self.float64 = float
-        self.int64 = int
-        
-    def array(self, *args, **kwargs):
-        return list(args[0]) if args else []
-        
-    def mean(self, arr):
-        return sum(arr) / len(arr) if arr else 0
-        
-    def std(self, arr):
-        return 1.0  # Mock implementation
-        
-    def zeros(self, shape):
-        if isinstance(shape, int):
-            return [0] * shape
-        return []
-
-
-class MockPandas:
-    """Mock Pandas implementation for testing environments."""
-    def __init__(self):
-        self.DataFrame = MockDataFrame
-        self.Series = MockSeries
-        
-    def read_csv(self, *args, **kwargs):
-        return MockDataFrame()
-
-
-class MockDataFrame:
-    """Mock DataFrame implementation."""
-    def __init__(self, data=None):
-        self.data = data or {}
-        
-    def __getitem__(self, key):
-        return MockSeries()
-        
-    def __setitem__(self, key, value):
-        pass
-        
-    def head(self, n=5):
-        return self
-        
-    def tail(self, n=5):
-        return self
-        
-    def dropna(self):
-        return self
-
-
-class MockSeries:
-    """Mock Series implementation."""
-    def __init__(self, data=None):
-        self.data = data or []
-        
-    def __getitem__(self, key):
-        return 0
-        
-    def mean(self):
-        return 0.0
-        
-    def std(self):
-        return 1.0
-
-
-# Add more mock classes as needed when moving them from runtime code
-
-class MockFinBERT:
-    """Mock FinBERT model for testing."""
-    
-    def __call__(self, *args, **kwargs):
-        return self
-
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-    def tolist(self):
-        return [0.33, 0.34, 0.33]  # neutral sentiment
-
-
-class MockSklearn:
-    """Mock scikit-learn for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockTalib:
-    """Mock TA-Lib for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockPortalocker:
-    """Mock portalocker for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockSchedule:
-    """Mock schedule for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockYfinance:
-    """Mock yfinance for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-        
-    def Ticker(self, symbol):
-        return MockTicker()
-
-
-class MockTicker:
-    """Mock yfinance Ticker for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockBeautifulSoup:
-    """Mock BeautifulSoup for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockFlask:
-    """Mock Flask for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockAlpacaClient:
-    """Mock Alpaca trading client for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockCircuitBreaker:
-    """Mock circuit breaker for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockMetric:
-    """Mock prometheus metric for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockIndex:
-    """Mock pandas Index for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockRolling:
-    """Mock pandas rolling for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockTradingClient:
-    """Mock Alpaca trading client for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockMarketOrderRequest:
-    """Mock Alpaca market order request for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockLimitOrderRequest:
-    """Mock Alpaca limit order request for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockGetOrdersRequest:
-    """Mock Alpaca get orders request for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockOrder:
-    """Mock Alpaca order for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockTradingStream:
-    """Mock Alpaca trading stream for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockStockHistoricalDataClient:
-    """Mock Alpaca stock historical data client for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockQuote:
-    """Mock Alpaca quote for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockStockBarsRequest:
-    """Mock Alpaca stock bars request for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockStockLatestQuoteRequest:
-    """Mock Alpaca stock latest quote request for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self
-
-
-class MockTimeFrame:
-    """Mock Alpaca time frame for testing."""
-    
-    def __getattr__(self, name):
-        return lambda *args, **kwargs: self+"""--- /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/unit/test_alpaca_api.py+++ /mnt/data/ai-trading-bot-main-3/ai-trading-bot-main/tests/unit/test_alpaca_api.py@@ -1,42 +1,32 @@ import pytest
-
-alpaca_api = pytest.importorskip("alpaca_api", reason="alpaca_api module not found")
+alpaca_api = pytest.importorskip('alpaca_api', reason='alpaca_api module not found')
 
 @pytest.mark.unit
 def test_pending_orders_lock_exists_and_is_lock():
-    assert hasattr(alpaca_api, "_pending_orders_lock")
-    lock = getattr(alpaca_api, "_pending_orders_lock")
-    # Check that it has threading lock behavior
+    assert hasattr(alpaca_api, '_pending_orders_lock')
+    lock = getattr(alpaca_api, '_pending_orders_lock')
     lock_type = type(lock).__name__
-    assert lock_type in ["RLock", "Lock"], f"Expected RLock or Lock, got {lock_type}"
-    assert hasattr(alpaca_api, "_pending_orders")
+    assert lock_type in ['RLock', 'Lock'], f'Expected RLock or Lock, got {lock_type}'
+    assert hasattr(alpaca_api, '_pending_orders')
     assert isinstance(alpaca_api._pending_orders, dict)
 
 @pytest.mark.unit
 def test_submit_order_uses_client_and_returns(dummy_alpaca_client, monkeypatch):
-    submit = getattr(alpaca_api, "submit_order", None)
+    submit = getattr(alpaca_api, 'submit_order', None)
     if submit is None:
-        pytest.skip("submit_order not available")
-    
-    # Mock the DRY_RUN setting to False so the actual client is used
-    monkeypatch.setattr(alpaca_api, "DRY_RUN", False)
-    monkeypatch.setattr(alpaca_api, "SHADOW_MODE", False)
-    
-    # Create a simple order request object 
+        pytest.skip('submit_order not available')
+    monkeypatch.setattr(alpaca_api, 'DRY_RUN', False)
+    monkeypatch.setattr(alpaca_api, 'SHADOW_MODE', False)
+
     class OrderReq:
+
         def __init__(self):
-            self.symbol = "META"
+            self.symbol = 'META'
             self.qty = 1
-            self.side = "buy"
-            self.time_in_force = "day"
-        
-        def __getattr__(self, name):
-            if name == '_test_scenario':
-                return False
-            return None
-    
+            self.side = 'buy'
+            self.time_in_force = 'day'
     order_req = OrderReq()
     res = submit(dummy_alpaca_client, order_req)
     assert res is not None
-    assert getattr(res, "id", None) is not None
-    assert dummy_alpaca_client.calls, "Client submit_order should be called"+    assert getattr(res, 'id', None) is not None
+    assert dummy_alpaca_client.calls, 'Client submit_order should be called'