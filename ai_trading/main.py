from __future__ import annotations
import argparse
import copy
import os
import threading
import time
import logging
from threading import Thread
import errno
import socket
import signal
import sys
from datetime import datetime, UTC
from zoneinfo import ZoneInfo
from pathlib import Path
import importlib.util
from typing import Any, Callable, Tuple
from types import SimpleNamespace
from ai_trading.env import ensure_dotenv_loaded

# Ensure environment variables are loaded before any logging configuration
ensure_dotenv_loaded()

import ai_trading.logging as _logging
from ai_trading.paths import LOG_DIR, ensure_runtime_paths
from ai_trading.runtime.shutdown import register_signal_handlers, request_stop, should_stop
from ai_trading.data.fetch import DataFetchError, EmptyBarsError
from ai_trading.execution.live_trading import NonRetryableBrokerError
from ai_trading.utils.datetime import ensure_datetime


def _resolve_log_file() -> str:
    """Resolve the bot log path from environment or defaults."""

    explicit = os.getenv("BOT_LOG_FILE")
    if explicit:
        path = Path(explicit).expanduser()
        if not path.is_absolute():
            raise SystemExit("BOT_LOG_FILE must be an absolute path when set")
        path.parent.mkdir(parents=True, exist_ok=True)
        return str(path)
    ensure_runtime_paths()
    log_path = (LOG_DIR / "bot.log").resolve()
    log_path.parent.mkdir(parents=True, exist_ok=True)
    return str(log_path)


LOG_FILE = _resolve_log_file()
# Configure logging with the desired file
# Logging must be initialized once here before importing heavy modules like
# ``ai_trading.core.bot_engine``.
_logging.configure_logging(log_file=LOG_FILE)

# Module logger
logger = _logging.get_logger(__name__)
_AUTH_PREFLIGHT_LOGGED = False

_STARTUP_PENDING_RECONCILED = False
_RUNTIME_CACHE_LOCK = threading.Lock()
_STATE_CACHE: Any | None = None
_RUNTIME_CACHE: Any | None = None
_RUNTIME_CFG_SNAPSHOT: dict[str, Any] | None = None

# Detect Alpaca SDK availability without importing heavy modules
def _safe_find_spec(module_name: str):
    try:
        return importlib.util.find_spec(module_name)
    except ValueError:
        return None


ALPACA_AVAILABLE = (
    _safe_find_spec("alpaca") is not None
    and _safe_find_spec("alpaca.trading.client") is not None
)

from ai_trading.settings import get_seed_int
from ai_trading.config import get_settings
from ai_trading.utils import get_pid_on_port
from ai_trading.utils.prof import StageTimer, SoftBudget
from ai_trading.utils.time import monotonic_time
from ai_trading.logging.redact import redact as _redact
from ai_trading.env.config_redaction import redact_config_env
from ai_trading.net.http import build_retrying_session, set_global_session, mount_host_retry_profile
from ai_trading.utils.http import clamp_request_timeout
from ai_trading.utils.base import is_market_open as _is_market_open_base, next_market_open
from ai_trading.position_sizing import resolve_max_position_size, _get_equity_from_alpaca, _CACHE

try:  # prefer modern budget context
    from ai_trading.core.budget import set_cycle_budget_context
except Exception:  # pragma: no cover - fallback when budget module unavailable
    from contextlib import nullcontext

    def set_cycle_budget_context(*args, **kwargs):  # type: ignore[override]
        return nullcontext()

try:
    from ai_trading.core.bot_engine import (
        emit_cycle_budget_summary,
        clear_cycle_budget_context,
    )
except Exception:  # pragma: no cover - fallback when bot engine unavailable

    def emit_cycle_budget_summary(*args, **kwargs):
        return None

    def clear_cycle_budget_context(*args, **kwargs):
        return None
from ai_trading.config.management import (
    get_env,
    validate_required_env,
    reload_env,
    _resolve_alpaca_env,
    TradingConfig,
)
from ai_trading.metrics import get_histogram, get_counter
from ai_trading.utils.env import alpaca_credential_status


def _config_snapshot(cfg: Any) -> dict[str, Any]:
    """Return a deep-copied snapshot of config values for cache comparison."""

    try:
        return copy.deepcopy(cfg.to_dict())
    except Exception:  # pragma: no cover - defensive fallback
        return {"__repr__": repr(cfg)}


def _resolve_cached_context(
    cfg: Any,
    state_factory: Callable[[], Any],
    runtime_builder: Callable[[Any], Any],
) -> Tuple[Any, Any, bool]:
    """Return cached bot state/runtime or rebuild them when config changes."""

    global _STATE_CACHE, _RUNTIME_CACHE, _RUNTIME_CFG_SNAPSHOT

    snapshot = _config_snapshot(cfg)

    with _RUNTIME_CACHE_LOCK:
        cached_state = _STATE_CACHE
        cached_runtime = _RUNTIME_CACHE
        if (
            cached_state is None
            or cached_runtime is None
            or _RUNTIME_CFG_SNAPSHOT != snapshot
        ):
            state = state_factory()
            runtime = runtime_builder(cfg)
            try:
                runtime.cfg = cfg
            except Exception:  # pragma: no cover - runtime without cfg attribute
                pass
            _STATE_CACHE = state
            _RUNTIME_CACHE = runtime
            _RUNTIME_CFG_SNAPSHOT = snapshot
            reused = False
        else:
            state = cached_state
            runtime = cached_runtime
            try:
                runtime.cfg = cfg
            except Exception:  # pragma: no cover - runtime without cfg attribute
                pass
            reused = True

    return state, runtime, reused


def _reset_warmup_cooldown_timestamp() -> None:
    """Clear the cached state's cooldown after a warm-up cycle."""

    with _RUNTIME_CACHE_LOCK:
        state = _STATE_CACHE
        if state is None:
            return
        try:
            if hasattr(state, "last_run_at"):
                setattr(state, "last_run_at", None)
        except Exception:
            return


def _is_truthy_env(name: str) -> bool:
    """Return ``True`` when environment variable ``name`` is truthy."""

    value = os.environ.get(name, "")
    return value.strip().lower() in {"1", "true", "yes", "on"}


def _is_test_mode() -> bool:
    """Detect whether the process is running under automated tests."""

    def _flag_enabled(flag: str) -> bool:
        raw = os.getenv(flag)
        if raw is None:
            return False
        candidate = raw.strip()
        if not candidate:
            return False
        return candidate.lower() not in {"0", "false", "no", "off"}

    return any(_flag_enabled(flag) for flag in ("PYTEST_RUNNING", "TESTING"))


def preflight_import_health() -> bool:
    """Run best-effort import checks returning ``True`` when all succeed."""

    import importlib

    if _is_truthy_env("IMPORT_PREFLIGHT_DISABLED"):
        logger.info("IMPORT_PREFLIGHT_SKIPPED", extra={"reason": "env_override"})
        return True

    core_modules = [
        "ai_trading.core.bot_engine",
        "ai_trading.risk.engine",
        "ai_trading.rl_trading",
        "ai_trading.telemetry.metrics_logger",
        "alpaca.trading.client",
    ]
    failures: list[dict[str, str]] = []
    for mod in core_modules:
        try:
            importlib.import_module(mod)
        except (ImportError, RuntimeError) as exc:  # pragma: no cover - surface import issues
            info = {
                "module_name": mod,
                "error": repr(exc),
                "exc_type": exc.__class__.__name__,
            }
            failures.append(info)
            logger.error("IMPORT_PREFLIGHT_FAILED", extra=info)

    if failures:
        logger.warning(
            "IMPORT_PREFLIGHT_DEGRADED",
            extra={"failed_modules": [f["module_name"] for f in failures]},
        )
    else:
        logger.info("IMPORT_PREFLIGHT_OK")

    ensure_trade_log_path()
    return not failures


def should_enforce_strict_import_preflight() -> bool:
    """Return ``True`` when import preflight failures should abort startup."""

    if _is_truthy_env("IMPORT_PREFLIGHT_DISABLED"):
        return False

    if any(
        _is_truthy_env(flag)
        for flag in ("AI_TRADING_SYSTEMD_COMPAT", "SYSTEMD_COMPAT", "SYSTEMD_COMPAT_MODE")
    ):
        logger.info("IMPORT_PREFLIGHT_COMPAT_MODE", extra={"mode": "systemd"})
        return False

    if os.environ.get("PYTEST_CURRENT_TEST"):
        has_key, has_secret = alpaca_credential_status()
        if not (has_key and has_secret):
            logger.info(
                "IMPORT_PREFLIGHT_RELAXED",
                extra={"reason": "pytest_missing_credentials"},
            )
            return False

    return True


def _check_alpaca_sdk() -> None:
    """Ensure the Alpaca SDK is installed before continuing."""
    if not ALPACA_AVAILABLE:
        logger.error("ALPACA_PY_REQUIRED: pip install alpaca-py is required")
        raise SystemExit(1)


def run_cycle() -> None:
    """Execute a single trading cycle using the core bot engine."""

    if should_stop():
        logger.info("CYCLE_STOP_REQUESTED", extra={"stage": "start"})
        return

    from ai_trading.alpaca_api import (
        AlpacaAuthenticationError,
        alpaca_get,
        is_alpaca_service_available,
        _set_alpaca_service_available,
    )

    execution_mode = str(get_env("EXECUTION_MODE", "sim", cast=str) or "sim").lower()
    if execution_mode == "disabled":
        _set_alpaca_service_available(False)
        _log_auth_preflight_failure(
            detail="Execution mode is disabled",
            action="Set AI_TRADING_EXECUTION_MODE to paper or live",
        )
        return

    if execution_mode in {"paper", "live"}:
        has_key, has_secret = alpaca_credential_status()
        if not (has_key and has_secret):
            _set_alpaca_service_available(False)
            _log_auth_preflight_failure(
                detail="Missing Alpaca API credentials",
                action="Verify ALPACA_API_KEY/ALPACA_SECRET_KEY",
            )
            return

    allow_after_hours = bool(get_env("ALLOW_AFTER_HOURS", "0", cast=bool))
    if not allow_after_hours:
        try:
            if not _is_market_open_base():
                logger.info("MARKET_CLOSED_SKIP_CYCLE")
                return
        except Exception:
            logger.debug("MARKET_OPEN_CHECK_FAILED", exc_info=True)

    if not is_alpaca_service_available():
        _log_auth_preflight_failure(
            detail="Alpaca authentication previously marked unavailable",
            action="Verify ALPACA_API_KEY/ALPACA_SECRET_KEY",
        )
        return

    try:
        alpaca_get("/v2/account/configurations", timeout=5)
    except AlpacaAuthenticationError as exc:
        _log_auth_preflight_failure(
            detail=str(exc),
            action="Verify ALPACA_API_KEY/ALPACA_SECRET_KEY",
        )
        return
    except Exception as exc:  # pragma: no cover - defensive
        logger.warning(
            "ALPACA_PREFLIGHT_UNEXPECTED",
            extra={"detail": str(exc), "exc_type": exc.__class__.__name__},
        )

    from ai_trading.core.bot_engine import (
        BotState,
        run_all_trades_worker,
        get_ctx,
        get_trade_logger,
        ensure_alpaca_attached,
        list_open_orders,
        cancel_all_open_orders,
        get_confirmed_pending_orders,
        set_cycle_budget_context,
        emit_cycle_budget_summary,
        clear_cycle_budget_context,
    )
    from ai_trading.core.runtime import (
        build_runtime,
        REQUIRED_PARAM_DEFAULTS,
        enhance_runtime_with_context,
    )
    from ai_trading.config.management import TradingConfig
    from ai_trading.config import get_settings
    from ai_trading.data import fetch as data_fetcher_module

    # Ensure trade log file exists before any trade-log reads occur. The
    # ``get_trade_logger`` helper lazily creates the log and writes the header on
    # first use so downstream components can safely read from it during startup.
    get_trade_logger()

    if hasattr(data_fetcher_module, "is_primary_provider_enabled") and not data_fetcher_module.is_primary_provider_enabled():
        logger.info("PRIMARY_PROVIDER_DISABLED_CYCLE_SKIP")
        _interruptible_sleep(5.0)
        return

    cfg = TradingConfig.from_env()

    # Carry through a pre-resolved max position size if available on Settings.
    S = get_settings()
    mps = getattr(S, "max_position_size", None)
    if mps is not None:
        try:
            object.__setattr__(cfg, "max_position_size", float(mps))
        except Exception:
            try:
                setattr(cfg, "max_position_size", float(mps))
            except Exception:  # pragma: no cover - defensive
                pass

    state, runtime, _ = _resolve_cached_context(cfg, BotState, build_runtime)

    lazy_ctx = get_ctx()
    if hasattr(state, "ctx") and getattr(state, "ctx", None) is None:
        state.ctx = lazy_ctx
    runtime = enhance_runtime_with_context(runtime, lazy_ctx)

    if not isinstance(getattr(runtime, "state", None), dict):
        try:
            runtime.state = {}
        except Exception:
            pass

    global _STARTUP_PENDING_RECONCILED
    if not _STARTUP_PENDING_RECONCILED:
        try:
            ensure_alpaca_attached(runtime)
            api = getattr(runtime, "api", None)
            if api is None:
                raise RuntimeError("runtime.api missing")
            open_orders = list_open_orders(api)
            pending_orders = get_confirmed_pending_orders(
                api,
                open_orders,
                require_confirmation=True,
            )
        except Exception:
            logger.debug("STARTUP_PENDING_RECONCILE_SKIPPED", exc_info=True)
        else:
            now_dt = datetime.now(UTC)
            pending_ids: list[str] = []
            pending_ages: list[int | None] = []
            for order in pending_orders:
                status_raw = getattr(order, "status", "")
                status_val = getattr(status_raw, "value", status_raw)
                try:
                    status = str(status_val).lower()
                except Exception:
                    status = ""
                if status not in {"new", "pending_new"}:
                    continue
                pending_ids.append(str(getattr(order, "id", "?")))
                age_s: int | None = None
                for attr in ("updated_at", "submitted_at", "created_at"):
                    raw_ts = getattr(order, attr, None)
                    if not raw_ts:
                        continue
                    try:
                        submitted = ensure_datetime(raw_ts)
                    except Exception:
                        continue
                    age_s = int(max(0.0, (now_dt - submitted).total_seconds()))
                    break
                pending_ages.append(age_s)

            try:
                cleanup_after = float(getattr(cfg, "order_stale_cleanup_interval", 120))
            except (TypeError, ValueError):
                cleanup_after = 120.0
            cleanup_after = max(10.0, min(cleanup_after, 3600.0))
            cleanup_after_int = int(cleanup_after)

            if pending_ids:
                numeric_ages = [age for age in pending_ages if age is not None]
                should_cancel = False
                if numeric_ages:
                    max_age = max(numeric_ages)
                    should_cancel = max_age >= cleanup_after
                else:
                    should_cancel = True
                if should_cancel:
                    try:
                        cancel_all_open_orders(runtime)
                    except Exception as exc:
                        extra = {
                            "canceled_ids": pending_ids[:20],
                            "cleanup_after_s": cleanup_after_int,
                            "detail": str(exc),
                        }
                        if numeric_ages:
                            extra["max_age_s"] = max_age
                        logger.warning(
                            "PENDING_ORDERS_STARTUP_CLEANUP_FAILED",
                            extra=extra,
                            exc_info=True,
                        )
                    else:
                        extra = {
                            "canceled_ids": pending_ids[:20],
                            "cleanup_after_s": cleanup_after_int,
                        }
                        if numeric_ages:
                            extra["max_age_s"] = max_age
                        logger.info(
                            "PENDING_ORDERS_STARTUP_CLEANUP",
                            extra=extra,
                        )
            _STARTUP_PENDING_RECONCILED = True

    missing = [k for k in REQUIRED_PARAM_DEFAULTS if k not in runtime.params]
    if missing:
        logger.error(
            "PARAMS_VALIDATE: missing keys in runtime.params; defaults will be applied",
            extra={"missing": missing},
        )

    try:
        run_all_trades_worker(state, runtime)
    except (EmptyBarsError, DataFetchError, NonRetryableBrokerError) as exc:
        logger.warning(
            "WARMUP_SYMBOL_ERRORS_TOLERATED",
            extra={"error": str(exc), "exc_type": exc.__class__.__name__},
        )
        return


def get_memory_optimizer():
    from ai_trading.config import get_settings

    S = get_settings()
    if not S.enable_memory_optimization:
        return None
    from ai_trading.utils import memory_optimizer

    return memory_optimizer


def optimize_memory():
    from ai_trading.config import get_settings

    S = get_settings()
    if not S.enable_memory_optimization:
        return {}
    from ai_trading.utils import memory_optimizer

    return memory_optimizer.report_memory_use()


class PortInUseError(RuntimeError):
    """Raised when the API server cannot bind to the requested port."""

    def __init__(self, port: int, pid: int | None = None):
        self.port = port
        self.pid = pid
        if pid:
            message = f"Port {port} is already in use by pid {pid}"
        else:
            message = f"Port {port} is already in use"
        super().__init__(message)


class ExistingApiDetected(PortInUseError):
    """Raised when another healthy ai-trading API instance owns the port."""

    def __init__(self, port: int):
        super().__init__(port, pid=None)


def _get_wait_window(settings: Any) -> float:
    """Resolve the API port wait window from multiple settings aliases."""

    for attr in ("wait_window", "api_port_wait_window", "api_port_retry_window", "startup_wait_window"):
        value = getattr(settings, attr, None)
        if value is None:
            continue
        try:
            return max(0.0, float(value))
        except (TypeError, ValueError):
            continue
    return 0.0


def _bind_probe(port: int) -> None:
    """Attempt to bind *port* to confirm availability."""

    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as probe:
        probe.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        probe.bind(("0.0.0.0", port))

config: Any | None = None
register_signal_handlers()


def _get_int_env(var: str, default: int | None = None) -> int | None:
    """Parse integer from environment. Return default on missing/invalid."""
    val = get_env(var)
    if val is None or val == "":
        return default
    try:
        return int(val)
    except (ValueError, TypeError):
        logger.warning("Invalid integer for %s=%r; using default %r", var, val, default)
        return default


def _as_float(val, default: float = 0.0) -> float:
    """Best-effort float conversion that tolerates None/str."""
    try:
        return float(val)
    except (TypeError, ValueError):
        return float(default)


def _install_signal_handlers() -> None:
    """Install signal handlers that log and request cooperative shutdown."""

    def _handler(signum, frame):  # pragma: no cover - exercised via unit tests
        try:
            signame = signal.Signals(signum).name
        except Exception:
            signame = str(signum)
        logger.info("SERVICE_SIGNAL", extra={"signal": signame})
        request_stop(f"signal:{signame}")

    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            signal.signal(sig, _handler)
        except (ValueError, OSError) as exc:
            logger.warning(
                "SIGNAL_HANDLER_INSTALL_FAILED",
                extra={"signal": sig, "error": str(exc)},
            )


def _fail_fast_env() -> None:
    """Reload and validate mandatory environment variables early."""

    test_mode = _is_test_mode()
    risk_default: str | None = None
    backfilled_alpaca: list[str] = []
    if test_mode:
        defaults = {
            "ALPACA_API_KEY": "test-key",
            "ALPACA_SECRET_KEY": "test-secret",
            "ALPACA_DATA_FEED": "iex",
            "WEBHOOK_SECRET": "test-webhook",
            "CAPITAL_CAP": "0.25",
            "DOLLAR_RISK_LIMIT": "0.05",
            "ALPACA_API_URL": "https://paper-api.alpaca.markets",
            "ALPACA_BASE_URL": "https://paper-api.alpaca.markets",
        }
        risk_default = defaults.pop("DOLLAR_RISK_LIMIT")
        for key, value in defaults.items():
            if not os.getenv(key):
                os.environ[key] = value
                if key in {"ALPACA_API_KEY", "ALPACA_SECRET_KEY"}:
                    backfilled_alpaca.append(key)

    alias_backfilled = False
    alias_risk_limit = os.getenv("DAILY_LOSS_LIMIT")
    canonical_risk_limit = os.getenv("DOLLAR_RISK_LIMIT")
    if (canonical_risk_limit is None or canonical_risk_limit.strip() == "") and (
        alias_risk_limit is not None and alias_risk_limit.strip() != ""
    ):
        os.environ["DOLLAR_RISK_LIMIT"] = alias_risk_limit
        alias_backfilled = True
    elif test_mode and (canonical_risk_limit is None or canonical_risk_limit.strip() == ""):
        if risk_default is not None:
            os.environ["DOLLAR_RISK_LIMIT"] = risk_default

    required = [
        "ALPACA_API_KEY",
        "ALPACA_SECRET_KEY",
        "ALPACA_DATA_FEED",
        "WEBHOOK_SECRET",
        "CAPITAL_CAP",
        "DOLLAR_RISK_LIMIT",
    ]
    loaded = reload_env(override=False)
    allow_missing_drawdown = test_mode or _is_truthy_env("RUN_HEALTHCHECK")
    if allow_missing_drawdown and "DOLLAR_RISK_LIMIT" in required:
        required.remove("DOLLAR_RISK_LIMIT")
    required_tuple = tuple(required)
    try:
        trading_cfg = TradingConfig.from_env(
            allow_missing_drawdown=allow_missing_drawdown
        )
    except (RuntimeError, ValueError) as e:
        logger.critical("ENV_VALIDATION_FAILED", extra={"error": str(e)})
        raise SystemExit(1) from e

    credential_warning_logged = False
    try:
        validate_required_env(required_tuple)
    except RuntimeError as exc:
        message = str(exc)
        _, _, tail = message.partition(":")
        missing = tuple(sorted(part.strip() for part in tail.split(",") if part.strip()))
        if not missing:
            logger.critical("ENV_VALIDATION_FAILED", extra={"error": message})
            raise SystemExit(1) from exc
        alpaca_fields = {"ALPACA_API_KEY", "ALPACA_SECRET_KEY"}
        non_alpaca_missing = tuple(name for name in missing if name not in alpaca_fields)
        if non_alpaca_missing:
            logger.critical("ENV_VALIDATION_FAILED", extra={"error": message})
            raise SystemExit(1) from exc
        missing_alpaca = tuple(name for name in missing if name in alpaca_fields)
        if missing_alpaca:
            logger.warning(
                "ALPACA_CREDENTIALS_MISSING",
                extra={"missing": missing_alpaca},
            )
            backfilled_alpaca.clear()
            credential_warning_logged = True

    if backfilled_alpaca and not credential_warning_logged:
        logger.warning(
            "ALPACA_CREDENTIALS_MISSING",
            extra={"missing": tuple(sorted(backfilled_alpaca))},
        )

    snapshot = {k: get_env(k, "") or "" for k in required_tuple}
    _, _, base_url = _resolve_alpaca_env()
    if not base_url:
        error = "Missing required environment variable: ALPACA_API_URL or ALPACA_BASE_URL"
        logger.critical("ENV_VALIDATION_FAILED", extra={"error": error})
        raise SystemExit(1)
    snapshot["ALPACA_API_URL"] = base_url
    logger.info(
        "ENV_CONFIG_LOADED",
        extra={"dotenv_path": loaded, **redact_config_env(snapshot)},
    )

    if os.getenv("RUN_HEALTHCHECK", "").strip() == "1":
        from ai_trading.settings import get_settings

        settings = get_settings()
        health_port = int(getattr(settings, "healthcheck_port", 0) or 0)
        api_port = int(getattr(settings, "api_port", 0) or 0)
        if health_port == api_port:
            message = (
                "RUN_HEALTHCHECK=1 requires HEALTHCHECK_PORT to differ from API_PORT; "
                f"both are set to {api_port}."
            )
            logger.critical(
                "HEALTHCHECK_PORT_CONFLICT",
                extra={"api_port": api_port, "healthcheck_port": health_port},
            )
            raise SystemExit(message)

    raw_risk_limit = get_env("DOLLAR_RISK_LIMIT")
    cfg_risk_limit = getattr(trading_cfg, "dollar_risk_limit", None)
    alias_raw = get_env("DAILY_LOSS_LIMIT")
    canonical_env_value = os.getenv("DOLLAR_RISK_LIMIT")
    if alias_backfilled:
        logger.warning(
            "DOLLAR_RISK_LIMIT_ALIAS_OVERRIDE",
            extra={
                "env_value": alias_raw,
                "canonical_env_value": canonical_env_value,
                "trading_config_value": cfg_risk_limit,
            },
        )
    elif raw_risk_limit not in (None, "") and cfg_risk_limit is not None:
        mismatch = False
        try:
            raw_risk_as_float = float(raw_risk_limit)  # type: ignore[arg-type]
        except (TypeError, ValueError):
            raw_risk_as_float = None
        try:
            cfg_risk_as_float = float(cfg_risk_limit)
        except (TypeError, ValueError):
            cfg_risk_as_float = None

        if raw_risk_as_float is not None and cfg_risk_as_float is not None:
            mismatch = raw_risk_as_float != cfg_risk_as_float
        else:
            mismatch = str(cfg_risk_limit) != str(raw_risk_limit)

        if mismatch:
            logger.warning(
                "DOLLAR_RISK_LIMIT_ALIAS_OVERRIDE",
                extra={
                    "env_value": raw_risk_limit,
                    "trading_config_value": cfg_risk_limit,
                },
            )


def _validate_runtime_config(cfg, tcfg) -> None:
    """Fail-fast runtime config checks."""
    errors = []
    mode = getattr(cfg, "trading_mode", "balanced")
    if mode not in {"aggressive", "balanced", "conservative"}:
        errors.append(f"TRADING_MODE invalid: {mode}")
    cap = _as_float(getattr(tcfg, "capital_cap", 0.0), 0.0)
    risk = _as_float(getattr(tcfg, "dollar_risk_limit", 0.0), 0.0)
    if not 0.0 < cap <= 1.0:
        errors.append(f"CAPITAL_CAP out of range: {cap}")
    if not 0.0 < risk <= 1.0:
        errors.append(f"DOLLAR_RISK_LIMIT out of range: {risk}")
    prev_eq = _CACHE.equity
    raw_eq = _get_equity_from_alpaca(cfg)
    eq: float | None
    if raw_eq is None:
        eq = None
    else:
        try:
            eq = float(raw_eq)
        except (TypeError, ValueError):
            logger.warning("ACCOUNT_EQUITY_INVALID", extra={"equity": raw_eq})
            eq = None
    targets = (cfg,) if cfg is tcfg else (cfg, tcfg)
    resolved_eq = eq if eq is not None and eq > 0 else None
    if resolved_eq is not None:
        for obj in targets:
            try:
                setattr(obj, "equity", resolved_eq)
            except Exception:
                try:
                    object.__setattr__(obj, "equity", resolved_eq)
                except Exception:  # pragma: no cover - defensive
                    pass
    else:
        logger.warning(
            "ACCOUNT_EQUITY_MISSING",
            extra={"equity": eq if eq is not None else raw_eq},
        )
        for obj in targets:
            try:
                setattr(obj, "equity", None)
            except Exception:
                try:
                    object.__setattr__(obj, "equity", None)
                except Exception:  # pragma: no cover - defensive
                    pass
    try:
        force = (_CACHE.value is None) or (eq != prev_eq)
        # Use full Settings for equity/credentials; mode is read from Settings
        resolved, _meta = resolve_max_position_size(cfg, tcfg, force_refresh=force)
        if hasattr(tcfg, "max_position_size"):
            tcfg.max_position_size = float(resolved)
        else:
            os.environ["AI_TRADING_MAX_POSITION_SIZE"] = str(float(resolved))
    except ValueError as e:
        errors.append(str(e))
    base_url = str(getattr(cfg, "alpaca_base_url", ""))
    paper = bool(getattr(cfg, "paper", True))
    if paper and "paper" not in base_url:
        errors.append(f"ALPACA_BASE_URL should be a paper endpoint when PAPER=True: {base_url}")
    if not paper and "paper" in base_url:
        errors.append(f"ALPACA_BASE_URL should be a live endpoint when PAPER=False: {base_url}")
    if errors:
        raise ValueError("; ".join(errors))


def _interruptible_sleep(total_seconds: float) -> None:
    """Sleep in slices while honoring shutdown requests."""

    remaining = float(total_seconds)
    step = 0.25
    while remaining > 0 and (not should_stop()):
        slice_seconds = min(step, remaining)
        time.sleep(max(0.0, slice_seconds))
        remaining -= step


def validate_environment() -> None:
    """Ensure required environment variables are present and dependencies are available."""
    from ai_trading.config.management import get_env, _resolve_alpaca_env

    global config

    missing: list[str] = []
    key, secret, base_url = _resolve_alpaca_env()
    if not key:
        missing.append("ALPACA_API_KEY")
    if not secret:
        missing.append("ALPACA_SECRET_KEY")
    if not base_url:
        missing.append("ALPACA_API_URL")

    webhook_secret = getattr(config, "WEBHOOK_SECRET", "") or get_env("WEBHOOK_SECRET", "")
    if not webhook_secret:
        missing.append("WEBHOOK_SECRET")

    for var in ("CAPITAL_CAP", "DOLLAR_RISK_LIMIT"):
        if not get_env(var):
            missing.append(var)

    if missing:
        raise RuntimeError(
            "Missing required environment variables: " + ", ".join(missing)
        )

    _ = get_settings()
    from ai_trading.paths import CACHE_DIR, DATA_DIR, LOG_DIR, MODELS_DIR, OUTPUT_DIR, ensure_runtime_paths

    ensure_runtime_paths()
    logger.info(
        "RUNTIME_PATHS_READY",
        extra={
            "data": str(DATA_DIR),
            "log": str(LOG_DIR),
            "cache": str(CACHE_DIR),
            "models": str(MODELS_DIR),
            "output": str(OUTPUT_DIR),
        },
    )


_TRADE_LOG_INITIALIZED = False


def ensure_trade_log_path() -> None:
    """Initialize trade log and verify the path is writable."""
    from ai_trading.core.bot_engine import get_trade_logger

    global _TRADE_LOG_INITIALIZED

    if _TRADE_LOG_INITIALIZED:
        return

    tl = get_trade_logger()
    path = Path(tl.path)
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "a"):
            pass
    except OSError as exc:  # pragma: no cover - fail fast on unwritable path
        logger.critical(
            "TRADE_LOG_PATH_UNWRITABLE",
            extra={"path": str(path), "error": str(exc)},
        )
        raise SystemExit(1) from exc
    else:
        logger.info(
            "TRADE_LOG_PATH_READY",
            extra={"path": str(path)},
        )
        _TRADE_LOG_INITIALIZED = True


def run_bot(*_a, **_k) -> int:
    """
    Main entry point for the trading bot.

    Sets up logging, validates configuration, and starts the bot.
    """
    ensure_dotenv_loaded()
    global config
    from ai_trading.logging import validate_logging_setup

    validation_result = validate_logging_setup()
    if not validation_result["validation_passed"]:
        logger.error("Logging validation failed: %s", validation_result["issues"])
    logger.info("Application startup - logging configured once")
    try:
        try:
            reload_env()
        except Exception as exc:  # noqa: BLE001
            logger.critical("ENV_LOAD_FAILED", extra={"error": str(exc)})
            return 1
        config = get_settings()
        if config is None:
            logger.critical("SETTINGS_UNAVAILABLE")
            return 1
        validate_environment()
        memory_optimizer = get_memory_optimizer()
        if memory_optimizer:
            memory_optimizer.enable_low_memory_mode()
            logger.info("Memory optimization enabled")
        logger.info("Bot startup complete - entering main loop")
        preflight_ok = preflight_import_health()
        if not preflight_ok:
            if should_enforce_strict_import_preflight():
                logger.critical("IMPORT_PREFLIGHT_ABORT", extra={"strict": True})
                raise SystemExit(1)
            logger.warning("IMPORT_PREFLIGHT_SOFT_FAIL", extra={"strict": False})
        if not _TRADE_LOG_INITIALIZED:
            ensure_trade_log_path()
        run_cycle()
        _reset_warmup_cooldown_timestamp()
        return 0
    except (ValueError, TypeError, RuntimeError) as e:
        logger.error("Bot startup failed: %s", e, exc_info=True)
        return 1


def run_flask_app(
    port: int = 5000,
    ready_signal: threading.Event | None = None,
    **run_kwargs,
) -> None:
    """Launch Flask API, retrying on sequential ports when necessary."""

    from ai_trading import app

    application = app.create_app()
    debug = run_kwargs.pop("debug", False)

    def _port_available(candidate: int) -> bool:
        families = [(socket.AF_INET, ("0.0.0.0", candidate))]
        if socket.has_ipv6:
            families.append((socket.AF_INET6, ("::", candidate)))

        for family, addr in families:
            try:
                with socket.socket(family, socket.SOCK_STREAM) as sock:
                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    if family == socket.AF_INET6 and hasattr(socket, "IPPROTO_IPV6"):
                        try:
                            sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)
                        except (OSError, AttributeError):
                            pass
                    sock.bind(addr)
            except OSError as bind_exc:
                if bind_exc.errno == errno.EADDRINUSE:
                    return False
        return True

    max_attempts = run_kwargs.pop("max_attempts", 5)
    attempt_port = port
    attempts = 0

    while attempts < max_attempts:
        attempts += 1
        pid = get_pid_on_port(attempt_port)
        available = _port_available(attempt_port)

        if not available:
            logger.warning(
                "API_PORT_UNAVAILABLE", extra={"port": attempt_port, "reason": "socket_bound"}
            )
            attempt_port += 1
            continue

        if pid:
            logger.error("API_PORT_OCCUPIED", extra={"port": attempt_port, "pid": pid})
            raise PortInUseError(attempt_port, pid)

        try:
            if ready_signal is not None:
                logger.info(
                    "Flask app created successfully, signaling ready on port %s",
                    attempt_port,
                )
                ready_signal.set()
            logger.info("Starting Flask app on 0.0.0.0:%s", attempt_port)
            application.run(host="0.0.0.0", port=attempt_port, debug=debug, **run_kwargs)
            return
        except OSError as exc:
            if exc.errno == errno.EADDRINUSE:
                logger.warning(
                    "API_PORT_BOUND_DURING_START", extra={"port": attempt_port}
                )
                attempt_port += 1
                continue
            raise

    raise PortInUseError(attempt_port)


def _probe_local_api_health(port: int) -> bool:
    """Return ``True`` when the ai-trading API responds on ``port``."""

    try:
        import http.client as _http
        import json as _json
    except Exception:  # pragma: no cover - stdlib import failures are unexpected
        return False

    conn = None
    resp = None
    try:
        conn = _http.HTTPConnection("127.0.0.1", port, timeout=1.5)
        conn.request("GET", "/healthz")
        resp = conn.getresponse()
        payload = resp.read()  # must consume response before closing
    except Exception:
        return False
    finally:
        try:
            conn.close()
        except Exception:
            pass

    if resp is None or resp.status != 200:
        return False
    try:
        data = _json.loads(payload.decode("utf-8"))
    except Exception:
        return False
    return bool(data) and data.get("service") == "ai-trading"


def start_api(ready_signal: threading.Event | None = None) -> None:
    """Spin up the Flask API server or raise if the port is unavailable."""

    ensure_dotenv_loaded()
    settings = get_settings()
    port = int(getattr(settings, "api_port", 9001) or 9001)
    wait_window = _get_wait_window(settings)
    start_time = time.monotonic()

    # ── Aux health server on HEALTHCHECK_PORT (non-blocking, separate Flask app)
    try:
        from ai_trading.health import HealthCheck
        health_port = int(getattr(settings, "healthcheck_port", 9101) or 9101)
        if int(port) != int(health_port):
            ctx = SimpleNamespace(host="0.0.0.0", port=health_port, service="ai-trading")
            hc = HealthCheck(ctx=ctx)
            th = threading.Thread(target=hc.run, name="health-server", daemon=True)
            th.start()
            logger.info("HEALTH_SERVER_STARTED", extra={"port": health_port})
        else:
            logger.info("HEALTH_SERVER_PORT_SHARED", extra={"port": port})
    except Exception as _exc:  # pragma: no cover - defensive
        logger.warning("HEALTH_SERVER_START_FAILED", extra={"error": str(_exc)})

    while True:
        if should_stop():
            logger.info("API_STARTUP_ABORTED", extra={"reason": "shutdown"})
            if ready_signal is not None:
                ready_signal.set()
            return

        try:
            _bind_probe(port)
        except OSError as exc:
            if exc.errno != errno.EADDRINUSE:
                raise

            elapsed = time.monotonic() - start_time
            pid = get_pid_on_port(port)
            conflict_extra = {"port": port}
            if pid:
                conflict_extra["pid"] = pid
                logger.warning("HEALTH_SERVER_START_FAILED", extra=conflict_extra)
                logger.warning(
                    "API_STARTUP_ABORTED",
                    extra={"port": port, "reason": "port_in_use", "pid": pid},
                )
                raise PortInUseError(port, pid) from exc

            if _probe_local_api_health(port):
                logger.warning("HEALTH_SERVER_START_FAILED", extra=conflict_extra)
                logger.warning(
                    "API_STARTUP_ABORTED",
                    extra={"port": port, "reason": "existing_api"},
                )
                raise ExistingApiDetected(port) from exc

            if elapsed >= wait_window:
                logger.warning("HEALTH_SERVER_START_FAILED", extra=conflict_extra)
                logger.warning(
                    "API_STARTUP_ABORTED",
                    extra={"port": port, "reason": "port_timeout"},
                )
                raise PortInUseError(port) from exc

            time.sleep(0.1)
            continue

        break

    run_flask_app(port, ready_signal)


def _assert_singleton_api(settings) -> None:
    """Ensure we are the only ai-trading API instance before trading warm-up."""

    env_label = str(getattr(settings, "env", "")).strip().lower()
    if os.getenv("PYTEST_RUNNING", "").strip().lower() in {"1", "true", "yes"} or env_label == "test":
        return
    port = int(getattr(settings, "api_port", 9001) or 9001)
    pid = get_pid_on_port(port)
    if pid:
        logger.critical(
            "API_PORT_OCCUPIED_BEFORE_START",
            extra={"port": port, "pid": pid},
        )
        raise PortInUseError(port, pid)
    if _probe_local_api_health(port):
        logger.critical(
            "API_PORT_HEALTHY_ELSEWHERE",
            extra={"port": port},
        )
        raise ExistingApiDetected(port)


def start_api_with_signal(api_ready: threading.Event, api_error: threading.Event) -> None:
    """Start API server and signal readiness/errors."""
    if should_stop():
        api_ready.set()
        return
    try:
        start_api(api_ready)
    except Exception as exc:  # noqa: BLE001
        logger.error("Failed to start API", exc_info=True)
        try:
            setattr(api_error, "exception", exc)
        except Exception:
            pass
        api_error.set()


def _init_http_session(cfg, retries: int = 3, delay: float = 1.0) -> bool:
    """Initialize the global HTTP client session with retry logic."""
    for attempt in range(1, retries + 1):
        if should_stop():
            logger.info("HTTP_SESSION_INIT_ABORT", extra={"attempt": attempt})
            return False
        try:
            connect_timeout = clamp_request_timeout(
                float(getattr(cfg, "http_connect_timeout", 5.0))
            )
            read_timeout = clamp_request_timeout(
                float(getattr(cfg, "http_read_timeout", 10.0))
            )
            session = build_retrying_session(
                pool_maxsize=int(getattr(cfg, "http_pool_maxsize", 32)),
                total_retries=int(getattr(cfg, "http_total_retries", 3)),
                backoff_factor=float(getattr(cfg, "http_backoff_factor", 0.3)),
                connect_timeout=connect_timeout,
                read_timeout=read_timeout,
            )
            # Apply host-specific retry profile for Alpaca if configured
            try:
                from urllib.parse import urlparse as _urlparse

                host = _urlparse(str(getattr(cfg, "alpaca_base_url", ""))).netloc
                if host:
                    # ENV override pattern: HTTP_RETRIES_<host> (dots → underscores), HTTP_BACKOFF_<host>
                    key = host.replace(".", "_")
                    import os as _os

                    _retries = int(_os.getenv(f"HTTP_RETRIES_{key}", "2"))
                    _bof = float(_os.getenv(f"HTTP_BACKOFF_{key}", "0.2"))
                    mount_host_retry_profile(session, host, total_retries=_retries, backoff_factor=_bof, pool_maxsize=int(getattr(cfg, "http_pool_maxsize", 32)))
            except Exception:
                pass
            set_global_session(session)
            logger.info(
                "REQUESTS_POOL_STATS",
                extra={
                    "transport": "requests",
                    "pool_maxsize": getattr(cfg, "http_pool_maxsize", 32),
                    "retries": getattr(cfg, "http_total_retries", 3),
                    "backoff_factor": getattr(cfg, "http_backoff_factor", 0.3),
                    "connect_timeout": connect_timeout,
                    "read_timeout": read_timeout,
                },
            )
            return True
        except Exception as exc:  # noqa: BLE001
            logger.error(
                "HTTP_SESSION_INIT_FAILED",
                extra={"attempt": attempt, "error": str(exc)},
            )
            if attempt < retries:
                _interruptible_sleep(delay)
    logger.critical("HTTP session initialization failed after %s attempts", retries)
    return False


def parse_cli(argv: list[str] | None = None):
    """Parse CLI arguments, tolerating unknown flags."""
    parser = argparse.ArgumentParser(description="AI Trading Bot")
    parser.add_argument("--iterations")
    parser.add_argument("--interval")
    args, _unknown = parser.parse_known_args(argv)
    return args


def main(argv: list[str] | None = None) -> None:
    """Start the API thread and repeatedly run trading cycles."""
    ensure_dotenv_loaded()
    _check_alpaca_sdk()
    _fail_fast_env()
    args = parse_cli(argv)
    global config
    config = S = get_settings()
    # Initialize TradingConfig-backed overrides (import-safe)
    try:
        from ai_trading.core import bot_engine as _be

        _be.initialize_runtime_config()
    except Exception:  # pragma: no cover - defensive
        logger.debug("RUNTIME_CONFIG_INIT_SKIPPED", exc_info=True)
    try:
        _assert_singleton_api(S)
    except PortInUseError as exc:
        logger.critical(
            "API_PORT_CONFLICT_FATAL",
            extra={"port": exc.port, "pid": exc.pid},
        )
        raise SystemExit(errno.EADDRINUSE) from exc
    allow_after_hours = bool(get_env("ALLOW_AFTER_HOURS", "0", cast=bool))
    try:
        if not _is_market_open_base():
            now = datetime.now(ZoneInfo("America/New_York"))
            if allow_after_hours:
                logger.warning(
                    "STARTUP_OUTSIDE_MARKET_HOURS",
                    extra={"now": now.isoformat(), "override": True},
                )
            else:
                try:
                    nxt = next_market_open(now)
                except Exception:
                    logger.error("NEXT_MARKET_OPEN_FAILED", exc_info=True)
                    raise SystemExit(1)
                wait = max((nxt - now).total_seconds(), 0.0)
                cap_raw = getattr(S, "interval_when_closed", 300)
                try:
                    cap = float(cap_raw)
                except (TypeError, ValueError):
                    cap = 300.0
                if cap != cap:  # NaN guard without importing math
                    cap = 300.0
                cap = max(cap, 0.0)
                sleep_for = min(wait, cap) if cap > 0 else 0.0
                logger.warning(
                    "MARKET_CLOSED_SLEEP",
                    extra={
                        "now": now.isoformat(),
                        "next_open": nxt.isoformat(),
                        "sleep_s": int(sleep_for),
                        "sleep_original_s": int(wait),
                        "sleep_cap_s": int(cap),
                    },
                )
                if sleep_for > 0:
                    _interruptible_sleep(sleep_for)
    except Exception:
        logger.debug("MARKET_OPEN_CHECK_FAILED", exc_info=True)
    # Align Settings.capital_cap with plain env when provided to avoid prefix alias gaps
    if not _is_test_mode():
        _cap_env = os.getenv("CAPITAL_CAP")
        if _cap_env:
            try:
                _cap_val = float(_cap_env)
                try:
                    setattr(S, "capital_cap", _cap_val)
                except Exception:
                    try:
                        object.__setattr__(S, "capital_cap", _cap_val)
                    except Exception:
                        pass
                try:
                    setattr(config, "capital_cap", _cap_val)
                except Exception:
                    try:
                        object.__setattr__(config, "capital_cap", _cap_val)
                    except Exception:
                        pass
            except Exception:
                pass
    if config is None:
        logger.critical(
            "SETTINGS_UNAVAILABLE",  # AI-AGENT-REF: clearer startup failure
            extra={
                "hint": "ensure configuration is accessible or run ai_trading.config.management.reload_env",
            },
        )
        raise SystemExit(1)
    logger.info(
        "DATA_CONFIG feed=%s adjustment=%s timeframe=1Day/1Min provider=alpaca", S.alpaca_data_feed, S.alpaca_adjustment
    )
    # Metrics for cycle timing and budget overruns (labels are no-op when metrics unavailable)
    # Labeled stage timings: fetch/compute/execute
    _cycle_stage_seconds = get_histogram("cycle_stage_seconds", "Cycle stage duration seconds", ["stage"])  # type: ignore[arg-type]
    _cycle_budget_over_total = get_counter("cycle_budget_over_total", "Budget-over events", ["stage"])  # type: ignore[arg-type]

    try:
        _validate_runtime_config(config, S)
    except ValueError as e:
        logger.critical("RUNTIME_CONFIG_INVALID", extra={"error": str(e)})
        raise
    if should_stop():
        logger.info("SERVICE_SHUTDOWN", extra={"reason": "preflight-stop"})
        return
    if not _init_http_session(config):
        return
    banner = {
        "mode": getattr(config, "trading_mode", "balanced"),
        "paper": getattr(config, "paper", True),
        "alpaca_base_url": getattr(config, "alpaca_base_url", ""),
        "capital_cap": _as_float(getattr(S, "capital_cap", 0.0), 0.0),
        "dollar_risk_limit": _as_float(getattr(S, "dollar_risk_limit", 0.0), 0.0),
        "max_position_mode": str(
            getattr(S, "max_position_mode", getattr(config, "max_position_mode", "STATIC"))
        ).upper(),
        "max_position_size": _as_float(getattr(S, "max_position_size", None), 0.0),
    }
    logger.info("STARTUP_BANNER", extra=_redact(banner))
    _install_signal_handlers()
    ensure_trade_log_path()
    try:
        run_cycle()
    except (TypeError, ValueError) as e:
        logger.critical(
            "Warm-up run_cycle failed during trading initialization; shutting down",
            exc_info=e,
        )
        raise SystemExit(1) from e
    except SystemExit as e:
        logger.error(
            "Warm-up run_cycle triggered SystemExit; shutting down",
            exc_info=e,
        )
        raise
    except Exception as e:  # noqa: BLE001
        logger.exception(
            "Warm-up run_cycle failed unexpectedly; shutting down",
            exc_info=e,
        )
        raise SystemExit(1) from e
    logger.info("Warm-up run_cycle completed")
    _reset_warmup_cooldown_timestamp()
    api_ready = threading.Event()
    api_error = threading.Event()
    t = Thread(target=start_api_with_signal, args=(api_ready, api_error), daemon=True)
    t.start()
    # Wait for the API to bind; treat configured port conflicts as fatal.
    try:
        if api_error.wait(timeout=2):
            raise getattr(api_error, "exception", RuntimeError("API failed to start"))
        if not api_ready.wait(timeout=10):
            if not t.is_alive():
                raise RuntimeError("API thread terminated unexpectedly during startup")
            logger.warning("API startup taking longer than expected, proceeding with degraded functionality")
    except PortInUseError as exc:
        logger.critical(
            "API_PORT_CONFLICT_FATAL",
            extra={"port": exc.port, "pid": exc.pid},
        )
        raise SystemExit(errno.EADDRINUSE) from exc
    except (RuntimeError, TimeoutError, OSError) as e:
        # Degrade gracefully for other failures so trading can continue.
        logger.error("API_STARTUP_DEGRADED", exc_info=e)
        api_error.set()
    S = get_settings()
    from ai_trading.utils.device import get_device  # AI-AGENT-REF: guard torch import

    get_device()
    raw_tick = get_env("HEALTH_TICK_SECONDS") or getattr(S, "health_tick_seconds", 300)
    recommended_health_tick = 30
    try:
        health_tick_seconds = int(raw_tick)
    except (ValueError, TypeError):
        health_tick_seconds = int(getattr(S, "health_tick_seconds", 300))
    if health_tick_seconds < recommended_health_tick:
        logger.warning(
            "HEALTH_TICK_INTERVAL_MINIMUM_ENFORCED",
            extra={
                "configured": health_tick_seconds,
                "normalized": recommended_health_tick,
            },
        )
        health_tick_seconds = recommended_health_tick
    health_tick_runtime = health_tick_seconds
    last_health = monotonic_time()
    env_iter = _get_int_env("SCHEDULER_ITERATIONS")
    raw_iter = (
        args.iterations
        if args.iterations is not None
        else (
            env_iter
            if env_iter is not None
            else getattr(S, "iterations", None) or getattr(S, "scheduler_iterations", 0)
        )
    )
    try:
        iterations = int(raw_iter)
    except ValueError:
        iterations = 0
    raw_interval = args.interval if args.interval is not None else getattr(S, "interval", 60)
    try:
        interval = int(raw_interval)
    except ValueError:
        interval = 60
    try:
        closed_interval = int(getattr(S, "interval_when_closed", 300))
    except Exception:
        closed_interval = 300
    seed = get_seed_int()
    logger.info("Runtime defaults resolved", extra={"iterations": iterations, "interval": interval, "seed": seed})
    count = 0
    # Track HTTP profile to reduce retries when market is closed
    _http_closed_profile = None  # None=unknown, True=closed, False=open
    memory_check_interval = 10
    try:
        while iterations <= 0 or count < iterations:
            try:
                closed = not _is_market_open_base()
            except Exception:
                closed = False
            # Toggle HTTP session profile only when state changes
            if _http_closed_profile is None or closed != _http_closed_profile:
                try:
                    if closed:
                        connect_timeout = clamp_request_timeout(float(getattr(S, "http_connect_timeout_closed", getattr(S, "http_connect_timeout", 5.0)) or getattr(S, "http_connect_timeout", 5.0)))
                        read_timeout = clamp_request_timeout(float(getattr(S, "http_read_timeout_closed", getattr(S, "http_read_timeout", 10.0)) or getattr(S, "http_read_timeout", 10.0)))
                        # Optional hint for executors to downsize when closed
                        try:
                            _ewc = os.getenv("EXEC_WORKERS_WHEN_CLOSED")
                            if _ewc:
                                os.environ["AI_TRADING_EXEC_WORKERS"] = _ewc
                                os.environ["AI_TRADING_PRED_WORKERS"] = _ewc
                        except Exception:
                            pass
                        session = build_retrying_session(
                            pool_maxsize=int(getattr(S, "http_pool_maxsize", 32)),
                            total_retries=1,
                            backoff_factor=0.1,
                            connect_timeout=connect_timeout,
                            read_timeout=read_timeout,
                        )
                        try:
                            from urllib.parse import urlparse as _urlparse
                            host = _urlparse(str(getattr(S, "alpaca_base_url", ""))).netloc
                            if host:
                                mount_host_retry_profile(session, host, total_retries=1, backoff_factor=0.1, pool_maxsize=int(getattr(S, "http_pool_maxsize", 32)))
                        except Exception:
                            pass
                        set_global_session(session)
                        logger.info(
                            "HTTP_PROFILE_CLOSED",
                            extra={"retries": 1, "backoff_factor": 0.1, "connect_timeout": connect_timeout, "read_timeout": read_timeout},
                        )
                    else:
                        # Restore configured profile
                        connect_timeout = clamp_request_timeout(float(getattr(S, "http_connect_timeout", 5.0)))
                        read_timeout = clamp_request_timeout(float(getattr(S, "http_read_timeout", 10.0)))
                        try:
                            if "EXEC_WORKERS_WHEN_CLOSED" in os.environ:
                                # Unset closed hint; do not remove explicit AI_TRADING_* overrides if user set them
                                os.environ.pop("AI_TRADING_EXEC_WORKERS", None)
                                os.environ.pop("AI_TRADING_PRED_WORKERS", None)
                        except Exception:
                            pass
                        session = build_retrying_session(
                            pool_maxsize=int(getattr(S, "http_pool_maxsize", 32)),
                            total_retries=int(getattr(S, "http_total_retries", 3)),
                            backoff_factor=float(getattr(S, "http_backoff_factor", 0.3)),
                            connect_timeout=connect_timeout,
                            read_timeout=read_timeout,
                        )
                        try:
                            from urllib.parse import urlparse as _urlparse
                            host = _urlparse(str(getattr(S, "alpaca_base_url", ""))).netloc
                            if host:
                                _retries = int(os.getenv(host.replace('.', '_').join(["HTTP_RETRIES_", ""])) or getattr(S, "http_total_retries", 3))
                                _bof = float(os.getenv(host.replace('.', '_').join(["HTTP_BACKOFF_", ""])) or getattr(S, "http_backoff_factor", 0.3))
                                mount_host_retry_profile(session, host, total_retries=int(_retries), backoff_factor=float(_bof), pool_maxsize=int(getattr(S, "http_pool_maxsize", 32)))
                        except Exception:
                            pass
                        set_global_session(session)
                        logger.info(
                            "HTTP_PROFILE_OPEN",
                            extra={
                                "retries": getattr(S, "http_total_retries", 3),
                                "backoff_factor": getattr(S, "http_backoff_factor", 0.3),
                                "connect_timeout": connect_timeout,
                                "read_timeout": read_timeout,
                            },
                        )
                    _http_closed_profile = closed
                except Exception:
                    pass
            raw_fraction = get_env(
                "CYCLE_COMPUTE_BUDGET",
                get_env("CYCLE_BUDGET_FRACTION", 0.9),
            )
            try:
                fraction = float(raw_fraction)
            except (TypeError, ValueError):
                fraction = 0.9
            # Dynamic interval: slow down when closed
            effective_interval = int(closed_interval if closed else interval)
            budget = None
            try:
                interval_ms = max(0.0, float(effective_interval)) * 1000.0
                fraction_clamped = max(0.0, min(1.0, float(fraction)))
                budget = SoftBudget(int(interval_ms * fraction_clamped))
                try:
                    _t0 = monotonic_time()
                    with StageTimer(logger, "CYCLE_FETCH"):
                        if count % memory_check_interval == 0:
                            gc_result = optimize_memory()
                            if gc_result.get("objects_collected", 0) > 100:
                                logger.info(
                                    "CYCLE_FETCH_GC",
                                    extra={
                                        "cycle": count,
                                        "objects_collected": gc_result["objects_collected"],
                                    },
                                )
                    try:
                        _cycle_stage_seconds.labels(stage="fetch").observe(max(0.0, monotonic_time() - _t0))  # type: ignore[call-arg]
                    except Exception:
                        pass
                    if budget.over_budget():
                        logger.warning("BUDGET_OVER", extra={"stage": "CYCLE_FETCH"})
                        try:
                            _cycle_budget_over_total.labels(stage="fetch").inc()  # type: ignore[call-arg]
                        except Exception:
                            pass
                    _t1 = monotonic_time()
                    if budget is not None:
                        set_cycle_budget_context(
                            budget,
                            interval_s=float(effective_interval),
                            fraction=fraction_clamped,
                        )
                    try:
                        with StageTimer(logger, "CYCLE_COMPUTE"):
                            run_cycle()
                    finally:
                        if budget is not None:
                            emit_cycle_budget_summary(logger)
                            clear_cycle_budget_context()
                    try:
                        _cycle_stage_seconds.labels(stage="compute").observe(max(0.0, monotonic_time() - _t1))  # type: ignore[call-arg]
                    except Exception:
                        pass
                    if budget.over_budget():
                        logger.warning("BUDGET_OVER", extra={"stage": "CYCLE_COMPUTE"})
                        try:
                            _cycle_budget_over_total.labels(stage="compute").inc()  # type: ignore[call-arg]
                        except Exception:
                            pass
                    _t2 = monotonic_time()
                    with StageTimer(logger, "CYCLE_EXECUTE"):
                        pass
                    try:
                        _cycle_stage_seconds.labels(stage="execute").observe(max(0.0, monotonic_time() - _t2))  # type: ignore[call-arg]
                    except Exception:
                        pass
                    if budget.over_budget():
                        logger.warning("BUDGET_OVER", extra={"stage": "CYCLE_EXECUTE"})
                        try:
                            _cycle_budget_over_total.labels(stage="execute").inc()  # type: ignore[call-arg]
                        except Exception:
                            pass
                except (ValueError, TypeError):
                    logger.exception("run_cycle failed")
            except Exception:
                logger.error(
                    "SCHEDULER_RUN_CYCLE_EXCEPTION",
                    extra={"iteration": count},
                    exc_info=True,
                )
                count += 1
                try:
                    backoff_seconds = int(effective_interval)
                except Exception:
                    backoff_seconds = interval
                backoff_seconds = max(1, min(30, backoff_seconds))
                _interruptible_sleep(backoff_seconds)
                _logging.flush_log_throttle_summaries()
                if should_stop():
                    logger.info("SERVICE_SHUTDOWN", extra={"reason": "signal"})
                    break
                continue
            if budget is None:
                continue
            count += 1
            logger.info(
                "CYCLE_TIMING",
                extra={"elapsed_ms": budget.elapsed_ms(), "within_budget": not budget.over_budget()},
            )
            _logging.flush_log_throttle_summaries()
            now_mono = monotonic_time()
            if now_mono - last_health >= health_tick_runtime:
                logger.info("HEALTH_TICK", extra={"iteration": count, "interval": effective_interval, "closed": closed})
                last_health = now_mono
            try:
                # Resolve mode directly from env to honor MAX_POSITION_MODE without relying on Settings
                _mode_env = os.getenv("MAX_POSITION_MODE") or os.getenv("AI_TRADING_MAX_POSITION_MODE")
                mode_now = str(
                    _mode_env
                    if _mode_env is not None
                    else getattr(S, "max_position_mode", getattr(config, "max_position_mode", "STATIC"))
                ).upper()
                if mode_now == "AUTO":
                    resolved_size, meta = resolve_max_position_size(config, S, force_refresh=False)
                    if float(getattr(S, "max_position_size", 0.0)) != resolved_size:
                        try:
                            setattr(S, "max_position_size", float(resolved_size))
                        except (AttributeError, TypeError):
                            pass
                        logger.info("POSITION_SIZING_REFRESHED", extra={**meta, "resolved": resolved_size})
            except (ValueError, KeyError, TypeError) as e:
                logger.warning("RUNTIME_SIZING_UPDATE_FAILED", exc_info=e)
            _interruptible_sleep(int(max(1, effective_interval)))
            if should_stop():
                logger.info("SERVICE_SHUTDOWN", extra={"reason": "signal"})
                break
    except KeyboardInterrupt:
        request_stop("keyboard-interrupt")
        logger.info("KeyboardInterrupt received — shutting down gracefully")
        return
    # If a finite number of iterations was requested, exit promptly so tests
    # and batch runs do not hang. Production runs use infinite iterations.
    if iterations > 0:
        logger.info("SCHEDULER_COMPLETE", extra={"iterations": count})
        return


if __name__ == "__main__":
    try:
        main()
    except SystemExit as exc:
        code = exc.code if isinstance(exc.code, int) else 1
        if code != 0:
            logger.error("SERVICE_EXIT", extra={"code": code})
        raise
    except BaseException:  # noqa: BLE001
        logger.exception("SERVICE_CRASH")
        sys.exit(1)
    else:
        sys.exit(0)
def _log_auth_preflight_failure(detail: str, action: str) -> None:
    """Emit a single critical log for Alpaca auth preflight failures."""

    global _AUTH_PREFLIGHT_LOGGED
    if _AUTH_PREFLIGHT_LOGGED:
        return
    _AUTH_PREFLIGHT_LOGGED = True
    logger.critical(
        "ALPACA_AUTH_PREFLIGHT_FAILED",
        extra={"detail": detail, "action": action},
    )
    _emit_capture_handler_record(detail, action)


def _emit_capture_handler_record(detail: str, action: str) -> None:
    """Forward the auth failure to pytest log capture handlers when present."""

    handler_refs = getattr(logging, "_handlerList", None)
    if not handler_refs:
        return
    try:
        base_logger = logger.logger  # type: ignore[attr-defined]
    except AttributeError:
        base_logger = logger
    for handler_ref in list(handler_refs):
        handler = handler_ref() if callable(handler_ref) else handler_ref
        if handler is None:
            continue
        if handler.__class__.__name__ != "LogCaptureHandler":
            continue
        record = base_logger.makeRecord(
            base_logger.name,
            logging.CRITICAL,
            __file__,
            _log_auth_preflight_failure.__code__.co_firstlineno,
            "ALPACA_AUTH_PREFLIGHT_FAILED",
            args=(),
            exc_info=None,
        )
        record.detail = detail
        record.action = action
        try:
            handler.emit(record)
        except Exception:  # pragma: no cover - defensive guard for custom handlers
            continue
