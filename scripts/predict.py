"""Prediction utilities using trained models."""

from __future__ import annotations

import argparse
import json
import logging
import os

# AI-AGENT-REF: graceful joblib fallback for testing
try:
    import joblib
except ImportError:
    # Create minimal joblib fallback
    class MockJoblib:
        @staticmethod
        def load(filename):
            # Return a minimal mock model
            class MockModel:
                def predict(self, X):
                    return [0] * len(X)
                def predict_proba(self, X):
                    return [[0.5, 0.5]] * len(X)
            return MockModel()

        @staticmethod
        def dump(obj, filename):
            pass  # Mock dump
    joblib = MockJoblib()

# AI-AGENT-REF: graceful pandas fallback for testing
try:
    import pandas as pd
except ImportError:
    # Import mock pandas from ai_trading.utils
    from ai_trading.utils import pd

# AI-AGENT-REF: Use HTTP utilities with proper timeout/retry
import config
from metrics_logger import log_metrics
from retrain import prepare_indicators

from ai_trading.utils import http

logger = logging.getLogger(__name__)

config.reload_env()
# FutureWarning now filtered globally in pytest.ini

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INACTIVE_FEATURES_FILE = os.path.join(BASE_DIR, "inactive_features.json")
MODELS_DIR = os.path.join(BASE_DIR, "models")


import threading
import time

# AI-AGENT-REF: Memory leak prevention with TTLCache
try:
    from cachetools import TTLCache
    # TTLCache with 5 min TTL and 1000 items max to prevent memory leaks
    _sentiment_cache = TTLCache(maxsize=1000, ttl=300)
    _CACHETOOLS_AVAILABLE = True
except ImportError:
    # Fallback to bounded dict cache if cachetools not available
    _sentiment_cache = {}
    _CACHETOOLS_AVAILABLE = False

_sentiment_lock = threading.Lock()
_last_request_time = {}
_min_request_interval = 1.0  # Minimum 1 second between requests per symbol
_cache_ttl = 300  # 5 minutes cache TTL

def fetch_sentiment(symbol: str) -> float:
    """Return a sentiment score for ``symbol`` using NewsAPI with rate limiting and TTL cache."""

    # Support both SENTIMENT_API_KEY and NEWS_API_KEY for backwards compatibility
    api_key = getattr(config, 'SENTIMENT_API_KEY', None) or config.NEWS_API_KEY
    if not api_key:
        logger.debug("No sentiment API key configured (checked SENTIMENT_API_KEY and NEWS_API_KEY)")
        return 0.0

    current_time = time.time()

    with _sentiment_lock:
        # Check cache first (TTLCache handles expiration automatically)
        if _CACHETOOLS_AVAILABLE:
            if symbol in _sentiment_cache:
                cached_score = _sentiment_cache[symbol]
                logger.debug("Using TTL cached sentiment for %s: %.2f", symbol, cached_score)
                return cached_score
        else:
            # Fallback manual cache management
            if symbol in _sentiment_cache:
                cached_score, cached_time = _sentiment_cache[symbol]
                if current_time - cached_time < _cache_ttl:
                    logger.debug("Using manual cached sentiment for %s: %.2f", symbol, cached_score)
                    return cached_score
                else:
                    # Remove expired entry
                    del _sentiment_cache[symbol]

        # Check rate limiting
        if symbol in _last_request_time:
            time_since_last = current_time - _last_request_time[symbol]
            if time_since_last < _min_request_interval:
                logger.warning(
                    "fetch_sentiment(%s) rate-limited â†’ returning cached/neutral 0.0",
                    symbol
                )
                # Return cached value if available, otherwise neutral
                if _CACHETOOLS_AVAILABLE and symbol in _sentiment_cache:
                    return _sentiment_cache[symbol]
                elif not _CACHETOOLS_AVAILABLE and symbol in _sentiment_cache:
                    return _sentiment_cache[symbol][0]
                return 0.0

        _last_request_time[symbol] = current_time

    try:
        # Support configurable sentiment API URL, with fallback to NewsAPI
        base_url = getattr(config, 'SENTIMENT_API_URL', 'https://newsapi.org/v2/everything')
        url = (
            f"{base_url}?q="
            f"{symbol}&pageSize=5&sortBy=publishedAt&apiKey={api_key}"
        )
        resp = http.get(url)
        resp.raise_for_status()
        arts = resp.json().get("articles", [])
        if not arts:
            score = 0.0
        else:
            score = sum(
                1 for a in arts if "positive" in (a.get("title") or "").lower()
            ) / len(arts)

        score = float(score)

        # Cache the result with memory leak prevention
        with _sentiment_lock:
            if _CACHETOOLS_AVAILABLE:
                _sentiment_cache[symbol] = score  # TTLCache handles expiration
            else:
                # Manual cache with size limit to prevent memory leaks
                if len(_sentiment_cache) >= 1000:
                    # Remove oldest entry
                    oldest_key = min(_sentiment_cache.keys(),
                                   key=lambda k: _sentiment_cache[k][1])
                    del _sentiment_cache[oldest_key]
                _sentiment_cache[symbol] = (score, current_time)

        logger.debug("Fetched fresh sentiment for %s: %.2f", symbol, score)
        return score

    except (Exception, ValueError) as exc:
        logger.error("fetch_sentiment failed for %s: %s", symbol, exc)
        # Return cached value if available during error, otherwise neutral
        with _sentiment_lock:
            if _CACHETOOLS_AVAILABLE and symbol in _sentiment_cache:
                return _sentiment_cache[symbol]
            elif not _CACHETOOLS_AVAILABLE and symbol in _sentiment_cache:
                return _sentiment_cache[symbol][0]
        return 0.0


def detect_regime(df: pd.DataFrame) -> str:
    """Classify a market regime based on moving average crossovers."""

    if df is None or df.empty or "close" not in df:
        return "chop"
    close = df["close"].astype(float)
    sma50 = close.rolling(50).mean()
    sma200 = close.rolling(200).mean()
    if sma50.iloc[-1] > sma200.iloc[-1]:
        return "bull"
    if sma50.iloc[-1] < sma200.iloc[-1]:
        return "bear"
    return "chop"


MODEL_FILES = {
    "bull": os.path.join(MODELS_DIR, "model_bull.pkl"),
    "bear": os.path.join(MODELS_DIR, "model_bear.pkl"),
    "chop": os.path.join(MODELS_DIR, "model_chop.pkl"),
}


def load_model(regime: str):
    """Load and return the model for the specified ``regime``."""

    path = MODEL_FILES.get(regime)
    if not path or not os.path.exists(path):
        raise FileNotFoundError(f"Model for regime '{regime}' not found: {path}")
    return joblib.load(path)


def predict(csv_path: str, freq: str = "intraday") -> tuple[int | None, float | None]:
    """Return the predicted class and probability for the data in ``csv_path``."""

    df = pd.read_csv(csv_path)
    symbol = os.path.splitext(os.path.basename(csv_path))[0]
    try:
        import inspect

        if "symbol" in inspect.signature(prepare_indicators).parameters:
            feat = prepare_indicators(df, freq=freq, symbol=symbol)
        else:
            feat = prepare_indicators(df, freq=freq)
    except (TypeError, ValueError, AttributeError):
        feat = prepare_indicators(df, freq=freq)
    if os.path.exists(INACTIVE_FEATURES_FILE):
        try:
            with open(INACTIVE_FEATURES_FILE, encoding="utf-8") as f:
                inactive = set(json.load(f))
            feat = feat.drop(
                columns=[c for c in inactive if c in feat.columns], errors="ignore"
            )
        except (OSError, json.JSONDecodeError) as exc:
            logger.warning("Failed loading inactive features: %s", exc)
    symbol = os.path.splitext(os.path.basename(csv_path))[0]
    feat["sentiment"] = fetch_sentiment(symbol)
    regime = detect_regime(df)
    if isinstance(regime, pd.Series):
        regime = regime.iloc[-1]
    model = load_model(regime)
    expected_features = list(model.feature_names_in_)
    missing = set(expected_features) - set(feat.columns)
    if missing:
        raise ValueError(f"Missing features: {missing}")
    features = (
        feat[expected_features]
        .iloc[-1:]
        .astype(dict.fromkeys(expected_features, "float64"))
    )
    try:
        pred = model.predict(features)[0]
        proba = model.predict_proba(features)[0][pred]
    except (ValueError, TypeError) as exc:
        logger.error("Prediction failed for %s: %s", symbol, exc)
        return None, None
    logger.info(
        "Regime: %s, Prediction: %s, Probability: %.4f",
        regime,
        pred,
        proba,
    )
    log_metrics({
        "timestamp": pd.Timestamp.now(tz="UTC").isoformat(),
        "symbol": symbol,
        "regime": regime,
        "prediction": int(pred),
        "probability": float(proba),
    }, filename="metrics/predictions.csv")
    return pred, proba


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Predict trade signal")
    parser.add_argument("csv", help="CSV file with OHLCV data")
    parser.add_argument("--freq", choices=["daily", "intraday"], default="intraday")
    args = parser.parse_args()
    predict(args.csv, args.freq)
