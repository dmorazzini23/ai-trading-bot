.....s..............................................................s.s. [  3%]
................................x....................................... [  7%]
........................................................................ [ 10%]
........................................................................ [ 14%]
........................................................................ [ 17%]
...............................s.............s.......................... [ 21%]
........................................................................ [ 24%]
........................................................................ [ 28%]
..................F..............FF.FFF................................F [ 31%]
...FFFFF..........................................s....F..........FF..FF [ 35%]
.s...F.................................................................. [ 38%]
.........................F........F..FF........F..s.........F..F.FFF.FF. [ 42%]
.....F........F....F...................................F.F.............. [ 45%]
.........................F.....ss.......s...F.....F..................... [ 49%]
......FFFF..................................................s......F..F. [ 53%]
...F.......................................F............F......sss...... [ 56%]
...sFFFFFFFFF......F..FF...F............................................ [ 60%]
.........................................F...FF..F...................... [ 63%]
.....................F............................s.F..............F.FF. [ 67%]
F...................F................................................... [ 70%]
......................F.........FFF.F.......................F.....FF.... [ 74%]
...F.................................................................... [ 77%]
F............................................................s.......... [ 81%]
..........................................FF....................F.F..... [ 84%]
............................F..F..................s...F............FF..F [ 88%]
F.....F......FFF.FFF................F.................F.F...F........... [ 91%]
....F.......F..F..FFF.F....FFFFF...................................F.... [ 95%]
.F...........F...F...........F.......................................... [ 99%]
...................                                                      [100%]
=================================== FAILURES ===================================
________________ test_feed_override_used_on_subsequent_requests ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7ebe8ef0>

    def test_feed_override_used_on_subsequent_requests(monkeypatch):
        _reset_state()
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(fetch, "_ALLOW_SIP", True)
        monkeypatch.setattr(fetch, "_HAS_SIP", True)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", False)
        monkeypatch.setattr(fetch, "alpaca_feed_failover", lambda: ("sip",))
        monkeypatch.setattr(fetch, "alpaca_empty_to_backup", lambda: False)
        monkeypatch.setattr(fetch, "_verify_minute_continuity", lambda df, *a, **k: df)
        monkeypatch.setattr(
            fetch,
            "_repair_rth_minute_gaps",
            lambda df, **k: (df, {"status": "ok"}, False),
        )
        monkeypatch.setattr(fetch, "provider_priority", lambda: ["alpaca_iex", "alpaca_sip"])
        monkeypatch.setattr(fetch, "max_data_fallbacks", lambda: 2)
        monkeypatch.setattr(
            fetch,
            "_backup_get_bars",
            lambda *a, **k: pd.DataFrame(
                {
                    "t": [start],
                    "o": [2.0],
                    "h": [2.0],
                    "l": [2.0],
                    "c": [2.0],
                    "v": [2],
                }
            ),
        )
    
        start = datetime(2024, 1, 2, 15, 30, tzinfo=UTC)
        end = start + timedelta(minutes=1)
    
        first_session = _Session(
            [
                _Resp({"bars": []}, correlation="iex"),
                _Resp(
                    {
                        "bars": [
                            {"t": "2024-01-01T00:00:00Z", "o": 1, "h": 1, "l": 1, "c": 1, "v": 1}
                        ]
                    },
                    correlation="sip",
                ),
            ]
        )
        monkeypatch.setattr(fetch, "_HTTP_SESSION", first_session)
    
        df_first = fetch.get_minute_df("AAPL", start, end)
        assert hasattr(df_first, "empty")
        assert not getattr(df_first, "empty", True)
        assert list(df_first.columns[:6]) == [
            "timestamp",
            "open",
            "high",
            "low",
            "close",
            "volume",
        ]
        assert first_session.calls[0]["feed"] == "iex"
        assert first_session.calls[1]["feed"] == "sip"
        assert fetch._FEED_OVERRIDE_BY_TF[("AAPL", "1Min")] == "sip"
    
        second_session = _Session([
            _Resp({"bars": []}, correlation="iex2"),
            _Resp(
                {
                    "bars": [
                        {"t": "2024-01-01T00:00:00Z", "o": 2, "h": 2, "l": 2, "c": 2, "v": 2}
                    ]
                },
                correlation="sip2",
            ),
        ])
        monkeypatch.setattr(fetch, "_HTTP_SESSION", second_session)
    
        df_second = fetch.get_minute_df("AAPL", start, end)
        assert hasattr(df_second, "empty")
        assert not getattr(df_second, "empty", True)
        assert list(df_second.columns[:6]) == [
            "timestamp",
            "open",
            "high",
            "low",
            "close",
            "volume",
        ]
>       assert fetch._FEED_SWITCH_HISTORY == [("AAPL", "1Min", "sip")]
E       AssertionError: assert [('AAPL', '1M...', 'finnhub')] == [('AAPL', '1Min', 'sip')]
E         Left contains one more item: ('AAPL', '1Min', 'finnhub')
E         Use -v to get more diff

tests/test_feed_failover.py:432: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:02:53Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_STAY | provider=alpaca-iex reason=decision_window_active cooldown=120s", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:02:53Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_STAY | provider=alpaca-sip reason=healthy cooldown=120s", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:7747 EMPTY_BARS_DETECTED
INFO     ai_trading.data.fetch:__init__.py:7769 ALPACA_FEED_SWITCH
INFO     ai_trading.data.fetch:__init__.py:7070 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.logging:__init__.py:1315 FETCH_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:7128 DATA_SOURCE_FALLBACK_SUCCESS
WARNING  ai_trading.data.provider_monitor:provider_monitor.py:1291 DATA_PROVIDER_DISABLED
INFO     ai_trading.data.fetch:__init__.py:1748 ALPACA_FEED_SWITCH
WARNING  ai_trading.data.fetch:__init__.py:2085 BACKUP_PROVIDER_USED
INFO     ai_trading.data.provider_monitor:provider_monitor.py:665 DATA_PROVIDER_STAY | provider=alpaca-sip reason=from_provider_disabled cooldown=599s
INFO     ai_trading.data.provider_monitor:provider_monitor.py:665 DATA_PROVIDER_STAY | provider=alpaca-iex reason=healthy cooldown=120s
INFO     ai_trading.data.provider_monitor:provider_monitor.py:665 DATA_PROVIDER_STAY | provider=alpaca-iex reason=decision_window_active cooldown=120s
WARNING  ai_trading.data.fetch:__init__.py:9283 ALPACA_FETCH_FAILED
INFO     ai_trading.data.fetch:__init__.py:1748 ALPACA_FEED_SWITCH
WARNING  ai_trading.data.fetch:__init__.py:2085 BACKUP_PROVIDER_USED
INFO     ai_trading.data.provider_monitor:provider_monitor.py:665 DATA_PROVIDER_STAY | provider=alpaca-sip reason=healthy cooldown=120s
______________________ test_persistent_empty_aborts_early ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7ebe9460>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7ece79b0>

    def test_persistent_empty_aborts_early(monkeypatch, caplog):
        start, end = _dt_range()
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        payloads = [{"bars": []}, {"bars": []}, {"bars": []}]
        corr_ids = ["id1", "id2", "id3"]
        sess = _Session(payloads, corr_ids)
        monkeypatch.setattr(fetch, "_HTTP_SESSION", sess)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", True)
        monkeypatch.setattr(fetch, "is_market_open", lambda: True)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: False)
        monkeypatch.setattr(
            fetch,
            "time",
            types.SimpleNamespace(monotonic=lambda: 0.0, sleep=lambda _s: None),
        )
    
        with caplog.at_level(logging.WARNING):
            out = fetch._fetch_bars("AAPL", start, end, "1Min", feed="iex")
    
        assert out is None
>       assert sess.calls == 2
E       assert 0 == 2
E        +  where 0 = <tests.test_fetch_empty_early_exit._Session object at 0x7c1d7ece7da0>.calls

tests/test_fetch_empty_early_exit.py:59: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:12Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1m", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
_____________________ test_warn_on_empty_when_market_open ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d91ee56a0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7ffda6f0>

    def test_warn_on_empty_when_market_open(monkeypatch, caplog):
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        start, end = _dt_range()
        sess = _Session([{ "bars": []} for _ in range(4)])
        monkeypatch.setattr(fetch, "_HTTP_SESSION", sess)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", True)
        monkeypatch.setattr(fetch, "is_market_open", lambda: True)
        monkeypatch.setattr(fetch, "_empty_should_emit", lambda *a, **k: True)
        monkeypatch.setattr(fetch, "_empty_record", lambda *a, **k: 1)
        monkeypatch.setattr(fetch, "_empty_classify", lambda **k: logging.WARNING)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: False)
    
        elapsed = 0.0
        delays: list[float] = []
    
        def _monotonic() -> float:
            return elapsed
    
        def _sleep(sec: float) -> None:
            nonlocal elapsed
            elapsed += sec
            delays.append(sec)
    
        monkeypatch.setattr(fetch, "time", types.SimpleNamespace(monotonic=_monotonic, sleep=_sleep))
    
        with caplog.at_level(logging.DEBUG):
            out = fetch._fetch_bars("AAPL", start, end, "1Min")
    
        assert out is None
        assert sess.calls <= 2
        retry_logs = [r for r in caplog.records if r.message == "RETRY_EMPTY_BARS"]
        if retry_logs:
            assert [r.attempt for r in retry_logs] == [1]
            assert [r.total_elapsed for r in retry_logs] == [0]
            assert delays == [1]
        else:
            assert not delays or delays == [1]
>       assert any(
            r.message in {"EMPTY_DATA", "ALPACA_EMPTY_RESPONSE_THRESHOLD"}
            and r.levelno >= logging.INFO
            for r in caplog.records
        )
E       assert False
E        +  where False = any(<generator object test_warn_on_empty_when_market_open.<locals>.<genexpr> at 0x7c1d7eada190>)

tests/test_fetch_empty_handling.py:80: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:16Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "finnhub", "from_provider": "alpaca_iex", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "interval": "1m", "start": "2024-01-01T00:00:00+00:00", "end": "2024-01-01T00:01:00+00:00", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
_____________________ test_skip_retry_outside_market_hours _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d91ee7d70>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7ffdac30>

    def test_skip_retry_outside_market_hours(monkeypatch, caplog):
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        start, end = _dt_range()
        sess = _Session([{"bars": []}])
        monkeypatch.setattr(fetch, "_HTTP_SESSION", sess)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", True)
        monkeypatch.setattr(fetch, "is_market_open", lambda: False)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: True)
        monkeypatch.setattr(fetch, "_ENABLE_HTTP_FALLBACK", False, raising=False)
        monkeypatch.setattr(fetch, "alpaca_empty_to_backup", lambda: False)
    
        with caplog.at_level(logging.INFO):
>           with pytest.raises(fetch.EmptyBarsError) as exc:
E           Failed: DID NOT RAISE <class 'ai_trading.data.fetch.EmptyBarsError'>

tests/test_fetch_empty_handling.py:125: Failed
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:24Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "finnhub", "from_provider": "alpaca_iex", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "interval": "1m", "start": "2024-01-01T00:00:00+00:00", "end": "2024-01-01T00:01:00+00:00", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
____________________ test_fetch_bars_raises_on_retry_limit _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d76692870>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d75967410>

    def test_fetch_bars_raises_on_retry_limit(monkeypatch, caplog):
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        start, end = _dt_range()
        sess = _Session([{"bars": []} for _ in range(2)])
        monkeypatch.setattr(fetch, "_HTTP_SESSION", sess)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", True)
        monkeypatch.setattr(fetch, "_sip_fallback_allowed", lambda *a, **k: False)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: False)
        monkeypatch.setattr(fetch, "_ENABLE_HTTP_FALLBACK", False, raising=False)
        monkeypatch.setattr(fetch, "_FETCH_BARS_MAX_RETRIES", 1, raising=False)
        monkeypatch.setattr(fetch, "max_data_fallbacks", lambda: 0)
        monkeypatch.setattr(fetch, "fh_fetcher", None)
        monkeypatch.setattr(fetch, "alpaca_empty_to_backup", lambda: False)
        monkeypatch.setattr(fetch, "time", types.SimpleNamespace(monotonic=lambda: 0.0, sleep=lambda _s: None))
    
        with caplog.at_level(logging.WARNING):
>           with pytest.raises(fetch.EmptyBarsError) as exc:
E           Failed: DID NOT RAISE <class 'ai_trading.data.fetch.EmptyBarsError'>

tests/test_fetch_empty_handling.py:149: Failed
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:28Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1m", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
____________________ test_fetch_bars_handles_empty_priority ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759662d0>

    def test_fetch_bars_handles_empty_priority(monkeypatch):
        start, end = _dt_range()
        payload = {"bars": [{"t": "2024-01-01T00:00:00Z", "o": 1, "h": 1, "l": 1, "c": 1, "v": 1}]}
        sess = _Session([payload])
        monkeypatch.setattr(fetch, "_HTTP_SESSION", sess)
        monkeypatch.setattr(fetch, "provider_priority", lambda: [])
        monkeypatch.setattr(fetch, "max_data_fallbacks", lambda: 1)
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        monkeypatch.setattr(fetch, "is_market_open", lambda: True)
    
        df = fetch._fetch_bars("AAPL", start, end, "1Min", feed="iex")
    
>       assert not df.empty
E       AttributeError: 'NoneType' object has no attribute 'empty'

tests/test_fetch_empty_priority.py:50: AttributeError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:33Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "finnhub", "from_provider": "alpaca_iex", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "interval": "1m", "start": "2024-01-01T00:00:00+00:00", "end": "2024-01-01T00:01:00+00:00", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:03:33Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=4 key=\"BACKUP_PROVIDER_EMPTY\"", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=4 key="BACKUP_PROVIDER_EMPTY"
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
______________ test_get_latest_price_uses_yahoo_when_alpaca_none _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7f922c60>

    def test_get_latest_price_uses_yahoo_when_alpaca_none(monkeypatch):
        """Alpaca returning ``None`` should trigger Yahoo fallback."""
    
        monkeypatch.setattr(
            bot_engine,
            "_alpaca_symbols",
            lambda: (lambda *_a, **_k: {"ap": None}, None),
        )
    
        called: dict[str, bool] = {"yahoo": False}
    
        def fake_yahoo(symbol, start, end, interval):  # noqa: ARG001
            called["yahoo"] = True
            return _df(101.0)
    
        monkeypatch.setattr(data_fetcher, "_backup_get_bars", fake_yahoo)
        monkeypatch.setattr(bot_engine, "get_latest_close", lambda df: float(df["close"].iloc[-1]))
    
        price = bot_engine.get_latest_price("AAPL")
    
>       assert called["yahoo"]
E       assert False

tests/test_get_latest_price_fallback.py:117: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:03:43Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:03:43Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "finnhub", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:03:47Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
_____________ test_get_latest_price_skips_non_positive_from_yahoo ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d5b03b0>

    def test_get_latest_price_skips_non_positive_from_yahoo(monkeypatch):
        """Zeroes from Yahoo should be treated as invalid and fall back further."""
    
        monkeypatch.setattr(bot_engine, "_PRICE_SOURCE", {})
    
        monkeypatch.setattr(
            bot_engine,
            "_alpaca_symbols",
            lambda: (lambda *_a, **_k: {"ap": None}, None),
        )
    
        def yahoo_zero(symbol, start, end, interval):  # noqa: ARG001
            return _df(0.0)
    
        monkeypatch.setattr(data_fetcher, "_backup_get_bars", yahoo_zero)
        monkeypatch.setattr(bot_engine, "get_latest_close", lambda df: float(df["close"].iloc[-1]))
    
        def fail_bars(symbol):  # noqa: ARG001
            raise RuntimeError("bars unavailable")
    
        monkeypatch.setattr(bot_engine, "get_bars_df", fail_bars)
    
        price = bot_engine.get_latest_price("AAPL")
    
        assert price is None
>       assert bot_engine._PRICE_SOURCE["AAPL"] == "yahoo_invalid"
E       AssertionError: assert 'alpaca_trade_invalid' == 'yahoo_invalid'
E         - yahoo_invalid
E         + alpaca_trade_invalid

tests/test_get_latest_price_fallback.py:283: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:00Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "bars unavailable", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:00Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
__________________ test_get_latest_price_handles_auth_failure __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d76649940>

    def test_get_latest_price_handles_auth_failure(monkeypatch):
        monkeypatch.setattr(alpaca_api, "_ALPACA_SERVICE_AVAILABLE", True)
        monkeypatch.setattr(bot_engine, "_PRICE_SOURCE", {}, raising=False)
        monkeypatch.setattr(
            bot_engine,
            "_GLOBAL_INTRADAY_FALLBACK_FEED",
            "yahoo",
            raising=False,
        )
        monkeypatch.setattr(
            bot_engine,
            "_GLOBAL_CYCLE_MINUTE_FEED_OVERRIDE",
            {"AAPL": "yahoo"},
            raising=False,
        )
        monkeypatch.setattr(
            bot_engine,
            "_INTRADAY_FEED_CACHE",
            "iex",
            raising=False,
        )
    
        captured: dict[str, Any] = {}
    
        def raise_auth(*_a, **_k):
            captured["params"] = dict(_k.get("params", {}) or {})
            monkeypatch.setattr(alpaca_api, "_ALPACA_SERVICE_AVAILABLE", False)
            raise AlpacaAuthenticationError("Unauthorized")
    
        def fail_backup(*_a, **_k):  # pragma: no cover - defensive guard
            raise AssertionError("Backup provider should not be queried on auth failure")
    
        monkeypatch.setattr(bot_engine, "_alpaca_symbols", lambda: (raise_auth, None))
        monkeypatch.setattr(data_fetcher, "_backup_get_bars", fail_backup)
        monkeypatch.setattr(bot_engine, "get_bars_df", fail_backup)
    
        price = bot_engine.get_latest_price("AAPL")
    
        assert price is None
>       assert bot_engine._PRICE_SOURCE["AAPL"] == "alpaca_auth_failed"
E       AssertionError: assert 'yahoo_invalid' == 'alpaca_auth_failed'
E         - alpaca_auth_failed
E         + yahoo_invalid

tests/test_get_latest_price_fallback.py:325: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:04Z", "level": "WARNING", "name": "ai_trading.core.bot_engine", "msg": "ALPACA_TRADE_FETCH_FAILED", "provider": "alpaca_trade", "symbol": "AAPL", "error": "HTTP 401: Unauthorized", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:04Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "finnhub", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:08Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.core.bot_engine:bot_engine.py:1518 ALPACA_TRADE_FETCH_FAILED
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
_______________ test_get_latest_price_prefer_backup_skips_alpaca _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7ece69f0>

    def test_get_latest_price_prefer_backup_skips_alpaca(monkeypatch):
        monkeypatch.setattr(bot_engine, "_PRICE_SOURCE", {}, raising=False)
    
        called = {"alpaca": False}
    
        def fake_alpaca(*_a, **_k):  # pragma: no cover - should not be invoked
            called["alpaca"] = True
            return {"ap": 123.0}
    
        monkeypatch.setattr(bot_engine, "_alpaca_symbols", lambda: (fake_alpaca, None))
    
        def fake_yahoo(symbol, start, end, interval):  # noqa: ARG001
            return _df(104.0)
    
        monkeypatch.setattr(data_fetcher, "_backup_get_bars", fake_yahoo)
        monkeypatch.setattr(
            bot_engine,
            "get_latest_close",
            lambda df: float(df["close"].iloc[-1]),
        )
    
        price = bot_engine.get_latest_price("AAPL", prefer_backup=True)
    
>       assert price == 104.0
E       assert None == 104.0

tests/test_get_latest_price_fallback.py:358: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:12Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "finnhub", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:16Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:16Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=4 key=\"BACKUP_PROVIDER_EMPTY\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:16Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=4 key="BACKUP_PROVIDER_EMPTY"
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
__________ test_resolve_trade_quote_prefers_backup_when_primary_zero ___________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d766931d0>

    def test_resolve_trade_quote_prefers_backup_when_primary_zero(monkeypatch):
        monkeypatch.setattr(bot_engine, "_PRICE_SOURCE", {}, raising=False)
    
        def fake_alpaca(*_a, **_k):
            return {"ap": 0.0, "bp": 0.0}
    
        monkeypatch.setattr(bot_engine, "_alpaca_symbols", lambda: (fake_alpaca, None))
    
        def fake_yahoo(symbol, start, end, interval):  # noqa: ARG001
            return _df(102.5)
    
        monkeypatch.setattr(data_fetcher, "_backup_get_bars", fake_yahoo)
        monkeypatch.setattr(
            bot_engine,
            "get_latest_close",
            lambda df: float(df["close"].iloc[-1]),
        )
    
        quote = bot_engine.resolve_trade_quote("AAPL")
    
>       assert quote.price == 102.5
E       AssertionError: assert None == 102.5
E        +  where None = namespace(price=None, source='yahoo_invalid').price

tests/test_get_latest_price_fallback.py:383: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:21Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "finnhub", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:25Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:25Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:33Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:04:33Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_PROVIDER_EMPTY", "provider": "finnhub", "symbol": "AAPL", "interval": "1d", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
______________________ test_fetch_success_no_error_logged ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7ebe9880>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d678710>

    def test_fetch_success_no_error_logged(monkeypatch, caplog):
        start, end = _dt_range()
        df = pd.DataFrame({"t": [start], "o": [1], "h": [1], "l": [1], "c": [1], "v": [1]})
        _reset_state(monkeypatch)
        monkeypatch.setenv("ENABLE_FINNHUB", "0")
        monkeypatch.setattr(data_fetcher, "_has_alpaca_keys", lambda: True)
        monkeypatch.setattr(data_fetcher, "_fetch_bars", lambda *a, **k: df)
    
        called = {"yahoo": False}
    
        def _yahoo(*a, **k):
            called["yahoo"] = True
            return pd.DataFrame()
    
        monkeypatch.setattr(data_fetcher, "_yahoo_get_bars", _yahoo)
    
        with caplog.at_level(logging.WARNING):
>           out = data_fetcher.get_minute_df("AAPL", start, end)

tests/test_get_minute_df_fetch_logging.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

symbol = 'AAPL'
start = datetime.datetime(2024, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)
end = datetime.datetime(2024, 1, 1, 0, 1, tzinfo=datetime.timezone.utc)
feed = None

    def get_minute_df(
        symbol: str,
        start: Any,
        end: Any,
        feed: str | None = None,
        *,
        backfill: str | None = None,
    ) -> pd.DataFrame | None:
        """Minute bars fetch with provider fallback and gap handling.
    
        Also updates in-memory minute cache for freshness checks."""
        pd = _ensure_pandas()
        last_complete_evaluations = 0
    
        def _evaluate_last_complete(
            fallback: _dt.datetime | None = None,
        ) -> _dt.datetime:
            nonlocal last_complete_evaluations
            try:
                value = _last_complete_minute(pd)
            except Exception:
                if fallback is not None:
                    return fallback
                raise
            else:
                last_complete_evaluations += 1
                return value
    
        start_dt = ensure_datetime(start)
        end_dt = ensure_datetime(end)
        pytest_active = _detect_pytest_env()
        last_complete_minute = _evaluate_last_complete()
        if end_dt > last_complete_minute:
            end_dt = max(start_dt, last_complete_minute)
        fallback_window_used = _used_fallback(symbol, "1Min", start_dt, end_dt)
        if pytest_active and fallback_window_used:
            _clear_minute_fallback_state(symbol, "1Min", start_dt, end_dt)
            fallback_window_used = False
        fallback_metadata: dict[str, str] | None = None
        skip_primary_due_to_fallback = False
        skip_due_to_metadata = False
        fallback_ttl_active = False
        if fallback_window_used:
            try:
                fallback_metadata = get_fallback_metadata(symbol, "1Min", start_dt, end_dt)
            except Exception:
                fallback_metadata = None
            provider_hint = None
            if isinstance(fallback_metadata, dict):
                provider_hint = fallback_metadata.get("fallback_provider") or fallback_metadata.get("resolved_provider")
                if provider_hint and str(provider_hint).strip().lower() == "yahoo":
                    skip_primary_due_to_fallback = True
                    skip_due_to_metadata = True
        window_has_session = _window_has_trading_session(start_dt, end_dt)
        global _state
        _state = {
            "window_has_session": bool(window_has_session),
            "no_session_forced": bool(not window_has_session),
        }
        tf_key = (symbol, "1Min")
        if not window_has_session and not _ENABLE_HTTP_FALLBACK:
            _SKIPPED_SYMBOLS.discard(tf_key)
            _EMPTY_BAR_COUNTS.pop(tf_key, None)
            _IEX_EMPTY_COUNTS.pop(tf_key, None)
            empty_frame = _empty_ohlcv_frame(pd)
            if empty_frame is not None:
                return empty_frame
            pandas_mod = load_pandas()
            if pandas_mod is not None:
                try:
                    return pandas_mod.DataFrame()
                except Exception:
                    pass
            return pd.DataFrame() if pd is not None else []  # type: ignore[return-value]
        _ensure_override_state_current()
        normalized_feed = _normalize_feed_value(feed) if feed is not None else None
        backup_provider_str, backup_provider_normalized = _resolve_backup_provider()
        resolved_backup_provider = backup_provider_normalized or backup_provider_str
        resolved_backup_feed = backup_provider_normalized or None
    
        http_fallback_env = os.getenv("ENABLE_HTTP_FALLBACK")
        if http_fallback_env is None:
            fallback_allowed_flag = bool(_ENABLE_HTTP_FALLBACK)
        else:
            fallback_allowed_flag = http_fallback_env.strip().lower() not in {"0", "false", "no", "off"}
    
        ttl_until: int | None = None
        now_s_cached: int | None = None
    
        def _now_seconds() -> int:
            nonlocal now_s_cached
            if now_s_cached is None:
                try:
                    now_s_cached = int(_dt.datetime.now(tz=UTC).timestamp())
                except Exception:
                    now_s_cached = int(_time_now())
            return now_s_cached
    
        try:
            ttl_until_value = _FALLBACK_UNTIL.get(tf_key)
            if ttl_until_value is not None:
                ttl_until = int(ttl_until_value)
        except Exception:
            ttl_until = None
        if ttl_until is not None:
            fallback_ttl_active = _now_seconds() < ttl_until
        if fallback_ttl_active:
            if pytest_active:
                fallback_ttl_active = False
            else:
                skip_primary_due_to_fallback = True
                if fallback_metadata is None:
                    fallback_metadata = {}
                if resolved_backup_provider:
                    fallback_metadata.setdefault("fallback_provider", resolved_backup_provider)
                if resolved_backup_feed:
                    fallback_metadata.setdefault("fallback_feed", resolved_backup_feed)
        if not fallback_ttl_active and skip_due_to_metadata:
            # Reconsider primary fetch attempts once fallback TTL expires.
            skip_primary_due_to_fallback = False
    
        forced_skip_engaged = False
        forced_skip_until = _BACKUP_SKIP_UNTIL.get(tf_key)
        if forced_skip_until is not None:
            if pytest_active and not fallback_allowed_flag:
                _clear_backup_skip(symbol, "1Min")
                skip_primary_due_to_fallback = False
            else:
                if not isinstance(forced_skip_until, datetime):
                    try:
                        forced_skip_until = datetime.fromtimestamp(float(forced_skip_until), tz=UTC)
                    except Exception:
                        _clear_backup_skip(symbol, "1Min")
                        forced_skip_until = None
                if isinstance(forced_skip_until, datetime):
                    now_dt = datetime.now(tz=UTC)
                    if now_dt < forced_skip_until:
                        skip_primary_due_to_fallback = True
                        if fallback_allowed_flag:
                            forced_skip_engaged = True
                    elif not (pytest_active and fallback_allowed_flag):
                        _clear_backup_skip(symbol, "1Min")
    
        disabled_until_map = getattr(provider_monitor, "disabled_until", {})
        if not isinstance(disabled_until_map, Mapping):
            disabled_until_map = {}
        if pytest_active and _alpaca_disabled_until is None and not disabled_until_map.get("alpaca"):
            if forced_skip_engaged:
                # Preserve dwell-induced skip while the backup cool-down is active.
                skip_primary_due_to_fallback = True
            else:
                skip_primary_due_to_fallback = False
                try:
                    _clear_minute_fallback_state(symbol, "1Min", start_dt, end_dt)
                except Exception:
                    pass
        elif forced_skip_engaged:
            skip_primary_due_to_fallback = True
    
        used_backup = False
        minute_metrics: dict[str, Any] = {
            "success_emitted": False,
            "fallback_tags": None,
            "fallback_emitted": False,
        }
        backup_skip_engaged = False
    
        def _register_backup_skip() -> None:
            nonlocal backup_skip_engaged
            backup_skip_engaged = True
            try:
                skip_until = datetime.now(tz=UTC) + _BACKUP_SKIP_WINDOW
            except Exception:
                skip_until = None
            _set_backup_skip(symbol, "1Min", until=skip_until)
    
        def _track_backup_frame(frame: Any | None) -> Any | None:
            if _frame_has_rows(frame):
                _register_backup_skip()
            return frame
    
        def _log_primary_failure(reason: str) -> None:
            nonlocal primary_failure_logged
            if primary_failure_logged:
                return
            if (
                reason == "fallback_in_use"
                and (normalized_feed == "sip" or requested_feed == "sip")
                and (_SIP_UNAUTHORIZED or _is_sip_unauthorized())
            ):
                _log_sip_unavailable(symbol, "1Min")
            logger.warning(
                "ALPACA_FETCH_FAILED",
                extra={"symbol": symbol, "err": str(reason)},
            )
            primary_failure_logged = True
    
        def _minute_backup_get_bars(
            symbol_arg: str,
            start_arg: Any,
            end_arg: Any,
            *,
            interval: str,
        ) -> Any | None:
            nonlocal backup_attempted
            backup_attempted = True
            return _track_backup_frame(
                _safe_backup_get_bars(symbol_arg, start_arg, end_arg, interval=interval)
            )
    
        def _record_minute_success(tags: dict[str, str], *, prefer_fallback: bool = False) -> None:
            if minute_metrics.get("success_emitted"):
                return
            selected_tags = dict(tags)
            fallback_tags = minute_metrics.get("fallback_tags")
            if (prefer_fallback or minute_metrics.get("fallback_emitted")) and isinstance(fallback_tags, dict):
                selected_tags = dict(fallback_tags)
            _incr("data.fetch.success", value=1.0, tags=selected_tags)
            minute_metrics["success_emitted"] = True
            minute_metrics["fallback_emitted"] = False
    
        def _record_minute_fallback_success(tags: dict[str, str]) -> None:
            minute_metrics["fallback_tags"] = dict(tags)
            minute_metrics["fallback_emitted"] = True
            _incr("data.fetch.fallback_success", value=1.0, tags=dict(tags))
    
        def _record_minute_fallback(
            *,
            frame: Any | None = None,
            timeframe: str = "1Min",
            window_start: _dt.datetime | None = None,
            window_end: _dt.datetime | None = None,
            from_feed: str | None = None,
        ) -> None:
            _log_primary_failure("fallback_in_use")
            start_window = window_start or start_dt
            end_window = window_end or end_dt
            source_feed = from_feed or normalized_feed or _DEFAULT_FEED
            provider_tag = resolved_backup_provider or "yahoo"
            feed_tag = resolved_backup_feed or provider_tag or "yahoo"
            if frame is not None:
                try:
                    attrs = getattr(frame, "attrs", {})
                except Exception:
                    attrs = {}
                if isinstance(attrs, dict):
                    provider_attr = attrs.get("data_provider") or attrs.get("fallback_provider")
                    feed_attr = attrs.get("data_feed") or attrs.get("fallback_feed")
                    if provider_attr:
                        provider_tag = str(provider_attr).strip() or provider_tag
                    if feed_attr:
                        feed_tag = str(feed_attr).strip() or feed_tag
            tags = {
                "provider": str(provider_tag or "unknown"),
                "symbol": symbol,
                "feed": str(feed_tag or provider_tag or "unknown"),
                "timeframe": timeframe,
            }
    
            frame_has_rows = _frame_has_rows(frame)
            _incr("data.fetch.fallback_attempt", value=1.0, tags=tags)
            if frame_has_rows:
                _record_minute_fallback_success(tags)
                _register_backup_skip()
                fallback_feed: str | None = None
                if feed_tag:
                    try:
                        fallback_feed = _normalize_feed_value(feed_tag)
                    except Exception:
                        try:
                            fallback_feed = str(feed_tag).strip().lower() or None
                        except Exception:
                            fallback_feed = None
                try:
                    source_feed_norm = _normalize_feed_value(source_feed)
                except Exception:
                    try:
                        source_feed_norm = str(source_feed).strip().lower() or None
                    except Exception:
                        source_feed_norm = None
                if (
                    source_feed_norm
                    and fallback_feed
                    and fallback_feed != source_feed_norm
                ):
                    _record_feed_switch(symbol, timeframe, source_feed_norm, fallback_feed)
                _record_minute_success(tags, prefer_fallback=True)
            _mark_fallback(
                symbol,
                timeframe,
                start_window,
                end_window,
                from_provider=f"alpaca_{source_feed}",
                fallback_df=frame,
                resolved_provider=resolved_backup_provider,
                resolved_feed=resolved_backup_feed,
                reason=_state.get("fallback_reason"),
            )
            _state["fallback_reason"] = None
        if normalized_feed is None:
            cached_cycle_feed = _fallback_cache_for_cycle(_get_cycle_id(), symbol, "1Min")
            if cached_cycle_feed:
                try:
                    normalized_feed = _normalize_feed_value(cached_cycle_feed)
                except Exception:
                    normalized_feed = str(cached_cycle_feed).strip().lower()
        finnhub_key_present = bool(os.getenv("FINNHUB_API_KEY")) and fh_fetcher is not None and not getattr(fh_fetcher, "is_stub", False)
        if tf_key in _SKIPPED_SYMBOLS:
            skip_window_until = _BACKUP_SKIP_UNTIL.get(tf_key)
            skip_window_active = False
            if isinstance(skip_window_until, datetime):
                try:
                    now_dt = datetime.now(tz=UTC)
                except Exception:
                    now_dt = None
                if now_dt is not None and skip_window_until > now_dt:
                    skip_window_active = True
            elif skip_window_until is not None:
                try:
                    until_float = float(skip_window_until)
                except Exception:
                    until_float = None
                if until_float is not None:
                    try:
                        converted_until = datetime.fromtimestamp(until_float, tz=UTC)
                    except Exception:
                        converted_until = None
                    if converted_until is not None:
                        try:
                            now_dt = datetime.now(tz=UTC)
                        except Exception:
                            now_dt = None
                        if now_dt is not None and converted_until > now_dt:
                            skip_window_active = True
            if skip_primary_due_to_fallback or (fallback_allowed_flag and skip_window_active):
                skip_primary_due_to_fallback = True
            elif window_has_session:
                if finnhub_key_present:
                    _SKIPPED_SYMBOLS.discard(tf_key)
                else:
                    logger.debug("SKIP_SYMBOL_EMPTY_BARS", extra={"symbol": symbol})
                    raise EmptyBarsError(f"empty_bars: symbol={symbol}, timeframe=1Min, skipped=1")
            else:
                _SKIPPED_SYMBOLS.discard(tf_key)
                _EMPTY_BAR_COUNTS.pop(tf_key, None)
        backoff_applied = False
        try:
            attempt = record_attempt(symbol, "1Min")
        except EmptyBarsError:
            cnt = _EMPTY_BAR_COUNTS.get(tf_key, MAX_EMPTY_RETRIES + 1)
            _log_with_capture(
                logging.ERROR,
                "ALPACA_EMPTY_BAR_MAX_RETRIES",
                extra={"symbol": symbol, "timeframe": "1Min", "occurrences": cnt},
            )
            log_empty_retries_exhausted(
                "alpaca",
                symbol=symbol,
                timeframe="1Min",
                feed=normalized_feed or _DEFAULT_FEED,
                retries=cnt,
            )
            raise
        attempt_count_snapshot = attempt
        used_backup = False
        success_marked = False
        fallback_logged = False
        fallback_frame: Any | None = None
        backup_attempted = False
        enable_finnhub = os.getenv("ENABLE_FINNHUB", "1").lower() not in ("0", "false")
        has_finnhub = finnhub_key_present
        use_finnhub = enable_finnhub and bool(has_finnhub)
        finnhub_disabled_requested = False
        df = None
        primary_frame_acquired = False
        last_empty_error: EmptyBarsError | None = None
        provider_str, backup_normalized = _resolve_backup_provider()
        backup_label = (backup_normalized or provider_str.lower() or "").strip()
        primary_label = f"alpaca_{normalized_feed or _DEFAULT_FEED}"
        primary_failure_logged = False
        allow_empty_override = False
    
        def _disable_signal_active(provider_label: str) -> bool:
            try:
                disabled_map = getattr(provider_monitor, "disabled_until", {})
            except Exception:
                return False
            if not isinstance(disabled_map, Mapping):
                return False
            candidates = {provider_label}
            base_label = provider_label.split("_", 1)[0].strip()
            if base_label:
                candidates.add(base_label)
            now: _dt.datetime | None = None
            for candidate in candidates:
                if not candidate:
                    continue
                until = disabled_map.get(candidate)
                if isinstance(until, _dt.datetime):
                    if now is None:
                        try:
                            now = _dt.datetime.now(UTC)
                        except Exception:
                            now = None
                    if now is None or until > now:
                        return True
                elif until:
                    return True
            return False
        force_primary_fetch = not skip_primary_due_to_fallback
        if backup_label:
            prefer_primary_first = bool(os.getenv("PYTEST_RUNNING")) or not _disable_signal_active(primary_label)
            if skip_primary_due_to_fallback:
                active_provider = backup_label
            elif prefer_primary_first:
                try:
                    monitored_choice = provider_monitor.active_provider(primary_label, backup_label)
                except Exception:
                    monitored_choice = primary_label
                active_provider = monitored_choice if monitored_choice == backup_label else primary_label
            else:
                active_provider = provider_monitor.active_provider(primary_label, backup_label)
            if active_provider == backup_label:
                try:
                    refreshed_last_minute = _evaluate_last_complete(last_complete_minute)
                except Exception:
                    refreshed_last_minute = last_complete_minute
                else:
                    last_complete_minute = refreshed_last_minute
                backup_end_dt = end_dt
                if refreshed_last_minute is not None:
                    candidate_end = min(backup_end_dt, refreshed_last_minute)
                    backup_end_dt = max(start_dt, candidate_end)
                    if backup_end_dt != end_dt:
                        end_dt = backup_end_dt
                backup_attempted = True
                try:
                    df = _minute_backup_get_bars(symbol, start_dt, backup_end_dt, interval="1m")
                except Exception:
                    df = None
                else:
                    df = _post_process(df, symbol=symbol, timeframe="1Min")
                    if df is not None and not getattr(df, "empty", True):
                        used_backup = True
                        force_primary_fetch = False
                        fallback_frame = df
                        _register_backup_skip()
                    else:
                        df = None
        requested_feed = normalized_feed or _DEFAULT_FEED
        feed_to_use = requested_feed
        initial_feed = requested_feed
        override_feed: str | None = None
        proactive_switch = False
        switch_recorded = False
    
        finnhub_api_key = os.getenv("FINNHUB_API_KEY")
        if finnhub_api_key and fh_fetcher is not None and not getattr(fh_fetcher, "is_stub", True):
            finnhub_frame = _minute_df_from_finnhub(symbol, start_dt, end_dt)
            if finnhub_frame is not None and not getattr(finnhub_frame, "empty", True):
                finnhub_processed = _post_process(finnhub_frame, symbol=symbol, timeframe="1Min")
                tags = {
                    "provider": "finnhub",
                    "symbol": symbol,
                    "feed": "finnhub",
                    "timeframe": "1Min",
                }
                _record_minute_fallback_success(tags)
                _register_backup_skip()
                used_backup = True
                backup_attempted = True
                fallback_frame = finnhub_processed
                _record_minute_success(tags, prefer_fallback=True)
                _mark_fallback(
                    symbol,
                    "1Min",
                    start_dt,
                    end_dt,
                    from_provider=f"alpaca_{requested_feed}",
                    fallback_df=finnhub_processed,
                    resolved_provider="finnhub",
                    resolved_feed="finnhub",
                    reason=_state.get("fallback_reason"),
                )
                _state["fallback_reason"] = None
                return finnhub_processed
    
        if _has_alpaca_keys() and force_primary_fetch:
            try:
                override_raw = _FEED_OVERRIDE_BY_TF.get(tf_key) if feed is None else None
                if override_raw is not None:
                    override_feed = _normalize_feed_value(override_raw)
                feed_to_use = override_feed or requested_feed
                initial_feed = requested_feed
                sip_locked_primary = _is_sip_unauthorized()
                if (
                    feed_to_use == "iex"
                    and _IEX_EMPTY_COUNTS.get(tf_key, 0) > 0
                    and _sip_configured()
                    and not sip_locked_primary
                ):
                    feed_to_use = "sip"
                    payload = _format_fallback_payload_df("1Min", "sip", start_dt, end_dt)
                    logger.info("DATA_SOURCE_FALLBACK_ATTEMPT", extra={"provider": "alpaca", "fallback": payload})
                    proactive_switch = True
                elif feed_to_use == "iex" and _IEX_EMPTY_COUNTS.get(tf_key, 0) > 0 and not _sip_configured():
                    _log_sip_unavailable(symbol, "1Min", "SIP_UNAVAILABLE")
                df = _fetch_bars(symbol, start_dt, end_dt, "1Min", feed=feed_to_use)
                fallback_http_provider = False
                if df is not None:
                    try:
                        attrs = getattr(df, "attrs", None)
                    except Exception:
                        attrs = None
                    if isinstance(attrs, dict):
                        provider_attr = attrs.get("data_provider") or attrs.get("fallback_provider")
                        if provider_attr and str(provider_attr).strip().lower() == "yahoo":
                            fallback_http_provider = True
                if fallback_http_provider:
                    used_backup = True
                    primary_frame_acquired = False
                    fallback_frame = df
                    _register_backup_skip()
                elif _frame_has_rows(df):
                    primary_frame_acquired = True
                if proactive_switch and feed_to_use != initial_feed:
                    if not switch_recorded:
                        _record_feed_switch(symbol, "1Min", initial_feed, feed_to_use)
                        switch_recorded = True
            except (EmptyBarsError, ValueError, RuntimeError, AttributeError) as e:
                provider_feed_label = f"alpaca_{feed_to_use}"
                if isinstance(e, EmptyBarsError):
                    last_empty_error = e
                    _log_fetch_minute_empty(provider_feed_label, "empty_bars", str(e), symbol=symbol)
                    now = datetime.now(UTC)
                    if end_dt > now or start_dt > now:
                        logger.info(
                            "ALPACA_EMPTY_BAR_FUTURE",
                            extra={"symbol": symbol, "timeframe": "1Min"},
                        )
                        _EMPTY_BAR_COUNTS.pop(tf_key, None)
                        _IEX_EMPTY_COUNTS.pop(tf_key, None)
                        _SKIPPED_SYMBOLS.discard(tf_key)
                        if not switch_recorded:
                            try:
                                priorities = provider_priority()
                            except Exception:
                                priorities = ()
                            fallback_target: str | None = None
                            current_feed = str(feed_to_use or initial_feed or "").replace("alpaca_", "")
                            for priority in priorities or ():
                                candidate = str(priority or "").replace("alpaca_", "")
                                if not candidate or candidate == current_feed:
                                    continue
                                if candidate in {"iex", "sip"}:
                                    fallback_target = candidate
                                    break
                            if fallback_target is None and current_feed == "iex" and _sip_configured():
                                fallback_target = "sip"
                            if fallback_target and fallback_target != current_feed:
                                _record_feed_switch(symbol, "1Min", current_feed, fallback_target)
                        return pd.DataFrame() if pd is not None else []  # type: ignore[return-value]
                    try:
                        market_open = is_market_open()
                    except Exception:  # pragma: no cover - defensive
                        market_open = True
                    if not market_open and not _state.get("no_session_forced", False):
                        _log_with_capture(
                            logging.INFO,
                            "ALPACA_EMPTY_BAR_MARKET_CLOSED",
                            extra={"symbol": symbol, "timeframe": "1Min"},
                        )
                        _EMPTY_BAR_COUNTS.pop(tf_key, None)
                        _IEX_EMPTY_COUNTS.pop(tf_key, None)
                        _SKIPPED_SYMBOLS.discard(tf_key)
                        return pd.DataFrame() if pd is not None else []  # type: ignore[return-value]
                    cnt = _EMPTY_BAR_COUNTS.get(tf_key, attempt)
                    if market_open:
                        if attempt == 1:
                            _log_with_capture(
                                logging.INFO,
                                "EMPTY_DATA",
                                extra={"symbol": symbol, "timeframe": "1Min"},
                            )
                        elif attempt >= _EMPTY_RETRY_THRESHOLD:
                            _log_with_capture(
                                logging.WARNING,
                                "ALPACA_EMPTY_RESPONSE_THRESHOLD",
                                extra={"symbol": symbol, "timeframe": "1Min", "attempt": attempt},
                            )
                    if cnt > _EMPTY_BAR_MAX_RETRIES:
                        _log_with_capture(
                            logging.ERROR,
                            "ALPACA_EMPTY_BAR_MAX_RETRIES",
                            extra={"symbol": symbol, "timeframe": "1Min", "occurrences": cnt},
                        )
                        log_empty_retries_exhausted(
                            "alpaca",
                            symbol=symbol,
                            timeframe="1Min",
                            feed=normalized_feed or _DEFAULT_FEED,
                            retries=cnt,
                        )
                        _SKIPPED_SYMBOLS.add(tf_key)
                        raise EmptyBarsError(f"empty_bars: symbol={symbol}, timeframe=1Min, max_retries={cnt}") from e
                    if cnt >= _EMPTY_BAR_THRESHOLD:
                        backoff = min(2 ** (cnt - _EMPTY_BAR_THRESHOLD), 60)
                        ctx = {
                            "symbol": symbol,
                            "timeframe": "1Min",
                            "occurrences": cnt,
                            "backoff": backoff,
                            "finnhub_enabled": use_finnhub,
                            "feed": normalized_feed or _DEFAULT_FEED,
                        }
                        _log_with_capture(logging.WARNING, "ALPACA_EMPTY_BAR_BACKOFF", extra=ctx)
                        time.sleep(backoff)
                        backoff_applied = True
                        alt_feed = None
                        max_fb = max_data_fallbacks()
                        attempted_feeds = _FEED_FAILOVER_ATTEMPTS.setdefault(tf_key, set())
                        current_feed = normalized_feed or _DEFAULT_FEED
                        sip_locked_backoff = _is_sip_unauthorized()
                        if max_fb >= 1 and len(attempted_feeds) < max_fb:
                            priority_order = list(provider_priority())
                            if not priority_order:
                                allow_empty_override = True
                            if not _sip_allowed():
                                priority_order = [
                                    prov for prov in priority_order if prov != "alpaca_sip"
                                ]
                            try:
                                cur_idx = priority_order.index(f"alpaca_{current_feed}")
                            except ValueError:
                                cur_idx = -1
                            ordered_scan: list[str] = []
                            if cur_idx >= 0:
                                ordered_scan.extend(priority_order[cur_idx + 1 :])
                                ordered_scan.extend(reversed(priority_order[:cur_idx]))
                            else:
                                ordered_scan = priority_order
                            for prov in ordered_scan:
                                if not prov.startswith("alpaca_"):
                                    continue
                                candidate = prov.split("_", 1)[1]
                                if candidate == current_feed:
                                    continue
                                if candidate not in {"iex", "sip"}:
                                    continue
                                if candidate in attempted_feeds:
                                    continue
                                if candidate == "sip" and (not _sip_configured() or sip_locked_backoff):
                                    continue
                                alt_feed = candidate
                                break
                        if (
                            alt_feed is None
                            and max_fb >= 1
                            and len(attempted_feeds) < max_fb
                            and current_feed == "iex"
                            and _sip_configured()
                            and not sip_locked_backoff
                            and "sip" not in attempted_feeds
                        ):
                            alt_feed = "sip"
                        if alt_feed and alt_feed != current_feed:
                            attempted_feeds.add(alt_feed)
                            if not switch_recorded:
                                _record_feed_switch(symbol, "1Min", current_feed, alt_feed)
                                switch_recorded = True
                            try:
                                df_alt = _fetch_bars(symbol, start_dt, end_dt, "1Min", feed=alt_feed)
                            except (EmptyBarsError, ValueError, RuntimeError) as alt_err:
                                logger.debug(
                                    "ALPACA_ALT_FEED_FAILED",
                                    extra={
                                        "symbol": symbol,
                                        "from_feed": normalized_feed or _DEFAULT_FEED,
                                        "to_feed": alt_feed,
                                        "err": str(alt_err),
                                    },
                                )
                                df_alt = None
                            else:
                                if df_alt is not None and not getattr(df_alt, "empty", True):
                                    logger.info(
                                        "ALPACA_ALT_FEED_SUCCESS",
                                        extra={
                                            "symbol": symbol,
                                            "from_feed": normalized_feed or _DEFAULT_FEED,
                                            "to_feed": alt_feed,
                                            "timeframe": "1Min",
                                        },
                                    )
                                    _IEX_EMPTY_COUNTS.pop(tf_key, None)
                                    df_alt = _post_process(df_alt, symbol=symbol, timeframe="1Min")
                                    df_alt = _verify_minute_continuity(df_alt, symbol, backfill=backfill)
                                    if _frame_has_rows(df_alt):
                                        primary_frame_acquired = True
                                        mark_success(symbol, "1Min")
                                        _EMPTY_BAR_COUNTS.pop(tf_key, None)
                                        _SKIPPED_SYMBOLS.discard(tf_key)
                                        last_empty_error = None
                                        return df_alt
                                    df = df_alt
                                    if df_alt is not None:
                                        last_empty_error = None
                                else:
                                    df = df_alt
                                    if (
                                        df_alt is not None
                                        and getattr(df_alt, "empty", True)
                                        and not switch_recorded
                                    ):
                                        _record_feed_switch(symbol, "1Min", current_feed, alt_feed)
                                        switch_recorded = True
                                    if df_alt is not None:
                                        last_empty_error = None
                                        return df_alt
                        if end_dt - start_dt > _dt.timedelta(days=1):
                            short_start = end_dt - _dt.timedelta(days=1)
                            logger.debug(
                                "ALPACA_SHORT_WINDOW_RETRY",
                                extra={
                                    "symbol": symbol,
                                    "timeframe": "1Min",
                                    "start": short_start.isoformat(),
                                    "end": end_dt.isoformat(),
                                    "feed": normalized_feed or _DEFAULT_FEED,
                                },
                            )
                            try:
                                df_short = _fetch_bars(
                                    symbol,
                                    short_start,
                                    end_dt,
                                    "1Min",
                                    feed=normalized_feed or _DEFAULT_FEED,
                                )
                            except (EmptyBarsError, ValueError, RuntimeError):
                                df_short = None
                            else:
                                if df_short is not None and not getattr(df_short, "empty", True):
                                    logger.info(
                                        "ALPACA_SHORT_WINDOW_SUCCESS",
                                        extra={
                                            "symbol": symbol,
                                            "timeframe": "1Min",
                                            "feed": normalized_feed or _DEFAULT_FEED,
                                            "start": short_start.isoformat(),
                                            "end": end_dt.isoformat(),
                                        },
                                    )
                                    _IEX_EMPTY_COUNTS.pop(tf_key, None)
                                    df_short = _post_process(df_short, symbol=symbol, timeframe="1Min")
                                    df_short = _verify_minute_continuity(df_short, symbol, backfill=backfill)
                                    if _frame_has_rows(df_short):
                                        primary_frame_acquired = True
                                        mark_success(symbol, "1Min")
                                        return df_short
                                    df = df_short
                        if not primary_frame_acquired:
                            df = None
                    else:
                        logger.debug(
                            "ALPACA_EMPTY_BARS",
                            extra={
                                "symbol": symbol,
                                "timeframe": "1Min",
                                "feed": normalized_feed or _DEFAULT_FEED,
                                "occurrences": cnt,
                            },
                        )
                        if not primary_frame_acquired:
                            df = None
                else:
                    if isinstance(e, ValueError) and "invalid_time_window" in str(e):
                        _log_fetch_minute_empty(provider_feed_label, "invalid_time_window", str(e), symbol=symbol)
                        last_empty_error = EmptyBarsError(
                            f"empty_bars: symbol={symbol}, timeframe=1Min, reason=invalid_time_window"
                        )
                        last_empty_error.__cause__ = e  # type: ignore[attr-defined]
                    else:
                        logger.warning("ALPACA_FETCH_FAILED", extra={"symbol": symbol, "err": str(e)})
                        primary_failure_logged = True
                    if not primary_frame_acquired:
                        df = None
        else:
            _warn_missing_alpaca(symbol, "1Min")
            df = None
    
        if (
            df is not None
            and feed is None
            and feed_to_use != initial_feed
            and not switch_recorded
            and not used_backup
        ):
            existing_override = _FEED_OVERRIDE_BY_TF.get(tf_key)
            normalized_override: str | None = None
            if existing_override is not None:
                try:
                    normalized_override = _normalize_feed_value(existing_override)
                except ValueError:
                    normalized_override = str(existing_override).strip().lower() or None
            if normalized_override != feed_to_use:
                _record_feed_switch(symbol, "1Min", initial_feed, feed_to_use)
                switch_recorded = True
        if (not primary_frame_acquired) and (df is None or getattr(df, "empty", True)):
            if fallback_frame is not None and not getattr(fallback_frame, "empty", True):
                df = fallback_frame
            elif use_finnhub:
                finnhub_df = None
                try:
                    finnhub_df = _finnhub_get_bars(symbol, start_dt, end_dt, "1m")
                except (FinnhubAPIException, ValueError, NotImplementedError) as e:
                    logger.debug("FINNHUB_FETCH_FAILED", extra={"symbol": symbol, "err": str(e)})
                else:
                    logger.debug(
                        "FINNHUB_FETCH_SUCCESS",
                        extra={
                            "symbol": symbol,
                            "rows": getattr(finnhub_df, "shape", (0,))[0] if finnhub_df is not None else 0,
                        },
                    )
                    finnhub_df = _annotate_df_source(
                        finnhub_df,
                        provider="finnhub",
                        feed="finnhub",
                    )
                df = finnhub_df
                used_backup = True
                if df is not None and not getattr(df, "empty", True):
                    fallback_frame = df
            elif not enable_finnhub:
                finnhub_disabled_requested = True
                warn_finnhub_disabled_no_data(
                    symbol,
                    timeframe="1Min",
                    start=start_dt,
                    end=end_dt,
                )
            else:
                log_finnhub_disabled(symbol)
        if (not primary_frame_acquired) and (df is None or getattr(df, "empty", True)):
            max_span = _dt.timedelta(days=7)
            total_span = end_dt - start_dt
            if total_span > max_span:
                logger.warning(
                    "YF_1M_RANGE_SPLIT",
                    extra={
                        "symbol": symbol,
                        "interval": "1m",
                        "start": start_dt.isoformat(),
                        "end": end_dt.isoformat(),
                        "max_days": 7,
                    },
                )
                _emit_capture_record(
                    "YF_1M_RANGE_SPLIT",
                    extra={
                        "symbol": symbol,
                        "interval": "1m",
                        "start": start_dt.isoformat(),
                        "end": end_dt.isoformat(),
                        "max_days": 7,
                    },
                )
                logger.warning("YF_1", extra={"symbol": symbol, "interval": "1m"})
                _emit_capture_record("YF_1", extra={"symbol": symbol, "interval": "1m"})
                dfs: list[pd.DataFrame] = []  # type: ignore[var-annotated]
                cur_start = start_dt
                while cur_start < end_dt:
                    cur_end = min(cur_start + max_span, end_dt)
                    dfs.append(_minute_backup_get_bars(symbol, cur_start, cur_end, interval="1m"))
                    used_backup = True
                    _register_backup_skip()
                    cur_start = cur_end
                if pd is not None and dfs:
                    df = pd.concat(dfs, ignore_index=True)
                    first_attrs = getattr(dfs[0], "attrs", {}) if dfs else {}
                    provider_attr = first_attrs.get("data_provider") or first_attrs.get("fallback_provider")
                    feed_attr = first_attrs.get("data_feed") or first_attrs.get("fallback_feed")
                    if "timestamp" in df.columns:
                        try:
                            df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, errors="coerce")
                        except Exception:
                            pass
                        else:
                            df = df.sort_values("timestamp")
                            df = df.drop_duplicates(subset="timestamp", keep="last")
                            df = df.reset_index(drop=True)
                    if provider_attr:
                        df = _annotate_df_source(
                            df,
                            provider=str(provider_attr),
                            feed=str(feed_attr) if feed_attr else None,
                        )
                    if df is not None and not getattr(df, "empty", True):
                        fallback_frame = df
                elif dfs:
                    df = dfs[0]
                    if df is not None and not getattr(df, "empty", True):
                        fallback_frame = df
                else:
                    df = pd.DataFrame() if pd is not None else []  # type: ignore[assignment]
            else:
                df = _minute_backup_get_bars(symbol, start_dt, end_dt, interval="1m")
                used_backup = True
                _register_backup_skip()
                if df is not None and not getattr(df, "empty", True):
                    fallback_frame = df
    
        if used_backup and df is not None and not getattr(df, "empty", True):
            processed_df = _post_process(df, symbol=symbol, timeframe="1Min")
            candidate_df = df
            if processed_df is not None and not getattr(processed_df, "empty", True):
                candidate_df = processed_df
            fallback_frame = candidate_df
            if candidate_df is not None and not getattr(candidate_df, "empty", True):
                df = candidate_df
                if not fallback_logged:
                    _record_minute_fallback(frame=df)
                    fallback_logged = True
                try:
                    attrs = getattr(df, "attrs", None)
                except Exception:
                    attrs = None
                if isinstance(attrs, dict):
                    coverage_meta = attrs.get("_coverage_meta")
                    if isinstance(coverage_meta, dict):
                        attrs["_coverage_meta"] = dict(coverage_meta)
                _set_price_reliability(df, reliable=True)
                mark_success(symbol, "1Min")
                success_marked = True
                _EMPTY_BAR_COUNTS.pop(tf_key, None)
                _register_backup_skip()
                if fallback_allowed_flag:
                    _SKIPPED_SYMBOLS.add(tf_key)
        attempt_count_snapshot = max(
            attempt_count_snapshot, _EMPTY_BAR_COUNTS.get(tf_key, attempt_count_snapshot)
        )
        if not backoff_applied and attempt_count_snapshot >= _EMPTY_BAR_THRESHOLD:
            backoff_delay = min(2 ** (attempt_count_snapshot - _EMPTY_BAR_THRESHOLD), 60)
            ctx = {
                "symbol": symbol,
                "timeframe": "1Min",
                "occurrences": attempt_count_snapshot,
                "backoff": backoff_delay,
                "finnhub_enabled": use_finnhub,
                "feed": normalized_feed or _DEFAULT_FEED,
            }
            _log_with_capture(logging.WARNING, "ALPACA_EMPTY_BAR_BACKOFF", extra=ctx)
            time.sleep(backoff_delay)
            backoff_applied = True
        if allow_empty_override:
            backup_attempted = False
        allow_empty_return = allow_empty_override or (not window_has_session)
        try:
            if pd is not None and isinstance(df, pd.DataFrame) and (not df.empty):
                if isinstance(df.index, pd.DatetimeIndex) and len(df.index) > 0:
                    last_ts = int(pd.Timestamp(df.index[-1]).tz_convert("UTC").timestamp())
                elif "timestamp" in df.columns:
                    last_ts = int(pd.Timestamp(df["timestamp"].iloc[-1]).tz_convert("UTC").timestamp())
                else:
                    last_ts = None
                if last_ts is not None:
                    set_cached_minute_timestamp(symbol, last_ts)
        except (ValueError, TypeError, KeyError, AttributeError):
            pass
        if finnhub_disabled_requested and (df is None or getattr(df, "empty", True)):
            warn_finnhub_disabled_no_data(
                symbol,
                timeframe="1Min",
                start=start_dt,
                end=end_dt,
            )
            dedupe_key = ":".join(
                [
                    symbol,
                    "1Min",
                    f"{start_dt.isoformat()}->{end_dt.isoformat()}",
                ]
            )
            if dedupe_key not in _FINNHUB_CAPTURE_KEYS:
                _FINNHUB_CAPTURE_KEYS.add(dedupe_key)
                handler = _find_pytest_capture_handler()
                root_handlers = getattr(logging.getLogger(), "handlers", None)
                if not (isinstance(root_handlers, list) and handler in root_handlers):
                    _emit_capture_record(
                        "FINNHUB_DISABLED_NO_DATA",
                        level=logging.INFO,
                        extra={
                            "symbol": symbol,
                            "timeframe": "1Min",
                            "start": start_dt.isoformat(),
                            "end": end_dt.isoformat(),
                        },
                    )
        original_df = df
        # Debug hooks intentionally removed after validation
        if original_df is None:
            if allow_empty_return and not backup_attempted:
                _IEX_EMPTY_COUNTS.pop(tf_key, None)
                _SKIPPED_SYMBOLS.discard(tf_key)
                _EMPTY_BAR_COUNTS.pop(tf_key, None)
                return pd.DataFrame() if pd is not None else []  # type: ignore[return-value]
            if last_empty_error is not None:
                raise last_empty_error
            raise EmptyBarsError(f"empty_bars: symbol={symbol}, timeframe=1Min")
        if getattr(original_df, "empty", False):
            if allow_empty_return and not backup_attempted:
                _IEX_EMPTY_COUNTS.pop(tf_key, None)
                _SKIPPED_SYMBOLS.discard(tf_key)
                return original_df
            if last_empty_error is not None:
                raise last_empty_error
>           raise EmptyBarsError(f"empty_bars: symbol={symbol}, timeframe=1Min")
E           ai_trading.data.fetch.EmptyBarsError: empty_bars: symbol=AAPL, timeframe=1Min

ai_trading/data/fetch/__init__.py:10109: EmptyBarsError
____________________ test_order_idempotency_cache_fallback _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d486b10>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d484e60>

    def test_order_idempotency_cache_fallback(monkeypatch, caplog):
        caplog.set_level(logging.WARNING)
        module = _load_module_without_cachetools(monkeypatch)
    
        assert any("cachetools not available" in record.message for record in caplog.records), (
            "Fallback import should emit a diagnostic message"
        )
    
        cache = module.OrderIdempotencyCache(ttl_seconds=0.1, max_size=10)
        key = cache.generate_key("AAPL", "buy", 1)
    
        assert cache.is_duplicate(key) is False
        cache.mark_submitted(key, "OID-1")
        assert cache.is_duplicate(key) is True
        assert cache.get_existing_order(key)["order_id"] == "OID-1"
    
        time.sleep(0.15)
>       assert cache.get_existing_order(key) is None
E       AssertionError: assert {'key': IdempotencyKey(symbol='AAPL', side=<OrderSide.BUY: 'buy'>, quantity=1, intent_bucket=29360104), 'order_id': 'OID-1', 'submitted_at': datetime.datetime(2025, 10, 27, 23, 4, 40, 547453, tzinfo=datetime.timezone.utc)} is None
E        +  where {'key': IdempotencyKey(symbol='AAPL', side=<OrderSide.BUY: 'buy'>, quantity=1, intent_bucket=29360104), 'order_id': 'OID-1', 'submitted_at': datetime.datetime(2025, 10, 27, 23, 4, 40, 547453, tzinfo=datetime.timezone.utc)} = <bound method OrderIdempotencyCache.get_existing_order of <ai_trading.execution.idempotency_fallback.OrderIdempotencyCache object at 0x7c1d7d454aa0>>(IdempotencyKey(symbol='AAPL', side=<OrderSide.BUY: 'buy'>, quantity=1, intent_bucket=29360104))
E        +    where <bound method OrderIdempotencyCache.get_existing_order of <ai_trading.execution.idempotency_fallback.OrderIdempotencyCache object at 0x7c1d7d454aa0>> = <ai_trading.execution.idempotency_fallback.OrderIdempotencyCache object at 0x7c1d7d454aa0>.get_existing_order

tests/test_idempotency_cache_fallback.py:51: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:40Z", "level": "WARNING", "name": "ai_trading.execution.idempotency_fallback", "msg": "cachetools not available; using fallback TTL cache implementation (degraded idempotency cache performance)", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.execution.idempotency_fallback:idempotency.py:20 cachetools not available; using fallback TTL cache implementation (degraded idempotency cache performance)
________________ test_iex_empty_switches_to_sip[preferred_list] ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d4b32f0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d4b3710>
preferred_feeds = ('sip',), provider_order = ['alpaca_iex', 'alpaca_sip']

    @pytest.mark.parametrize(
        ("preferred_feeds", "provider_order"),
        [
            (("sip",), ["alpaca_iex", "alpaca_sip"]),
            ((), ["alpaca_iex"]),
        ],
        ids=["preferred_list", "no_preference"],
    )
    def test_iex_empty_switches_to_sip(monkeypatch, caplog, preferred_feeds, provider_order):
        symbol = "AAPL"
        start = datetime(2024, 1, 1, tzinfo=ZoneInfo("UTC"))
        end = start + timedelta(days=1)
    
        _reset_feed_state()
        fetch._IEX_EMPTY_COUNTS[(symbol, "1Min")] = fetch._IEX_EMPTY_THRESHOLD + 1
        monkeypatch.setenv("TESTING", "1")
        monkeypatch.setenv("ALPACA_API_KEY", "dummy")
        monkeypatch.setenv("ALPACA_SECRET_KEY", "dummy")
        monkeypatch.setattr(fetch, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: False)
        monkeypatch.setattr(fetch, "is_market_open", lambda: False)
        monkeypatch.setattr(fetch, "alpaca_feed_failover", lambda: preferred_feeds)
        monkeypatch.setattr(fetch, "provider_priority", lambda: provider_order)
    
        feeds = []
    
        def fake_get(url, params=None, headers=None, timeout=None):
            feeds.append(params.get("feed"))
            if params.get("feed") == "iex":
                data = {"bars": []}
            else:
                data = {"bars": [{"t": "2024-01-01T00:00:00Z", "o": 1, "h": 1, "l": 1, "c": 1, "v": 1}]}
            return types.SimpleNamespace(
                status_code=200,
                text=json.dumps(data),
                headers={"Content-Type": "application/json"},
                json=lambda: data,
            )
    
        monkeypatch.setattr(fetch.requests, "get", fake_get)
        monkeypatch.setattr(http.requests, "get", fake_get, raising=False)
        monkeypatch.setattr(fetch, "_HTTP_SESSION", types.SimpleNamespace(get=fake_get), raising=False)
    
        monkeypatch.setattr(fetch, "_sip_fallback_allowed", lambda *a, **k: True)
    
        before = inc_provider_fallback("alpaca_iex", "alpaca_sip")
        with caplog.at_level("INFO"):
            df = fetch._fetch_bars(symbol, start, end, "1Min", feed="iex")
        after = inc_provider_fallback("alpaca_iex", "alpaca_sip")
    
>       assert feeds[:2] == ["iex", "sip"]
E       AssertionError: assert [] == ['iex', 'sip']
E         Right contains 2 more items, first extra item: 'iex'
E         Use -v to get more diff

tests/test_iex_sip_fallback.py:76: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
________________ test_iex_empty_switches_to_sip[no_preference] _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d4b31a0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d75958650>
preferred_feeds = (), provider_order = ['alpaca_iex']

    @pytest.mark.parametrize(
        ("preferred_feeds", "provider_order"),
        [
            (("sip",), ["alpaca_iex", "alpaca_sip"]),
            ((), ["alpaca_iex"]),
        ],
        ids=["preferred_list", "no_preference"],
    )
    def test_iex_empty_switches_to_sip(monkeypatch, caplog, preferred_feeds, provider_order):
        symbol = "AAPL"
        start = datetime(2024, 1, 1, tzinfo=ZoneInfo("UTC"))
        end = start + timedelta(days=1)
    
        _reset_feed_state()
        fetch._IEX_EMPTY_COUNTS[(symbol, "1Min")] = fetch._IEX_EMPTY_THRESHOLD + 1
        monkeypatch.setenv("TESTING", "1")
        monkeypatch.setenv("ALPACA_API_KEY", "dummy")
        monkeypatch.setenv("ALPACA_SECRET_KEY", "dummy")
        monkeypatch.setattr(fetch, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(fetch, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda *a, **k: True)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda *a, **k: False)
        monkeypatch.setattr(fetch, "is_market_open", lambda: False)
        monkeypatch.setattr(fetch, "alpaca_feed_failover", lambda: preferred_feeds)
        monkeypatch.setattr(fetch, "provider_priority", lambda: provider_order)
    
        feeds = []
    
        def fake_get(url, params=None, headers=None, timeout=None):
            feeds.append(params.get("feed"))
            if params.get("feed") == "iex":
                data = {"bars": []}
            else:
                data = {"bars": [{"t": "2024-01-01T00:00:00Z", "o": 1, "h": 1, "l": 1, "c": 1, "v": 1}]}
            return types.SimpleNamespace(
                status_code=200,
                text=json.dumps(data),
                headers={"Content-Type": "application/json"},
                json=lambda: data,
            )
    
        monkeypatch.setattr(fetch.requests, "get", fake_get)
        monkeypatch.setattr(http.requests, "get", fake_get, raising=False)
        monkeypatch.setattr(fetch, "_HTTP_SESSION", types.SimpleNamespace(get=fake_get), raising=False)
    
        monkeypatch.setattr(fetch, "_sip_fallback_allowed", lambda *a, **k: True)
    
        before = inc_provider_fallback("alpaca_iex", "alpaca_sip")
        with caplog.at_level("INFO"):
            df = fetch._fetch_bars(symbol, start, end, "1Min", feed="iex")
        after = inc_provider_fallback("alpaca_iex", "alpaca_sip")
    
>       assert feeds[:2] == ["iex", "sip"]
E       AssertionError: assert [] == ['iex', 'sip']
E         Right contains 2 more items, first extra item: 'iex'
E         Use -v to get more diff

tests/test_iex_sip_fallback.py:76: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:5092 BACKUP_PROVIDER_EMPTY
___________________ test_module_imports_without_heavy_stacks ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d4b2b70>

    def test_module_imports_without_heavy_stacks(monkeypatch):
        heavy_roots = {"torch", "gymnasium", "pandas", "pyarrow", "sklearn", "matplotlib"}
        before = set(sys.modules)
        # Running the module main should not pull heavy deps implicitly
        monkeypatch.setattr(sys, "argv", ["ai_trading"])
        monkeypatch.setenv("PYTEST_RUNNING", "1")
>       runpy.run_module("ai_trading", run_name="__main__")

tests/test_import_side_effects.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<frozen runpy>:222: in run_module
    ???
<frozen runpy>:148: in _get_module_details
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mod_name = 'ai_trading.__main__', error = <class 'ImportError'>

>   ???
E   RuntimeWarning: 'ai_trading.__main__' found in sys.modules after import of package 'ai_trading', but prior to execution of 'ai_trading.__main__'; this may result in unpredictable behaviour

<frozen runpy>:128: RuntimeWarning
___________________ test_run_module_emits_no_runtime_warning ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d457680>

    def test_run_module_emits_no_runtime_warning(monkeypatch):
        monkeypatch.setattr(sys, "argv", ["ai_trading"])
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        with warnings.catch_warnings(record=True) as caught:
            warnings.simplefilter("always", RuntimeWarning)
            runpy.run_module("ai_trading", run_name="__main__")
        runtime_warnings = [w for w in caught if issubclass(w.category, RuntimeWarning)]
>       assert not runtime_warnings, f"RuntimeWarning emitted: {runtime_warnings}"
E       AssertionError: RuntimeWarning emitted: [<warnings.WarningMessage object at 0x7c1d7d457a70>]
E       assert not [<warnings.WarningMessage object at 0x7c1d7d457a70>]

tests/test_import_side_effects.py:25: AssertionError
____________________________ test_submodules_import ____________________________

    def test_submodules_import() -> None:
        pkg = importlib.import_module("ai_trading")
        for modinfo in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + "."):
>           _safe_import(modinfo.name)

tests/test_imports_smoke.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_imports_smoke.py:16: in _safe_import
    importlib.import_module(name)
/usr/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ImportError: cannot import name 'Blueprint' from 'flask' (unknown location)

ai_trading/diagnostics/http_diag.py:6: ImportError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:42Z", "level": "INFO", "name": "ai_trading.logging", "msg": "ParameterValidator initialized with institutional safety bounds", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.logging:parameter_validator.py:23 ParameterValidator initialized with institutional safety bounds
_____________________ test_calculate_entry_size_zero_price _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7195da90>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7195e360>

    def test_calculate_entry_size_zero_price(monkeypatch, caplog):
        """Ensure calculate_entry_size skips symbols with invalid pricing data."""
    
        from ai_trading.core.bot_engine import calculate_entry_size
    
        monkeypatch.setenv("PYTEST_RUNNING", "1")
    
        class _DummySeries:
            def __init__(self, values):
                self._values = list(values)
    
            def pct_change(self, fill_method=None):  # noqa: D401 - minimal stub
                changes = []
                for idx in range(1, len(self._values)):
                    prev = self._values[idx - 1]
                    curr = self._values[idx]
                    changes.append(((curr - prev) / prev) if prev else 0.0)
                return _DummySeries(changes or [0.0])
    
            def dropna(self):  # noqa: D401 - minimal stub
                return self
    
            @property
            def values(self):  # noqa: D401 - minimal stub
                return self._values
    
        class _DummyDataFrame:
            empty = False
    
            def __init__(self, close_values):
                self._close = _DummySeries(close_values)
    
            def __getitem__(self, key):  # noqa: D401 - minimal stub
                if key != "close":
                    raise KeyError(key)
                return self._close
    
        daily_df = _DummyDataFrame([100.0, 101.0, 102.0])
    
        class _DummyAPI:
            def get_account(self):
                return SimpleNamespace(cash="10000")
    
        class _DummyFetcher:
            def get_daily_df(self, *_args, **_kwargs):
                return daily_df
    
        class _DummyQuote:
            ask_price = 100.5
            bid_price = 100.0
    
        class _DummyDataClient:
            def get_stock_latest_quote(self, *_args, **_kwargs):
                return _DummyQuote()
    
        ctx = SimpleNamespace(
            api=_DummyAPI(),
            data_fetcher=_DummyFetcher(),
            data_client=_DummyDataClient(),
            volume_threshold=100_000,
            params={},
            max_position_dollars=10_000.0,
        )
    
        caplog.set_level("INFO")
        size = calculate_entry_size(ctx, "TEST", price=0.0, atr=1.0, win_prob=0.6)
    
        assert size == 0
        assert any("SKIP_INVALID_PRICE" in rec.message for rec in caplog.records)
    
        caplog.clear()
    
        monkeypatch.setattr(
            "ai_trading.core.bot_engine.fractional_kelly_size",
            lambda *args, **kwargs: 0,
        )
    
        size = calculate_entry_size(ctx, "TEST", price=100.0, atr=1.0, win_prob=0.6)
    
>       assert size == 0
E       assert 21 == 0

tests/test_kelly_confidence_fix.py:211: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:42Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "SKIP_INVALID_PRICE", "symbol": "TEST", "price": 0.0, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:15754 SKIP_INVALID_PRICE
____________ test_module_import_without_pandas[ai_trading.metrics] _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759a5df0>
module = 'ai_trading.metrics'

    @pytest.mark.parametrize(
        "module",
        [
            "ai_trading.features.pipeline",
            "ai_trading.portfolio.sizing",
            "ai_trading.monitoring.metrics",
            "ai_trading.metrics",
        ],
    )
    def test_module_import_without_pandas(monkeypatch, module):
        """Modules should import even if pandas is unavailable."""
        pytest.importorskip("pandas")
        if module == "ai_trading.features.pipeline":
            pytest.importorskip("sklearn")
        for key in ["ALPACA_API_KEY", "ALPACA_SECRET_KEY", "ALPACA_BASE_URL", "WEBHOOK_SECRET"]:
            monkeypatch.setenv(key, "test")
        monkeypatch.delitem(sys.modules, "pandas", raising=False)
        real_import = builtins.__import__
    
        def fake_import(name, *args, **kwargs):
            if name.startswith("pandas"):
                raise ModuleNotFoundError("No module named 'pandas'")
            try:
                return real_import(name, *args, **kwargs)
            except ModuleNotFoundError:
                return types.ModuleType(name)
    
        monkeypatch.setattr(builtins, "__import__", fake_import)
        monkeypatch.delitem(sys.modules, module, raising=False)
>       importlib.import_module(module)

tests/test_lazy_pandas_import.py:36: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
ai_trading/metrics/__init__.py:82: in <module>
    REGISTRY = _ensure_names_map(REGISTRY)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

registry = <object object at 0x7c1d80deaa40>

    def _ensure_names_map(registry):
        """Ensure ``registry`` exposes a ``_names_to_collectors`` mapping."""
    
        if registry is None:
            return None
        if not hasattr(registry, "_names_to_collectors"):
>           setattr(registry, "_names_to_collectors", {})
E           AttributeError: 'object' object has no attribute '_names_to_collectors'

ai_trading/metrics/__init__.py:78: AttributeError
___________________ test_throttle_summaries_flush_each_cycle ___________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d3df770>

    @pytest.mark.unit
    def test_throttle_summaries_flush_each_cycle(caplog):
        flush = getattr(log_mod, "flush_log_throttle_summaries", None)
        throttle_filter = getattr(log_mod, "_THROTTLE_FILTER", None)
        if flush is None or throttle_filter is None:
            pytest.skip("Log throttle helpers unavailable")
    
        caplog.set_level(logging.INFO)
        with throttle_filter._lock:  # type: ignore[attr-defined]
            throttle_filter._state.clear()  # type: ignore[attr-defined]
    
        logger = log_mod.get_logger("ai_trading.tests.deduper")
    
        for _ in range(4):
            logger.info("PROVIDER_SPAM", extra={"provider": "feed-a"})
    
        with throttle_filter._lock:  # type: ignore[attr-defined]
            first_cycle_suppressed = int(
                throttle_filter._state.get("PROVIDER_SPAM", {}).get("suppressed", 0)  # type: ignore[attr-defined]
            )
        assert first_cycle_suppressed >= 3
    
        flush()
    
        messages = [rec.getMessage() for rec in caplog.records]
        summaries = [msg for msg in messages if msg.startswith("LOG_THROTTLE_SUMMARY")]
>       assert summaries
E       assert []

tests/test_log_deduper_provider_spam.py:36: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:46Z", "level": "INFO", "name": "ai_trading.tests.deduper", "msg": "PROVIDER_SPAM", "provider": "feed-a", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.tests.deduper:test_log_deduper_provider_spam.py:24 PROVIDER_SPAM
INFO     ai_trading.tests.deduper:test_log_deduper_provider_spam.py:24 PROVIDER_SPAM
INFO     ai_trading.tests.deduper:test_log_deduper_provider_spam.py:24 PROVIDER_SPAM
INFO     ai_trading.tests.deduper:test_log_deduper_provider_spam.py:24 PROVIDER_SPAM
______________________ test_provider_dedupe_emits_summary ______________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d3deab0>

    @pytest.mark.unit
    def test_provider_dedupe_emits_summary(caplog):
        deduper = getattr(log_mod, "provider_log_deduper", None)
        record_suppressed = getattr(log_mod, "record_provider_log_suppressed", None)
        reset = getattr(log_mod, "reset_provider_log_dedupe", None)
        flush = getattr(log_mod, "flush_log_throttle_summaries", None)
        if None in {deduper, record_suppressed, reset, flush}:
            pytest.skip("Provider dedupe helpers unavailable")
    
        reset()
        caplog.set_level(logging.INFO)
        logger = log_mod.get_logger("ai_trading.tests.provider")
    
        raw_ttl = getattr(require("ai_trading.config.settings").get_settings(), "logging_dedupe_ttl_s", 0)
        ttl = max(1, int(raw_ttl))
        key = "DATA_PROVIDER_SWITCHOVER:primary->backup"
    
        assert deduper.should_log(key, ttl)
        logger.info("DATA_PROVIDER_SWITCHOVER", extra={"from_provider": "primary", "to_provider": "backup"})
    
        assert not deduper.should_log(key, ttl)
        record_suppressed("DATA_PROVIDER_SWITCHOVER")
    
        flush()
    
        def _summary_count(record) -> tuple[str, int]:
            message = record.getMessage()
            count = None
            for token in message.split():
                if token.startswith("suppressed="):
                    try:
                        count = int(token.split("=", 1)[1])
                    except ValueError:  # pragma: no cover - defensive parsing
                        count = None
            return message, count if count is not None else -1
    
        def _provider_summaries() -> list[logging.LogRecord]:
            return [
                rec
                for rec in caplog.records
                if rec.getMessage().startswith("LOG_THROTTLE_SUMMARY")
                and 'key="DATA_PROVIDER_SWITCHOVER"' in rec.getMessage()
            ]
    
        assert not _provider_summaries(), "Unexpected summary for single suppressed event"
    
        caplog.clear()
    
        # Second flush cycle should keep prior suppressions without emitting yet.
        record_suppressed("DATA_PROVIDER_SWITCHOVER")
        flush()
        assert not _provider_summaries(), "Summary emitted before threshold reached"
    
        caplog.clear()
    
        # Third cycle accumulates to threshold across flushes and should emit once.
        record_suppressed("DATA_PROVIDER_SWITCHOVER")
        flush()
        summaries = _provider_summaries()
>       assert summaries, "Expected summary once suppression threshold reached"
E       AssertionError: Expected summary once suppression threshold reached
E       assert []

tests/test_log_deduper_provider_spam.py:131: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:46Z", "level": "INFO", "name": "ai_trading.tests.provider", "msg": "DATA_PROVIDER_SWITCHOVER", "from_provider": "primary", "to_provider": "backup", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.tests.provider:test_log_deduper_provider_spam.py:90 DATA_PROVIDER_SWITCHOVER
__________________________ test_health_rows_throttle ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759bd940>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71950260>

    def test_health_rows_throttle(monkeypatch, caplog):
        caplog.set_level("DEBUG")
        utils._last_health_log = 0.0
        t = [0.0]
        monkeypatch.setattr(time, "monotonic", lambda: t[0])
>       utils.health_rows_passed([1])

tests/test_logging_behavior.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'health_rows_passed'

    def __getattr__(name: str) -> Any:  # AI-AGENT-REF: importlib-based lazy loader
        target = _LAZY_MAP.get(name)
        if not target:
>           raise AttributeError(f"module {__name__} has no attribute {name}")
E           AttributeError: module ai_trading.utils has no attribute health_rows_passed

ai_trading/utils/__init__.py:110: AttributeError
___________________________ test_no_secrets_in_logs ____________________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7187b0b0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71878380>

    def test_no_secrets_in_logs(caplog, monkeypatch):
        caplog.set_level(logging.INFO)
        monkeypatch.setenv("NEWS_API_KEY", "TOPSECRETKEY")
        logger = get_logger(__name__)
        logger.info("boot with key=%s", os.getenv("NEWS_API_KEY"))
        joined = "\n".join(m.message for m in caplog.records)
>       assert "TOPSECRETKEY" not in joined
E       AssertionError: assert 'TOPSECRETKEY' not in 'boot with key=TOPSECRETKEY'
E         'TOPSECRETKEY' is contained here:
E           boot with key=TOPSECRETKEY

tests/test_logging_scrubbed.py:13: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:04:46Z", "level": "INFO", "name": "tests.test_logging_scrubbed", "msg": "boot with key=***", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     tests.test_logging_scrubbed:test_logging_scrubbed.py:11 boot with key=TOPSECRETKEY
___________________ test_main_exits_when_alpaca_sdk_missing ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7187bf50>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7185a8a0>

>   ???
E   assert 98 == 1
E    +  where 98 = SystemExit(98).code
E    +    where SystemExit(98) = <ExceptionInfo SystemExit(98) tblen=2>.value

tests/test_main_alpaca_py_required.py:12: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:10Z", "level": "ERROR", "name": "ai_trading.main", "msg": "Failed to start API", "bot_phase": "GENERAL", "exc": "ai_trading.main.PortInUseError: port 9001 in use by pid 1044150"}
{"ts": "2025-10-27T23:05:10Z", "level": "CRITICAL", "name": "ai_trading.main", "msg": "API_PORT_CONFLICT_FATAL", "port": 9001, "pid": 1044150, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.main:main.py:1301 Failed to start API
Traceback (most recent call last):
  File "/root/ai-trading-bot-1/ai_trading/main.py", line 1212, in start_api
    _bind_probe(port)
  File "/root/ai-trading-bot-1/ai_trading/main.py", line 599, in _bind_probe
    probe.bind(("0.0.0.0", port))
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/ai-trading-bot-1/ai_trading/main.py", line 1299, in start_api_with_signal
    start_api(api_ready)
  File "/root/ai-trading-bot-1/ai_trading/main.py", line 1227, in start_api
    raise PortInUseError(port, pid) from exc
ai_trading.main.PortInUseError: port 9001 in use by pid 1044150
CRITICAL ai_trading.main:main.py:1549 API_PORT_CONFLICT_FATAL
______________________________ test_run_flask_app ______________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d718b4110>

    def test_run_flask_app(monkeypatch):
        """Flask app runs on provided port and forwards kwargs."""
        called = {}
    
        class App:
            def run(self, host, port, debug=False, **kwargs):
                called["args"] = (host, port)
                called["debug"] = debug
                called["kwargs"] = kwargs
    
        monkeypatch.setattr(app, "create_app", lambda: App())
>       main.run_flask_app(1234, debug=True, extra=1)

tests/test_main_extended2.py:95: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

port = 1234, ready_signal = None, run_kwargs = {'debug': True, 'extra': 1}
app = <module 'ai_trading.app' from '/root/ai-trading-bot-1/ai_trading/app.py'>
application = <tests.test_main_extended2.test_run_flask_app.<locals>.App object at 0x7c1d7187bc50>
_broker_snapshot = <function run_flask_app.<locals>._broker_snapshot at 0x7c1d71886e80>

    def run_flask_app(
        port: int = 5000,
        ready_signal: threading.Event | None = None,
        **run_kwargs,
    ) -> None:
        """Launch Flask API, retrying on sequential ports when necessary."""
    
        from ai_trading import app
    
        application = app.create_app()
    
        def _mask_identifier(value: str | None, keep: int = 4) -> str:
            if not value:
                return ""
            prefix = str(value)[: max(keep, 0)]
            if not prefix:
                return "***"
            return f"{prefix}***"
    
        def _broker_snapshot() -> dict[str, Any]:
            snapshot: dict[str, Any] = {"available": False}
            try:
                from ai_trading.core import bot_engine as _bot_engine
            except Exception:
                logger.debug("BROKER_DIAG_IMPORT_FAILED", exc_info=True)
                return snapshot
    
            client = getattr(_bot_engine, "trading_client", None)
            if client is None:
                return snapshot
    
            snapshot["available"] = True
    
            try:
                orders = _bot_engine.list_open_orders(client)
                if orders is None:
                    snapshot["open_orders"] = 0
                else:
                    orders_list = list(orders)
                    snapshot["open_orders"] = len(orders_list)
            except Exception:
                logger.debug("BROKER_DIAG_OPEN_ORDERS_FAILED", exc_info=True)
    
            try:
                list_positions = getattr(client, "list_positions", None)
                if callable(list_positions):
                    positions = list_positions()
                    if positions is None:
                        snapshot["positions"] = 0
                    else:
                        positions_list = list(positions)
                        snapshot["positions"] = len(positions_list)
            except Exception:
                logger.debug("BROKER_DIAG_POSITIONS_FAILED", exc_info=True)
    
            try:
                get_account = getattr(client, "get_account", None)
                if callable(get_account):
                    account = get_account()
                    if account is not None:
                        acct_payload: dict[str, Any] = {
                            "status": getattr(account, "status", None),
                            "trading_blocked": getattr(account, "trading_blocked", None),
                        }
                        identifier = getattr(account, "id", None) or getattr(
                            account, "account_number", None
                        )
                        if identifier:
                            acct_payload["id_masked"] = _mask_identifier(str(identifier))
                        snapshot["account"] = acct_payload
            except Exception:
                logger.debug("BROKER_DIAG_ACCOUNT_FAILED", exc_info=True)
    
            return snapshot
    
>       application.config.setdefault("broker_snapshot_fn", _broker_snapshot)
E       AttributeError: 'App' object has no attribute 'config'

ai_trading/main.py:1088: AttributeError
________________________ test_run_flask_app_port_in_use ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7185b6b0>

    def test_run_flask_app_port_in_use(monkeypatch):
        """OSError EADDRINUSE triggers retry on next port."""
        called = []
    
        class App:
            def run(self, host, port, debug=False, **kwargs):
                called.append(port)
                if len(called) == 1:
                    raise OSError(errno.EADDRINUSE, "address in use")
                raise SystemExit
    
        monkeypatch.setattr(app, "create_app", lambda: App())
        monkeypatch.setattr(main, "get_pid_on_port", lambda p: None)
        with pytest.raises(SystemExit):
>           main.run_flask_app(1234)

tests/test_main_extended2.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

port = 1234, ready_signal = None, run_kwargs = {}
app = <module 'ai_trading.app' from '/root/ai-trading-bot-1/ai_trading/app.py'>
application = <tests.test_main_extended2.test_run_flask_app_port_in_use.<locals>.App object at 0x7c1d718b41d0>
_broker_snapshot = <function run_flask_app.<locals>._broker_snapshot at 0x7c1d71890cc0>

    def run_flask_app(
        port: int = 5000,
        ready_signal: threading.Event | None = None,
        **run_kwargs,
    ) -> None:
        """Launch Flask API, retrying on sequential ports when necessary."""
    
        from ai_trading import app
    
        application = app.create_app()
    
        def _mask_identifier(value: str | None, keep: int = 4) -> str:
            if not value:
                return ""
            prefix = str(value)[: max(keep, 0)]
            if not prefix:
                return "***"
            return f"{prefix}***"
    
        def _broker_snapshot() -> dict[str, Any]:
            snapshot: dict[str, Any] = {"available": False}
            try:
                from ai_trading.core import bot_engine as _bot_engine
            except Exception:
                logger.debug("BROKER_DIAG_IMPORT_FAILED", exc_info=True)
                return snapshot
    
            client = getattr(_bot_engine, "trading_client", None)
            if client is None:
                return snapshot
    
            snapshot["available"] = True
    
            try:
                orders = _bot_engine.list_open_orders(client)
                if orders is None:
                    snapshot["open_orders"] = 0
                else:
                    orders_list = list(orders)
                    snapshot["open_orders"] = len(orders_list)
            except Exception:
                logger.debug("BROKER_DIAG_OPEN_ORDERS_FAILED", exc_info=True)
    
            try:
                list_positions = getattr(client, "list_positions", None)
                if callable(list_positions):
                    positions = list_positions()
                    if positions is None:
                        snapshot["positions"] = 0
                    else:
                        positions_list = list(positions)
                        snapshot["positions"] = len(positions_list)
            except Exception:
                logger.debug("BROKER_DIAG_POSITIONS_FAILED", exc_info=True)
    
            try:
                get_account = getattr(client, "get_account", None)
                if callable(get_account):
                    account = get_account()
                    if account is not None:
                        acct_payload: dict[str, Any] = {
                            "status": getattr(account, "status", None),
                            "trading_blocked": getattr(account, "trading_blocked", None),
                        }
                        identifier = getattr(account, "id", None) or getattr(
                            account, "account_number", None
                        )
                        if identifier:
                            acct_payload["id_masked"] = _mask_identifier(str(identifier))
                        snapshot["account"] = acct_payload
            except Exception:
                logger.debug("BROKER_DIAG_ACCOUNT_FAILED", exc_info=True)
    
            return snapshot
    
>       application.config.setdefault("broker_snapshot_fn", _broker_snapshot)
E       AttributeError: 'App' object has no attribute 'config'

ai_trading/main.py:1088: AttributeError
______________________ test_run_flask_app_skips_ipv6_port ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d718582f0>

    def test_run_flask_app_skips_ipv6_port(monkeypatch):
        """IPv6-bound port is skipped in favor of a free one."""
        s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
        s.bind(("::", 0))
        s.listen(1)
        port = s.getsockname()[1]
        called = []
    
        class App:
            def run(self, host, port, debug=False, **kwargs):
                called.append(port)
                raise SystemExit
    
        monkeypatch.setattr(app, "create_app", lambda: App())
        with pytest.raises(SystemExit):
>           main.run_flask_app(port)

tests/test_main_extended2.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

port = 58215, ready_signal = None, run_kwargs = {}
app = <module 'ai_trading.app' from '/root/ai-trading-bot-1/ai_trading/app.py'>
application = <tests.test_main_extended2.test_run_flask_app_skips_ipv6_port.<locals>.App object at 0x7c1d71858980>
_broker_snapshot = <function run_flask_app.<locals>._broker_snapshot at 0x7c1d71885e40>

    def run_flask_app(
        port: int = 5000,
        ready_signal: threading.Event | None = None,
        **run_kwargs,
    ) -> None:
        """Launch Flask API, retrying on sequential ports when necessary."""
    
        from ai_trading import app
    
        application = app.create_app()
    
        def _mask_identifier(value: str | None, keep: int = 4) -> str:
            if not value:
                return ""
            prefix = str(value)[: max(keep, 0)]
            if not prefix:
                return "***"
            return f"{prefix}***"
    
        def _broker_snapshot() -> dict[str, Any]:
            snapshot: dict[str, Any] = {"available": False}
            try:
                from ai_trading.core import bot_engine as _bot_engine
            except Exception:
                logger.debug("BROKER_DIAG_IMPORT_FAILED", exc_info=True)
                return snapshot
    
            client = getattr(_bot_engine, "trading_client", None)
            if client is None:
                return snapshot
    
            snapshot["available"] = True
    
            try:
                orders = _bot_engine.list_open_orders(client)
                if orders is None:
                    snapshot["open_orders"] = 0
                else:
                    orders_list = list(orders)
                    snapshot["open_orders"] = len(orders_list)
            except Exception:
                logger.debug("BROKER_DIAG_OPEN_ORDERS_FAILED", exc_info=True)
    
            try:
                list_positions = getattr(client, "list_positions", None)
                if callable(list_positions):
                    positions = list_positions()
                    if positions is None:
                        snapshot["positions"] = 0
                    else:
                        positions_list = list(positions)
                        snapshot["positions"] = len(positions_list)
            except Exception:
                logger.debug("BROKER_DIAG_POSITIONS_FAILED", exc_info=True)
    
            try:
                get_account = getattr(client, "get_account", None)
                if callable(get_account):
                    account = get_account()
                    if account is not None:
                        acct_payload: dict[str, Any] = {
                            "status": getattr(account, "status", None),
                            "trading_blocked": getattr(account, "trading_blocked", None),
                        }
                        identifier = getattr(account, "id", None) or getattr(
                            account, "account_number", None
                        )
                        if identifier:
                            acct_payload["id_masked"] = _mask_identifier(str(identifier))
                        snapshot["account"] = acct_payload
            except Exception:
                logger.debug("BROKER_DIAG_ACCOUNT_FAILED", exc_info=True)
    
            return snapshot
    
>       application.config.setdefault("broker_snapshot_fn", _broker_snapshot)
E       AttributeError: 'App' object has no attribute 'config'

ai_trading/main.py:1088: AttributeError
______________________ test_validate_environment_missing _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71859160>

    def test_validate_environment_missing(monkeypatch):
        """validate_environment errors when secret missing."""
        monkeypatch.setattr(main.config, 'WEBHOOK_SECRET', '', raising=False)
>       with pytest.raises(RuntimeError):
E       Failed: DID NOT RAISE <class 'RuntimeError'>

tests/test_main_extended2.py:161: Failed
------------------------------ Captured log call -------------------------------
INFO     ai_trading.main:main.py:911 RUNTIME_PATHS_READY
_____________________________ test_main_runs_once ______________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7185b170>

    def test_main_runs_once(monkeypatch):
        """main executes a single cycle when configured."""
        monkeypatch.setenv("SCHEDULER_ITERATIONS", "1")
        called = {}
    
        # AI-AGENT-REF: Fix lambda signature to accept ready_signal parameter and set it
        def mock_start_api(ready_signal=None):
            called.setdefault("api", True)
            if ready_signal:
                ready_signal.set()  # Important: signal that API is ready
    
        monkeypatch.setattr(main, "start_api", mock_start_api)
    
        def _cycle():
            called["cycle"] = called.get("cycle", 0) + 1
    
        monkeypatch.setattr(main, "run_cycle", _cycle)
        monkeypatch.setattr(main.time, "sleep", lambda s: None)
        main.main()
        assert called.get("api")
>       assert called.get("cycle") == 1
E       AssertionError: assert 2 == 1
E        +  where 2 = <built-in method get of dict object at 0x7c1d71aa8a80>('cycle')
E        +    where <built-in method get of dict object at 0x7c1d71aa8a80> = {'api': True, 'cycle': 2}.get

tests/test_main_extended2.py:185: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:10Z", "level": "WARNING", "name": "ai_trading.main", "msg": "ALPACA_PY_SKIPPED_UNDER_TEST", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "INFO", "name": "ai_trading.main", "msg": "ENV_CONFIG_LOADED", "dotenv_path": null, "ALPACA_DATA_FEED": "iex", "CAPITAL_CAP": 0.25, "ALPACA_API_URL": "https://paper-api.alpaca.markets", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "WARNING", "name": "ai_trading.main", "msg": "MARKET_CLOSED_SLEEP", "now": "2025-10-27T19:05:10.541885-04:00", "next_open": "2025-10-28T09:30:00-04:00", "sleep_s": 300, "sleep_original_s": 51889, "sleep_cap_s": 300, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "INFO", "name": "ai_trading.main", "msg": "DATA_CONFIG feed=iex adjustment=all timeframe=1Day/1Min provider=alpaca", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "WARNING", "name": "ai_trading.position_sizing", "msg": "ALPACA_HTTP_ERROR", "url": "https://paper-api.alpaca.markets/v2/account", "status": null, "reason": "http_error", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "WARNING", "name": "ai_trading.main", "msg": "ACCOUNT_EQUITY_MISSING", "equity": 0.0, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "INFO", "name": "ai_trading.main", "msg": "REQUESTS_POOL_STATS", "transport": "requests", "pool_maxsize": 32, "retries": 3, "backoff_factor": 0.3, "connect_timeout": 5.0, "read_timeout": 10.0, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "INFO", "name": "ai_trading.main", "msg": "STARTUP_BANNER", "mode": "balanced", "paper": true, "alpaca_base_url": "https://paper-api.alpaca.markets", "capital_cap": 0.25, "dollar_risk_limit": 0.05, "max_position_mode": "STATIC", "max_position_size": 8000.0, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:10Z", "level": "INFO", "name": "ai_trading.main", "msg": "Warm-up run_cycle completed", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:12Z", "level": "INFO", "name": "ai_trading.main", "msg": "Runtime defaults resolved", "iterations": 1, "interval": 60, "seed": 42, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:12Z", "level": "INFO", "name": "ai_trading.main", "msg": "HTTP_PROFILE_CLOSED", "retries": 1, "backoff_factor": 0.1, "connect_timeout": 5.0, "read_timeout": 10.0, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:12Z", "level": "INFO", "name": "ai_trading.main", "msg": "CYCLE_TIMING", "elapsed_ms": 0.08731894195079803, "within_budget": true, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:12Z", "level": "INFO", "name": "ai_trading.main", "msg": "SCHEDULER_COMPLETE", "iterations": 1, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.main:main.py:281 ALPACA_PY_SKIPPED_UNDER_TEST
INFO     ai_trading.main:main.py:740 ENV_CONFIG_LOADED
WARNING  ai_trading.main:main.py:1428 MARKET_CLOSED_SLEEP
INFO     ai_trading.main:main.py:1472 DATA_CONFIG feed=iex adjustment=all timeframe=1Day/1Min provider=alpaca
WARNING  ai_trading.position_sizing:position_sizing.py:316 ALPACA_HTTP_ERROR
WARNING  ai_trading.main:main.py:836 ACCOUNT_EQUITY_MISSING
INFO     ai_trading.main:main.py:1345 REQUESTS_POOL_STATS
INFO     ai_trading.main:main.py:1501 STARTUP_BANNER
INFO     ai_trading.main:main.py:1532 Warm-up run_cycle completed
INFO     ai_trading.main:main.py:1603 Runtime defaults resolved
INFO     ai_trading.main:main.py:1643 HTTP_PROFILE_CLOSED
INFO     ai_trading.main:main.py:1785 CYCLE_TIMING
INFO     ai_trading.main:main.py:1823 SCHEDULER_COMPLETE
____________________ test_ensure_trade_log_path_unwritable _____________________

tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_ensure_trade_log_path_unw0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d718b55e0>

    def test_ensure_trade_log_path_unwritable(tmp_path, monkeypatch):
        """ensure_trade_log_path exits when the path is not writable."""
    
        log_path = tmp_path / "subdir" / "trades.csv"
        parent = log_path.parent
        parent.mkdir()
        parent.chmod(0o400)
        monkeypatch.setattr(bot_engine, "TRADE_LOG_FILE", str(log_path))
        bot_engine._TRADE_LOGGER_SINGLETON = None
    
>       with pytest.raises(SystemExit) as exc:
E       Failed: DID NOT RAISE <class 'SystemExit'>

tests/test_main_trade_log_path.py:56: Failed
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:14Z", "level": "WARNING", "name": "ai_trading.logging", "msg": "TRADE_LOGGER_FALLBACK_ACTIVE", "preferred_path": "/tmp/pytest-of-root/pytest-316/test_ensure_trade_log_path_unw0/subdir/trades.csv", "fallback_path": "/root/ai-trading-bot-1/logs/trades.csv", "reason": "log_dir_not_writable", "detail": "/tmp/pytest-of-root/pytest-316/test_ensure_trade_log_path_unw0/subdir", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.logging:__init__.py:889 TRADE_LOG_FALLBACK_USER_STATE
WARNING  ai_trading.logging:__init__.py:889 TRADE_LOGGER_FALLBACK_ACTIVE
______________________ test_market_closed_sleep_is_capped ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d718cc9e0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d718cc560>

    def test_market_closed_sleep_is_capped(monkeypatch, caplog):
        from ai_trading import main as main_module
    
        caplog.set_level(logging.WARNING, logger=main_module.logger.name)
    
        stub_settings = types.SimpleNamespace(
            interval_when_closed=120,
            alpaca_data_feed="sip",
            alpaca_adjustment="raw",
        )
    
        monkeypatch.setattr(main_module, "ensure_dotenv_loaded", lambda: None)
        monkeypatch.setattr(main_module, "_check_alpaca_sdk", lambda: None)
        monkeypatch.setattr(main_module, "_fail_fast_env", lambda: None)
        monkeypatch.setattr(main_module, "_validate_runtime_config", lambda *args, **kwargs: None)
        monkeypatch.setattr(main_module, "_init_http_session", lambda *args, **kwargs: False)
        monkeypatch.setattr(main_module, "get_settings", lambda: stub_settings)
        monkeypatch.setattr(main_module, "_is_market_open_base", lambda: False)
    
        sleep_calls = []
    
        def fake_sleep(duration):
            sleep_calls.append(duration)
    
        monkeypatch.setattr(main_module.time, "sleep", fake_sleep)
        monkeypatch.setattr(main_module, "config", None, raising=False)
    
        delta = timedelta(hours=10)
        monkeypatch.setattr(main_module, "next_market_open", lambda now: now + delta)
    
        main_module.main([])
    
>       assert sleep_calls == [pytest.approx(120.0)]
E       assert [0.25, 0.25, ...25, 0.25, ...] == [120.0 ± 1.2e-04]
E         At index 0 diff: 0.25 != 120.0 ± 1.2e-04
E         Left contains 479 more items, first extra item: 0.25
E         Use -v to get more diff

tests/test_market_closed_logging.py:118: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.main:main.py:1428 MARKET_CLOSED_SLEEP
_______________ test_trade_persistence_updates_canonical_history _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d718ce180>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_trade_persistence_updates0')
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d718cecc0>

    def test_trade_persistence_updates_canonical_history(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path,
        caplog: pytest.LogCaptureFixture,
    ) -> None:
        canonical_path = tmp_path / "trade_history.parquet"
        trade_log_path = tmp_path / "trades.csv"
    
        import ai_trading.meta_learning.persistence as persistence
    
        monkeypatch.setattr(persistence, "_CANONICAL_PATH", canonical_path)
        monkeypatch.setattr(persistence, "_PANDAS_MISSING_LOGGED", False)
    
        import ai_trading.core.bot_engine as bot_engine
    
        monkeypatch.setattr(bot_engine, "TRADE_LOG_FILE", str(trade_log_path), raising=False)
        bot_engine._TRADE_LOGGER_SINGLETON = None
        bot_engine._TRADE_LOG_CACHE = None
        bot_engine._TRADE_LOG_CACHE_LOADED = False
        bot_engine._EMPTY_TRADE_LOG_INFO_EMITTED = False
    
        caplog.set_level(logging.INFO)
        first_read = bot_engine._read_trade_log(str(trade_log_path))
        assert first_read is None
        empty_logs = [rec for rec in caplog.records if "TRADE_LOG_EMPTY" in rec.getMessage()]
        assert empty_logs, "expected empty trade log notification"
    
        ctx = SimpleNamespace(risk_engine=SimpleNamespace(register_fill=lambda *_: None))
        engine = ExecutionEngine(ctx=ctx)
        order = Order("AAPL", OrderSide.BUY, 100, order_type=OrderType.MARKET, price=Money(150))
        order.add_fill(100, Money(151))
        signal = SimpleNamespace(
            symbol="AAPL",
            side="buy",
            strategy="alpha",
            confidence=0.8,
            signal_tags="alpha",
            weight=0.5,
        )
        engine._order_signal_meta[order.id] = _SignalMeta(signal=signal, requested_qty=100, signal_weight=0.5)
        caplog.clear()
        engine._handle_execution_event(order, "completed")
    
>       assert canonical_path.exists()
E       AssertionError: assert False
E        +  where False = <bound method Path.exists of PosixPath('/tmp/pytest-of-root/pytest-316/test_trade_persistence_updates0/trade_history.parquet')>()
E        +    where <bound method Path.exists of PosixPath('/tmp/pytest-of-root/pytest-316/test_trade_persistence_updates0/trade_history.parquet')> = PosixPath('/tmp/pytest-of-root/pytest-316/test_trade_persistence_updates0/trade_history.parquet').exists

tests/test_meta_learn_persistence.py:63: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:14Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "TRADE_LOG_EMPTY | action=fall_back_allow_all", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:10745 TRADE_LOG_EMPTY | action=fall_back_allow_all
______________________ test_register_checks_existing_keys ______________________

    def test_register_checks_existing_keys():
        registry.reset_registry()
        reg = registry.get_registry()
    
        first = metrics.Counter("dup_total", "Doc", registry=reg)
        # registering the same metric again should simply return the existing instance
        same = registry.register(first)
        assert same is first
    
        # registering a different metric with the same name should also return the existing one
        second = metrics.Counter("dup_total", "Doc")
        retrieved = registry.register(second)
>       assert retrieved is first
E       assert <ai_trading.metrics._NoopMetric object at 0x7c1d71a838c0> is <ai_trading.metrics._NoopMetric object at 0x7c1d71a80ef0>

tests/test_metrics_registry.py:37: AssertionError
______________________ test_age_cached_minute_timestamps _______________________

    def test_age_cached_minute_timestamps() -> None:
        now = int(datetime.now(UTC).timestamp())
        earlier = now - 30
        set_cached_minute_timestamp("MSFT", earlier)
        # simulate old insertion
        _MINUTE_CACHE["MSFT"] = (earlier, now - 30)
        removed = age_cached_minute_timestamps(max_age_seconds=10)
>       assert removed == 1
E       assert 2 == 1

tests/test_minute_cache_helpers.py:31: AssertionError
__________________ test_heuristic_fallback_marked_placeholder __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d2e5df0>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_heuristic_fallback_marked0')

    def test_heuristic_fallback_marked_placeholder(monkeypatch, tmp_path):
        external = tmp_path / "ext"
        internal = tmp_path / "int"
        external.mkdir()
        internal.mkdir()
    
        monkeypatch.setenv("AI_TRADING_MODELS_DIR", str(external))
    
        import ai_trading.paths as paths
        import ai_trading.model_loader as model_loader
    
        importlib.reload(paths)
        ml = importlib.reload(model_loader)
        monkeypatch.setattr(ml, "INTERNAL_MODELS_DIR", internal)
        ml.ML_MODELS.clear()
    
>       model = ml.load_model("MISSING_PLACEHOLDER")

tests/test_model_loading.py:143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

symbol = 'MISSING_PLACEHOLDER'

    def load_model(symbol: str) -> object:
        """Load an ML model for ``symbol``.
    
        Two locations are searched in order:
    
        * The external models directory resolved from
          :data:`ai_trading.paths.MODELS_DIR` (``AI_TRADING_MODELS_DIR``).
        * Built-in models shipped with the package under ``INTERNAL_MODELS_DIR``.
    
        The first matching ````.pkl```` file is deserialized with :mod:`joblib`. If no
        model file exists in either location, a ``RuntimeError`` is raised. Paths are
        validated to remain within their respective base directories.
        """
    
        # Prefer active model from registry when available
        try:
            from ai_trading.model_registry import get_active_model_meta
    
            meta = get_active_model_meta(symbol)
            if meta and meta.get("path"):
                try:
                    return joblib.load(Path(meta["path"]))
                except Exception as exc:
                    logger.warning("MODEL_REGISTRY_LOAD_FAILED for %s: %s", symbol, exc)
        except Exception:
            pass
    
        dirs = (MODELS_DIR, INTERNAL_MODELS_DIR)
        for base in dirs:
            path = (base / f"{symbol}.pkl").resolve()
            if not path.is_relative_to(base):
                raise RuntimeError(f"Model path escapes models directory: {path}")
            if path.exists():
                try:
                    model = joblib.load(path)
                except Exception as exc:  # noqa: BLE001 - joblib may raise various errors
                    msg = f"Failed to load model for '{symbol}' at '{path}': {exc}"
                    logger.error(
                        "MODEL_LOAD_ERROR",
                        extra={"symbol": symbol, "path": str(path), "error": str(exc)},
                    )
                    raise RuntimeError(msg) from exc
                ML_MODELS[symbol] = model
                return model
    
        logger.error(
            "MODEL_FILE_MISSING",
            extra={"symbol": symbol, "paths": [str(p) for p in dirs]},
        )
>       raise RuntimeError("Model required but not configured")
E       RuntimeError: Model required but not configured

ai_trading/model_loader.py:196: RuntimeError
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.model_loader:model_loader.py:192 MODEL_FILE_MISSING
____________________________ test_generate_ret_nan _____________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d2282f0>

    def test_generate_ret_nan(monkeypatch):
        df = pd.DataFrame({"close": [1,2,3,4]})
        ctx = Ctx(df)
        strat = MomentumStrategy(lookback=3)
        ctx.data_fetcher.df.loc[len(df)-1, "close"] = float('nan')
>       monkeypatch.setattr(momentum.pd, "isna", lambda v: True)
E       AttributeError: None has no attribute 'isna'

tests/test_momentum_extra.py:33: AttributeError
_____________________ test_bot_engine_import_no_nameerror ______________________

    def test_bot_engine_import_no_nameerror():
        """Test that bot_engine can be imported without NameError for BUY_THRESHOLD.
    
        This test creates a controlled environment and tries to import bot_engine,
        specifically checking for the NameError that was occurring before the fix.
        """
    
        # Create a test script that tries to import bot_engine
        test_script = '''
    import os
    import sys
    
    # Set test environment BEFORE any imports
    os.environ["PYTEST_RUNNING"] = "1"
    
    # Set minimal required environment variables to prevent hangs/errors
    os.environ.update({
        "ALPACA_API_KEY": "FAKE_TEST_API_KEY_NOT_REAL_123456789",  # Valid format
        "ALPACA_SECRET_KEY": "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789",  # Valid format
        "ALPACA_BASE_URL": "https://paper-api.alpaca.markets",
        "WEBHOOK_SECRET": "fake-test-webhook-not-real",
        "FLASK_PORT": "9000",
        "TRADING_MODE": "balanced",
        "DOLLAR_RISK_LIMIT": "0.05",
        "TESTING": "1",  # Enable testing mode to avoid expensive validations
        "TRADE_LOG_FILE": "test_trades.csv",
        "SEED": "42",
        "RATE_LIMIT_BUDGET": "190",
        # Add more environment variables that could prevent import hangs
        "DISABLE_DAILY_RETRAIN": "True",
        "DRY_RUN": "True",
        "SHADOW_MODE": "True",
        "PYTEST_RUNNING": "1",  # AI-AGENT-REF: Enable fast import mode for bot_engine
    })
    
    try:
        # This should trigger validate_trading_parameters() during import
    from ai_trading.core import bot_engine
    
        print("SUCCESS: bot_engine imported without NameError")
        exit_code = 0
    except NameError as e:
        if "BUY_THRESHOLD" in str(e):
            print(f"FAILURE: NameError for BUY_THRESHOLD still occurs: {e}")
            exit_code = 1
        elif any(param in str(e) for param in ["CAPITAL_CAP", "CONF_THRESHOLD", "DOLLAR_RISK_LIMIT"]):
            print(f"FAILURE: NameError for trading parameter: {e}")
            exit_code = 1
        else:
            print(f"OTHER_NAMEERROR: {e}")
            exit_code = 2
    except Exception as e:
        # Other exceptions are expected due to missing dependencies, incomplete env, etc.
        print(f"OTHER_EXCEPTION: {type(e).__name__}: {e}")
        exit_code = 0  # This is OK
    
    sys.exit(exit_code)
    '''
    
        # Write the test script to a temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(test_script)
            script_path = f.name
    
        try:
            # Get the project root directory
            project_root = Path(__file__).resolve().parents[1]
    
            # Run the test script in a subprocess with proper PYTHONPATH
            env = os.environ.copy()
            env['PYTHONPATH'] = str(project_root)
    
            try:
>               result = subprocess.run(
                    [sys.executable, script_path],
                    capture_output=True,
                    text=True,
                    env=env,
                    timeout=15,  # AI-AGENT-REF: Increase timeout to 15 seconds for more realistic import time
                    check=True
                )

tests/test_nameerror_integration.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = None, capture_output = True, timeout = 15, check = True
popenargs = (['/usr/bin/python3', '/tmp/tmpczkh5y_0.py'],)
kwargs = {'env': {'AI_HTTP_TIMEOUT': '10', 'AI_TRADING_HEALTH_TICK_SECONDS': '30', 'AI_TRADING_MODELS_DIR': '/root/ai-trading-bot-1/models', 'AI_TRADING_MODEL_MODULE': 'dummy_model', ...}, 'stderr': -1, 'stdout': -1, 'text': True}
process = <Popen: returncode: 1 args: ['/usr/bin/python3', '/tmp/tmpczkh5y_0.py']>
stdout = ''
stderr = '  File "/tmp/tmpczkh5y_0.py", line 30\n    from ai_trading.core import bot_engine\n    ^\nIndentationError: expected an indented block after \'try\' statement on line 28\n'
retcode = 1

    def run(*popenargs,
            input=None, capture_output=False, timeout=None, check=False, **kwargs):
        """Run command with arguments and return a CompletedProcess instance.
    
        The returned instance will have attributes args, returncode, stdout and
        stderr. By default, stdout and stderr are not captured, and those attributes
        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them,
        or pass capture_output=True to capture both.
    
        If check is True and the exit code was non-zero, it raises a
        CalledProcessError. The CalledProcessError object will have the return code
        in the returncode attribute, and output & stderr attributes if those streams
        were captured.
    
        If timeout is given, and the process takes too long, a TimeoutExpired
        exception will be raised.
    
        There is an optional argument "input", allowing you to
        pass bytes or a string to the subprocess's stdin.  If you use this argument
        you may not also use the Popen constructor's "stdin" argument, as
        it will be used internally.
    
        By default, all communication is in bytes, and therefore any "input" should
        be bytes, and the stdout and stderr will be bytes. If in text mode, any
        "input" should be a string, and stdout and stderr will be strings decoded
        according to locale encoding, or by "encoding" if set. Text mode is
        triggered by setting any of text, encoding, errors or universal_newlines.
    
        The other arguments are the same as for the Popen constructor.
        """
        if input is not None:
            if kwargs.get('stdin') is not None:
                raise ValueError('stdin and input arguments may not both be used.')
            kwargs['stdin'] = PIPE
    
        if capture_output:
            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not None:
                raise ValueError('stdout and stderr arguments may not be used '
                                 'with capture_output.')
            kwargs['stdout'] = PIPE
            kwargs['stderr'] = PIPE
    
        with Popen(*popenargs, **kwargs) as process:
            try:
                stdout, stderr = process.communicate(input, timeout=timeout)
            except TimeoutExpired as exc:
                process.kill()
                if _mswindows:
                    # Windows accumulates the output in a single blocking
                    # read() call run on child threads, with the timeout
                    # being done in a join() on those threads.  communicate()
                    # _after_ kill() is required to collect that and add it
                    # to the exception.
                    exc.stdout, exc.stderr = process.communicate()
                else:
                    # POSIX _communicate already populated the output so
                    # far into the TimeoutExpired exception.
                    process.wait()
                raise
            except:  # Including KeyboardInterrupt, communicate handled that.
                process.kill()
                # We don't call process.wait() as .__exit__ does that for us.
                raise
            retcode = process.poll()
            if check and retcode:
>               raise CalledProcessError(retcode, process.args,
                                         output=stdout, stderr=stderr)
E               subprocess.CalledProcessError: Command '['/usr/bin/python3', '/tmp/tmpczkh5y_0.py']' returned non-zero exit status 1.

/usr/lib/python3.12/subprocess.py:571: CalledProcessError

During handling of the above exception, another exception occurred:

    def test_bot_engine_import_no_nameerror():
        """Test that bot_engine can be imported without NameError for BUY_THRESHOLD.
    
        This test creates a controlled environment and tries to import bot_engine,
        specifically checking for the NameError that was occurring before the fix.
        """
    
        # Create a test script that tries to import bot_engine
        test_script = '''
    import os
    import sys
    
    # Set test environment BEFORE any imports
    os.environ["PYTEST_RUNNING"] = "1"
    
    # Set minimal required environment variables to prevent hangs/errors
    os.environ.update({
        "ALPACA_API_KEY": "FAKE_TEST_API_KEY_NOT_REAL_123456789",  # Valid format
        "ALPACA_SECRET_KEY": "FAKE_TEST_SECRET_KEY_NOT_REAL_123456789",  # Valid format
        "ALPACA_BASE_URL": "https://paper-api.alpaca.markets",
        "WEBHOOK_SECRET": "fake-test-webhook-not-real",
        "FLASK_PORT": "9000",
        "TRADING_MODE": "balanced",
        "DOLLAR_RISK_LIMIT": "0.05",
        "TESTING": "1",  # Enable testing mode to avoid expensive validations
        "TRADE_LOG_FILE": "test_trades.csv",
        "SEED": "42",
        "RATE_LIMIT_BUDGET": "190",
        # Add more environment variables that could prevent import hangs
        "DISABLE_DAILY_RETRAIN": "True",
        "DRY_RUN": "True",
        "SHADOW_MODE": "True",
        "PYTEST_RUNNING": "1",  # AI-AGENT-REF: Enable fast import mode for bot_engine
    })
    
    try:
        # This should trigger validate_trading_parameters() during import
    from ai_trading.core import bot_engine
    
        print("SUCCESS: bot_engine imported without NameError")
        exit_code = 0
    except NameError as e:
        if "BUY_THRESHOLD" in str(e):
            print(f"FAILURE: NameError for BUY_THRESHOLD still occurs: {e}")
            exit_code = 1
        elif any(param in str(e) for param in ["CAPITAL_CAP", "CONF_THRESHOLD", "DOLLAR_RISK_LIMIT"]):
            print(f"FAILURE: NameError for trading parameter: {e}")
            exit_code = 1
        else:
            print(f"OTHER_NAMEERROR: {e}")
            exit_code = 2
    except Exception as e:
        # Other exceptions are expected due to missing dependencies, incomplete env, etc.
        print(f"OTHER_EXCEPTION: {type(e).__name__}: {e}")
        exit_code = 0  # This is OK
    
    sys.exit(exit_code)
    '''
    
        # Write the test script to a temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(test_script)
            script_path = f.name
    
        try:
            # Get the project root directory
            project_root = Path(__file__).resolve().parents[1]
    
            # Run the test script in a subprocess with proper PYTHONPATH
            env = os.environ.copy()
            env['PYTHONPATH'] = str(project_root)
    
            try:
                result = subprocess.run(
                    [sys.executable, script_path],
                    capture_output=True,
                    text=True,
                    env=env,
                    timeout=15,  # AI-AGENT-REF: Increase timeout to 15 seconds for more realistic import time
                    check=True
                )
            except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
                # Handle subprocess timeout gracefully
>               assert False, "Subprocess timeout - bot_engine import took longer than 5 seconds"
E               AssertionError: Subprocess timeout - bot_engine import took longer than 5 seconds
E               assert False

tests/test_nameerror_integration.py:95: AssertionError
________________________ test_price_source_prefers_nbbo ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d719744d0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71a12690>

    def test_price_source_prefers_nbbo(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
        caplog.set_level(logging.INFO)
        monkeypatch.setenv("PRICE_SLIPPAGE_BPS", "2")
        quotes = {
            None: SimpleNamespace(bid_price=100.0, ask_price=100.5),
        }
        ctx = _ctx_with_quotes(quotes)
        price, source = bot_engine._resolve_limit_price(ctx, "AAPL", "buy", None, None)
        assert source == "broker_nbbo"
        assert price == pytest.approx(100.2708, rel=1e-6)
        messages = [rec.message for rec in caplog.records if rec.message.startswith("ORDER_PRICE_SOURCE")]
>       assert messages and "reason=ok" in messages[-1]
E       assert ([])

tests/test_order_price_sourcing.py:74: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:16Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "QUOTE_SOURCE_BROKER_NBBO", "symbol": "AAPL", "reason": "ok", "limit_price": 100.2708, "bid": "100.000000", "ask": "100.500000", "mid": "100.250000", "side": "buy", "slippage_bps": 2.0, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:22075 QUOTE_SOURCE_BROKER_NBBO
_______________ test_price_source_primary_mid_when_nbbo_missing ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7f703d70>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71a11d00>

    def test_price_source_primary_mid_when_nbbo_missing(
        monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture
    ) -> None:
        caplog.set_level(logging.INFO)
        monkeypatch.setenv("PRICE_SLIPPAGE_BPS", "2")
        monkeypatch.setattr(bot_engine, "DATA_FEED_INTRADAY", "iex", raising=False)
        quotes = {
            None: SimpleNamespace(bid_price=0.0, ask_price=0.0),
            "iex": SimpleNamespace(bid_price=104.0, ask_price=105.0),
        }
        ctx = _ctx_with_quotes(quotes)
        price, source = bot_engine._resolve_limit_price(ctx, "MSFT", "sell", None, None)
        assert source == "primary_mid"
        assert price == pytest.approx(104.4783, rel=1e-6)
        messages = [rec.message for rec in caplog.records if rec.message.startswith("ORDER_PRICE_SOURCE")]
>       assert messages and "reason=broker_nbbo:no_bid_ask" in messages[-1]
E       assert ([])

tests/test_order_price_sourcing.py:92: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:16Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "QUOTE_SOURCE_PRIMARY_MID", "symbol": "MSFT", "reason": "broker_nbbo:no_bid_ask", "limit_price": 104.4783, "bid": "104.000000", "ask": "105.000000", "mid": "104.500000", "side": "sell", "slippage_bps": 2.0, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:22075 QUOTE_SOURCE_PRIMARY_MID
_________________________ test_price_source_backup_mid _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71bf0ec0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71bf3aa0>

    def test_price_source_backup_mid(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
        caplog.set_level(logging.INFO)
        monkeypatch.setenv("PRICE_SLIPPAGE_BPS", "2")
        monkeypatch.setattr(bot_engine, "DATA_FEED_INTRADAY", "iex", raising=False)
        monkeypatch.setattr(bot_engine.data_fetcher_module, "_sip_configured", lambda: True, raising=False)
        quotes = {
            None: SimpleNamespace(bid_price=0.0, ask_price=0.0),
            "iex": SimpleNamespace(bid_price=0.0, ask_price=0.0),
            "sip": SimpleNamespace(bid_price=200.0, ask_price=201.0),
        }
        ctx = _ctx_with_quotes(quotes)
        price, source = bot_engine._resolve_limit_price(ctx, "TSLA", "buy", None, None)
        assert source == "backup_mid"
        assert price == pytest.approx(200.5409, rel=1e-6)
        messages = [rec.message for rec in caplog.records if rec.message.startswith("ORDER_PRICE_SOURCE")]
>       assert messages and "primary_mid:iex:no_bid_ask" in messages[-1]
E       assert ([])

tests/test_order_price_sourcing.py:110: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:16Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "QUOTE_SOURCE_BACKUP_MID", "symbol": "TSLA", "reason": "broker_nbbo:no_bid_ask;primary_mid:iex:no_bid_ask", "limit_price": 200.5409, "bid": "200.000000", "ask": "201.000000", "mid": "200.500000", "side": "buy", "slippage_bps": 2.0, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:22075 QUOTE_SOURCE_BACKUP_MID
____________________ test_price_source_last_close_fallback _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71bf3bc0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d2289e0>

    def test_price_source_last_close_fallback(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture) -> None:
        caplog.set_level(logging.INFO)
        monkeypatch.setenv("PRICE_SLIPPAGE_BPS", "2")
        monkeypatch.setattr(bot_engine, "DATA_FEED_INTRADAY", "iex", raising=False)
        monkeypatch.setattr(bot_engine.data_fetcher_module, "_sip_configured", lambda: False, raising=False)
        quotes = {
            None: None,
            "iex": None,
        }
        ctx = _ctx_with_quotes(quotes)
        minute_df = pd.DataFrame()
        price, source = bot_engine._resolve_limit_price(ctx, "NFLX", "buy", minute_df, 50.0)
>       assert source == "last_close"
E       AssertionError: assert None == 'last_close'

tests/test_order_price_sourcing.py:125: AssertionError
_______________________ test_meta_learning_mixed_format ________________________

    def test_meta_learning_mixed_format():
        """Test that meta-learning can handle mixed audit/meta-learning log formats."""
    
        # Test with the actual trades.csv file
        quality_report = validate_trade_data_quality("trades.csv")
    
        # Verify mixed format detection
>       assert quality_report["file_exists"], "Trade log file should exist"
E       AssertionError: Trade log file should exist
E       assert False

tests/test_performance_fixes.py:32: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:17Z", "level": "WARNING", "name": "ai_trading.meta_learning.core", "msg": "TRADE_HISTORY_MISSING: trades.csv", "hint": "Seed with `python -m ai_trading.tools.seed_trade_history` or see docs/SEED_TRADE_HISTORY.md", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.meta_learning.core:core.py:277 TRADE_HISTORY_MISSING: trades.csv
___________________________ test_comprehensive_fixes ___________________________

    def test_comprehensive_fixes():
        """Run comprehensive test of all performance fixes."""
    
>       test_meta_learning_mixed_format()

tests/test_performance_fixes.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def test_meta_learning_mixed_format():
        """Test that meta-learning can handle mixed audit/meta-learning log formats."""
    
        # Test with the actual trades.csv file
        quality_report = validate_trade_data_quality("trades.csv")
    
        # Verify mixed format detection
>       assert quality_report["file_exists"], "Trade log file should exist"
E       AssertionError: Trade log file should exist
E       assert False

tests/test_performance_fixes.py:32: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.meta_learning.core:core.py:277 TRADE_HISTORY_MISSING: trades.csv
___________________________ test_short_close_queued ____________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d92db3ec0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7eabf470>

    def test_short_close_queued(monkeypatch, caplog):
        state = bot_engine.BotState()
        state.position_cache = {"TSLA": -44}
        bot_engine.state = state
        caplog.set_level("INFO")
    
        orders = []
        monkeypatch.setattr(bot_engine, "submit_order", lambda ctx, symbol, qty, side: orders.append((symbol, qty, side)))
        monkeypatch.setattr(bot_engine, "is_market_open", lambda: True)
        monkeypatch.setattr(bot_engine, "fetch_minute_df_safe", lambda s: pd.DataFrame({"close": [1]}, index=[pd.Timestamp("2023-01-01")]))
        monkeypatch.setattr(bot_engine, "_safe_trade", lambda *a, **k: None)
>       monkeypatch.setattr(bot_engine.prediction_executor, "submit", lambda fn, s: types.SimpleNamespace(result=lambda: fn(s)))
E       AttributeError: None has no attribute 'submit'

tests/test_portfolio.py:20: AttributeError
____________________________ test_predict_function _____________________________

tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_predict_function0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71975c40>

    @pytest.mark.smoke
    def test_predict_function(tmp_path, monkeypatch):
        predict = _import_predict(monkeypatch)
        monkeypatch.setattr(predict, "load_model", lambda regime: DummyModel())
        csv_path = tmp_path / "AAPL.csv"
        pd.DataFrame({"close": [1.0]}).to_csv(csv_path, index=False)
>       pred, proba = predict.predict(str(csv_path))

tests/test_predict_smoke.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/predict.py:45: in predict
    features = feature_prepare.prepare_indicators(df)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df =    close  high  low  volume  open  vwap  rsi  atr  kc_lower  kc_mid  kc_upper
0    1.0   1.0  1.0     1.0   1.0   0.0  0.0  0.0       NaN     NaN       NaN
freq = 'daily'

    def prepare_indicators(df: "pd.DataFrame", freq: str = 'daily') -> "pd.DataFrame":
        pd = load_pandas()
        try:
            ta = importlib.import_module('pandas_ta')
        except ImportError as exc:  # pragma: no cover - optional dependency
            msg = (
                "pandas_ta is required for indicator calculations. "
                "Install it via `pip install pandas-ta` or include it from "
                "requirements-extras-ta.txt."
            )
            raise ImportError(msg) from exc
        df = df.copy()
        rename_map = {}
        variants = {'high': ['High', 'HIGH', 'H', 'h'], 'low': ['Low', 'LOW', 'L', 'l'], 'close': ['Close', 'CLOSE', 'C', 'c'], 'open': ['Open', 'OPEN', 'O', 'o'], 'volume': ['Volume', 'VOLUME', 'V', 'v']}
        for std, cols in variants.items():
            for col in cols:
                if col in df.columns:
                    rename_map[col] = std
        if rename_map:
            df = df.rename(columns=rename_map)
        for col in ['high', 'low', 'close', 'volume']:
            if col not in df.columns:
                if col in ('high', 'low') and 'close' in df.columns:
                    df[col] = df['close']
                elif col == 'volume':
                    df[col] = 1
                else:
                    raise KeyError(f"Column '{col}' not found in DataFrame in prepare_indicators")
        if 'open' not in df.columns:
            df['open'] = df['close']
        for col in ['high', 'low', 'close', 'volume', 'open']:
            if col in df.columns:
                df[col] = df[col].astype(float)
        idx = safe_to_datetime(df.index, context='retrain index')
        if idx.empty:
            raise ValueError('Invalid date values in dataframe')
        df = df.sort_index()
        df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume']).astype(float)
        df.dropna(subset=['vwap'], inplace=True)
        df['rsi'] = ta.rsi(df['close'], length=14).astype('float64')
        df.dropna(subset=['rsi'], inplace=True)
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14).astype('float64')
        df.dropna(subset=['atr'], inplace=True)
        df['kc_lower'] = np.nan
        df['kc_mid'] = np.nan
        df['kc_upper'] = np.nan
        try:
>           kc = ta.kc(df['high'], df['low'], df['close'], length=20)
E           AttributeError: module 'pandas_ta' has no attribute 'kc'

ai_trading/features/prepare.py:61: AttributeError
_____________________ test_preflight_initializes_trade_log _____________________

tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_preflight_initializes_tra0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7595b410>

    def test_preflight_initializes_trade_log(tmp_path, monkeypatch):
        """preflight_import_health creates the trade log file on startup."""
        # Stub alpaca client modules required by preflight_import_health
        monkeypatch.setitem(sys.modules, "alpaca", types.ModuleType("alpaca"))
        monkeypatch.setitem(sys.modules, "alpaca.trading", types.ModuleType("alpaca.trading"))
        monkeypatch.setitem(
            sys.modules, "alpaca.trading.client", types.ModuleType("alpaca.trading.client")
        )
    
        log_path = tmp_path / "trades.csv"
        monkeypatch.setattr(bot_engine, "TRADE_LOG_FILE", str(log_path), raising=False)
        bot_engine._TRADE_LOGGER_SINGLETON = None
    
        main_module.preflight_import_health()
    
>       assert log_path.exists()
E       AssertionError: assert False
E        +  where False = <bound method Path.exists of PosixPath('/tmp/pytest-of-root/pytest-316/test_preflight_initializes_tra0/trades.csv')>()
E        +    where <bound method Path.exists of PosixPath('/tmp/pytest-of-root/pytest-316/test_preflight_initializes_tra0/trades.csv')> = PosixPath('/tmp/pytest-of-root/pytest-316/test_preflight_initializes_tra0/trades.csv').exists

tests/test_preflight_trade_logger.py:23: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:18Z", "level": "INFO", "name": "ai_trading.main", "msg": "IMPORT_PREFLIGHT_OK", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.main:main.py:241 IMPORT_PREFLIGHT_OK
________________ test_soft_budget_direct_usage_elapsed_and_over ________________

    def test_soft_budget_direct_usage_elapsed_and_over():
        budget = SoftBudget(50)
    
        initial_elapsed = budget.elapsed_ms()
        assert initial_elapsed <= 1
    
        time.sleep(0.005)
        assert budget.elapsed_ms() >= 1
        assert budget.remaining() > 0.0
    
        time.sleep(0.07)
>       assert budget.over_budget() is True
E       assert False is True
E        +  where False = <bound method SoftBudget.over_budget of <ai_trading.utils.prof.SoftBudget object at 0x7c1d71952960>>()
E        +    where <bound method SoftBudget.over_budget of <ai_trading.utils.prof.SoftBudget object at 0x7c1d71952960>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d71952960>.over_budget

tests/test_prof_budget.py:20: AssertionError
________________ test_soft_budget_context_manager_resets_start _________________

    def test_soft_budget_context_manager_resets_start():
        budget = SoftBudget(50)
        time.sleep(0.005)
        assert budget.elapsed_ms() >= 1
    
        with budget as managed_budget:
            assert managed_budget is budget
            elapsed_on_enter = managed_budget.elapsed_ms()
            assert elapsed_on_enter <= 1
    
            time.sleep(0.005)
            assert managed_budget.elapsed_ms() >= 1
            assert managed_budget.remaining() > 0.0
    
        time.sleep(0.07)
>       assert budget.over_budget() is True
E       assert False is True
E        +  where False = <bound method SoftBudget.over_budget of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759be420>>()
E        +    where <bound method SoftBudget.over_budget of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759be420>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d759be420>.over_budget

tests/test_prof_budget.py:39: AssertionError
______________ test_elapsed_ms_accumulates_fractional_nanoseconds ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759bca10>

    def test_elapsed_ms_accumulates_fractional_nanoseconds(monkeypatch):
        sequence = iter(
            [
                0,  # __init__ / reset
                400_000,  # < 1ms, stays at 0
                700_000,  # cumulative < 1ms, still 0
                1_100_000,  # crosses 1ms boundary -> 1
                1_350_000,  # remainder retained -> stays 1
                2_200_000,  # cumulative crosses 2ms -> 2
            ]
        )
    
        monkeypatch.setattr(prof.time, "perf_counter_ns", lambda: next(sequence))
    
        budget = SoftBudget(10)
>       assert budget.elapsed_ms() == 1
E       assert 0.001300126314163208 == 1
E        +  where 0.001300126314163208 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bec30>>()
E        +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bec30>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bec30>.elapsed_ms

tests/test_prof_budget.py:57: AssertionError
______________ test_over_budget_and_remaining_use_existing_start _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759bdb80>

    def test_over_budget_and_remaining_use_existing_start(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                4_000_000,  # over_budget -> 4ms elapsed
                7_000_000,  # remaining -> 7ms elapsed (> budget)
                7_000_000,  # elapsed_ms -> same observation to confirm start retained
            ]
        )
        monkeypatch.setattr(prof.time, "perf_counter_ns", lambda: next(sequence))
    
        budget = SoftBudget(5)
        assert budget.over_budget() is False
>       assert budget.remaining() == 0.0
E       assert 4.996080994606018 == 0.0
E        +  where 4.996080994606018 = <bound method SoftBudget.remaining of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759e45c0>>()
E        +    where <bound method SoftBudget.remaining of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759e45c0>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d759e45c0>.remaining

tests/test_prof_budget.py:77: AssertionError
______________________ test_soft_budget_direct_regression ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759bd250>

    def test_soft_budget_direct_regression(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                400_000,  # elapsed_ms -> <1ms, emits minimal tick
                700_000,  # remaining -> accumulates fractional ns, still 1ms
                1_400_000,  # elapsed_ms -> crosses 1ms boundary
                5_500_000,  # over_budget -> accumulates to 5ms
                5_500_000,  # remaining -> no further time passes
            ]
        )
    
        monkeypatch.setattr(
            "ai_trading.utils.prof.time.perf_counter_ns", lambda: next(sequence)
        )
    
        budget = SoftBudget(5)
    
>       assert budget.elapsed_ms() == 1
E       assert 0.0010998919606208801 == 1
E        +  where 0.0010998919606208801 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bcb90>>()
E        +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bcb90>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d759bcb90>.elapsed_ms

tests/test_prof_soft_budget_regression.py:24: AssertionError
_________________ test_soft_budget_context_manager_regression __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d759bef00>

    def test_soft_budget_context_manager_regression(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                200_000,  # __enter__ reset start
                500_000,  # elapsed_ms -> <1ms, emits minimal tick
                900_000,  # remaining -> still relying on accumulated fractional ns
                6_200_000,  # over_budget -> exceeds 5ms budget
                6_200_000,  # elapsed_ms after context exit -> same observation
                6_200_000,  # remaining -> no further time passes
            ]
        )
    
        monkeypatch.setattr(
            "ai_trading.utils.prof.time.perf_counter_ns", lambda: next(sequence)
        )
    
        budget = SoftBudget(5)
    
        with budget as managed:
>           assert managed.elapsed_ms() == 1
E           assert 0.002129003405570984 == 1
E            +  where 0.002129003405570984 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d71af25a0>>()
E            +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d71af25a0>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d71af25a0>.elapsed_ms

tests/test_prof_soft_budget_regression.py:51: AssertionError
_________________ test_soft_budget_short_sleep_respects_budget _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d8039ac30>

    def test_soft_budget_short_sleep_respects_budget(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                500_000,  # elapsed_ms -> <1ms
                500_000,  # over_budget -> same observation, still under budget
                1_500_000,  # elapsed_ms -> crosses 1ms boundary
                1_500_000,  # over_budget -> >= budget
            ]
        )
    
        monkeypatch.setattr(
            "ai_trading.utils.prof.time.perf_counter_ns", lambda: next(sequence)
        )
    
        budget = SoftBudget(1)
    
>       assert budget.elapsed_ms() == 1
E       assert 0.0011101365089416504 == 1
E        +  where 0.0011101365089416504 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d594080>>()
E        +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d594080>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d594080>.elapsed_ms

tests/test_prof_soft_budget_regression.py:76: AssertionError
________________ test_soft_budget_reset_clears_fractional_state ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7598d7c0>

    def test_soft_budget_reset_clears_fractional_state(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                500_000,  # elapsed_ms -> <1ms
                800_000,  # reset -> establishes new baseline
                900_000,  # elapsed_ms after reset -> <1ms
                900_000,  # over_budget -> same observation, still under budget
            ]
        )
    
        monkeypatch.setattr(
            "ai_trading.utils.prof.time.perf_counter_ns", lambda: next(sequence)
        )
    
        budget = SoftBudget(5)
    
>       assert budget.elapsed_ms() == 1
E       assert 0.0007799826562404633 == 1
E        +  where 0.0007799826562404633 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d2e6870>>()
E        +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d2e6870>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d7d2e6870>.elapsed_ms

tests/test_prof_soft_budget_regression.py:99: AssertionError
______________ test_soft_budget_fractional_over_budget_threshold _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71bf1910>

    def test_soft_budget_fractional_over_budget_threshold(monkeypatch):
        sequence = iter(
            [
                0,  # __init__
                900_000,  # elapsed_ms -> <1ms
                1_800_000,  # over_budget -> cumulative 1.8ms (< 2ms)
                2_300_000,  # over_budget -> cumulative 2.3ms (>= 2ms)
            ]
        )
    
        monkeypatch.setattr(
            "ai_trading.utils.prof.time.perf_counter_ns", lambda: next(sequence)
        )
    
        budget = SoftBudget(2)
    
>       assert budget.elapsed_ms() == 1
E       assert 0.0009401701390743256 == 1
E        +  where 0.0009401701390743256 = <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759d4110>>()
E        +    where <bound method SoftBudget.elapsed_ms of <ai_trading.utils.prof.SoftBudget object at 0x7c1d759d4110>> = <ai_trading.utils.prof.SoftBudget object at 0x7c1d759d4110>.elapsed_ms

tests/test_prof_soft_budget_regression.py:121: AssertionError
_______________ test_primary_dwell_prevents_immediate_switchback _______________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7664a570>

    def test_primary_dwell_prevents_immediate_switchback(caplog):
        monitor = ProviderMonitor(threshold=1, cooldown=30, primary_dwell_seconds=600)
        monitor.decision_window_seconds = 0
        primary = "alpaca"
        backup = "yahoo"
    
        caplog.set_level(logging.INFO)
    
        first = monitor.update_data_health(primary, backup, healthy=False, reason="timeout", severity="degraded")
        assert first == backup
    
        state = monitor._pair_states[(primary, backup)]
        state["consecutive_passes"] = monitor.recovery_passes_required
        state["cooldown"] = 0
        monitor.min_recovery_seconds = 0
    
        caplog.clear()
        second = monitor.update_data_health(primary, backup, healthy=True, reason="recovered", severity="good")
>       assert second == backup
E       AssertionError: assert 'alpaca' == 'yahoo'
E         - yahoo
E         + alpaca

tests/test_provider_monitor.py:27: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:18Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_SWITCHOVER | from=alpaca to=yahoo reason=timeout cooldown=120s", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:18Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_SWITCHOVER | from=yahoo to=alpaca reason=recovered cooldown=0s", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.logging:alerts.py:92 AlertManager initialized
INFO     ai_trading.data.provider_monitor:provider_monitor.py:1571 DATA_PROVIDER_SWITCHOVER | from=alpaca to=yahoo reason=timeout cooldown=120s
INFO     ai_trading.data.provider_monitor:provider_monitor.py:1554 DATA_PROVIDER_SWITCHOVER | from=yahoo to=alpaca reason=recovered cooldown=0s
________________________ test_record_switchover_backoff ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d3360f0>

    def test_record_switchover_backoff(monkeypatch):
        base = datetime(2024, 1, 1, tzinfo=UTC)
    
        class FakeDT(datetime):
            current = base
    
            @classmethod
            def now(cls, tz=None):  # type: ignore[override]
                return cls.current
    
        monkeypatch.setattr(pm, "datetime", FakeDT)
        alerts = DummyAlerts()
        monitor = pm.ProviderMonitor(
            cooldown=10,
            switchover_threshold=2,
            backoff_factor=2,
            max_cooldown=40,
            alert_manager=alerts,
        )
    
        monitor.record_switchover("a", "b")
        assert monitor.consecutive_switches == 1
        assert monitor.consecutive_switches_by_provider["a"] == 1
        assert monitor._current_switch_cooldowns["a"] == 10
    
        FakeDT.current = base + timedelta(seconds=5)
        monitor.record_switchover("b", "a")
        assert monitor.consecutive_switches == 1
        # No immediate return to ``a`` because recovery dwell requirements are unmet.
        assert monitor.consecutive_switches_by_provider.get("b", 0) == 0
        assert monitor._current_switch_cooldowns["b"] == 10
        assert not alerts.calls
    
        FakeDT.current = base + timedelta(seconds=8)
        monitor.record_switchover("a", "b")
        assert monitor.consecutive_switches == 2
        assert monitor.consecutive_switches_by_provider["a"] == 2
        assert monitor._current_switch_cooldowns["a"] == 20
>       assert len(alerts.calls) == 1
E       assert 0 == 1
E        +  where 0 = len([])
E        +    where [] = <tests.test_provider_monitor_backoff.DummyAlerts object at 0x7c1d7d3362d0>.calls

tests/test_provider_monitor_backoff.py:109: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:18Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_STAY | provider=b reason=insufficient_health_passes cooldown=900s", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.provider_monitor:provider_monitor.py:1109 DATA_PROVIDER_STAY | provider=b reason=insufficient_health_passes cooldown=900s
__________________ test_record_switchover_thrashing_disables ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d3353a0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d334440>

    def test_record_switchover_thrashing_disables(monkeypatch, caplog):
        base = datetime(2024, 1, 1, tzinfo=UTC)
    
        class FakeDT(datetime):
            current = base
    
            @classmethod
            def now(cls, tz=None):  # type: ignore[override]
                return cls.current
    
        monotonic_values = iter([0.0, 30.0, 60.0])
    
        def fake_monotonic_time() -> float:
            return next(monotonic_values)
    
        monkeypatch.setattr(pm, "datetime", FakeDT)
        monkeypatch.setattr(pm, "monotonic_time", fake_monotonic_time)
    
        monitor = pm.ProviderMonitor(cooldown=10, max_cooldown=40)
    
        with caplog.at_level(logging.INFO):
            monitor.record_switchover("alpaca", "yahoo")
            monitor.record_switchover("alpaca", "yahoo")
            action = monitor.record_switchover("alpaca", "yahoo")
    
>       assert action is pm.ProviderAction.DISABLE
E       AssertionError: assert None is <ProviderAction.DISABLE: 'disable'>
E        +  where <ProviderAction.DISABLE: 'disable'> = <enum 'ProviderAction'>.DISABLE
E        +    where <enum 'ProviderAction'> = pm.ProviderAction

tests/test_provider_monitor_backoff.py:143: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.logging:alerts.py:92 AlertManager initialized
__________________ test_provider_quiet_period_respects_config __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d150380>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d150b00>

    def test_provider_quiet_period_respects_config(
        monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture
    ) -> None:
        monkeypatch.setenv("PROVIDER_SWITCH_QUIET_SECONDS", "45")
        monkeypatch.setenv("PROVIDER_MAX_COOLDOWN_SECONDS", "180")
        get_settings.cache_clear()
    
        monitor = ProviderMonitor(cooldown=10, switchover_threshold=3)
        assert monitor.max_cooldown == pytest.approx(180.0)
    
        caplog.set_level(logging.WARNING)
        for _ in range(3):
            monitor.record_switchover("alpaca_iex", "yahoo")
    
        blocked = [record for record in caplog.records if record.message == "DATA_PROVIDER_SWITCHOVER_BLOCKED"]
>       assert blocked
E       assert []

tests/test_provider_quiet_period_config.py:25: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.logging:alerts.py:92 AlertManager initialized
_________________ test_run_cycle_aborts_on_alpaca_auth_failure _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d120350>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d1220c0>

    def test_run_cycle_aborts_on_alpaca_auth_failure(monkeypatch, caplog):
        sys.modules.pop("ai_trading.core.bot_engine", None)
        monkeypatch.setenv("ALLOW_AFTER_HOURS", "1")
        monkeypatch.setattr(main, "_is_market_open_base", lambda: True)
    
        import ai_trading.alpaca_api as alpaca_api
    
        monkeypatch.setattr(alpaca_api, "_ALPACA_SERVICE_AVAILABLE", True)
    
        def raise_auth(*_a, **_k):
            monkeypatch.setattr(alpaca_api, "_ALPACA_SERVICE_AVAILABLE", False)
            raise AlpacaAuthenticationError("Unauthorized")
    
        monkeypatch.setattr(alpaca_api, "alpaca_get", raise_auth)
    
        with caplog.at_level("CRITICAL"):
            main.run_cycle()
    
>       assert "ALPACA_AUTH_PREFLIGHT_FAILED" in caplog.text
E       AssertionError: assert 'ALPACA_AUTH_PREFLIGHT_FAILED' in ''
E        +  where '' = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d1220c0>.text

tests/test_run_cycle_after_hours.py:38: AssertionError
------------------------------ Captured log setup ------------------------------
INFO     ai_trading.core.bot_engine:__init__.py:759 INDICATOR_IMPORT_OK
INFO     ai_trading.core.bot_engine:__init__.py:759 RL_IMPORT_OK
WARNING  ai_trading.core.bot_engine:bot_engine.py:3018 Alpaca SDK not installed
INFO     ai_trading.core.bot_engine:__init__.py:759 RUNTIME_SETTINGS_RESOLVED
INFO     ai_trading.core.bot_engine:bot_engine.py:3047 Config settings loaded, validation deferred to runtime
INFO     ai_trading.core.bot_engine:bot_engine.py:7011 Trading mode is set to 'balanced'
_________________________ test_run_all_trades_overlap __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d04e8a0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d7d04f5c0>

    def test_run_all_trades_overlap(monkeypatch, caplog):
        monkeypatch.delenv("AI_TRADING_MODEL_PATH", raising=False)
        monkeypatch.delenv("AI_TRADING_MODEL_MODULE", raising=False)
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(bot_engine, "_MODEL_CACHE", None, raising=False)
        monkeypatch.setattr(bot_engine, "_global_ctx", None, raising=False)
        monkeypatch.setattr(bot_engine, "_ctx", None, raising=False)
        monkeypatch.setattr(bot_engine, "ctx", None, raising=False)
    
        monkeypatch.setattr(
            bot_engine.data_fetcher_module,
            "build_fetcher",
            lambda *_: types.SimpleNamespace(source="stub"),
        )
    
        state = bot_engine.BotState()
        runtime = bot_engine.get_ctx()
        caplog.set_level("INFO")
    
        monkeypatch.setattr(bot_engine, "is_market_open", lambda: True)
        monkeypatch.setattr(bot_engine, "check_pdt_rule", lambda ctx: False)
        monkeypatch.setattr(bot_engine, "_prepare_run", lambda ctx, st: (0.0, True, []))
        monkeypatch.setattr(bot_engine, "_process_symbols", lambda *a, **k: ([], {}))
        monkeypatch.setattr(bot_engine, "_send_heartbeat", lambda: None)
    
>       api_obj = runtime.api

tests/test_run_overlap.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/core/bot_engine.py:12361: in __getattr__
    self._ensure_initialized()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ai_trading.core.bot_engine.LazyBotContext object at 0x7c1d7d04f620>

    def _ensure_initialized(self):
        """Ensure the context is initialized."""
        _init_metrics()
        global _ctx, ctx, _exec_engine, data_fetcher
    
        if self._initialized and self._context is not None:
            return
    
        # Initialize Alpaca clients first if needed
        if should_import_alpaca_sdk():
            _initialize_alpaca_clients()
    
        # AI-AGENT-REF: add null check for stream to handle Alpaca unavailable gracefully
        if stream and hasattr(stream, "subscribe_trade_updates"):
            try:
                stream.subscribe_trade_updates(on_trade_update)
            except (
                FileNotFoundError,
                PermissionError,
                IsADirectoryError,
                JSONDecodeError,
                ValueError,
                KeyError,
                TypeError,
                OSError,
            ) as e:  # AI-AGENT-REF: narrow exception
                logger.warning("Failed to subscribe to trade updates: %s", e)
    
        fetcher = data_fetcher_module.build_fetcher()
        self._context = BotContext(
            api=trading_client,
            data_client=data_client,
            data_fetcher=fetcher,
            signal_manager=signal_manager,
            trade_logger=get_trade_logger(),
            sem=Semaphore(4),
            volume_threshold=get_volume_threshold(),
            entry_start_offset=ENTRY_START_OFFSET,
            entry_end_offset=ENTRY_END_OFFSET,
            market_open=MARKET_OPEN,
            market_close=MARKET_CLOSE,
            regime_lookback=REGIME_LOOKBACK,
            regime_atr_threshold=REGIME_ATR_THRESHOLD,
            daily_loss_limit=get_daily_loss_limit(),
            kelly_fraction=params.get("KELLY_FRACTION", 0.6),
            capital_scaler=CapitalScalingEngine(params),
            adv_target_pct=0.002,
            max_position_dollars=10_000,
            params=params,
            confirmation_count={},
            trailing_extremes={},
            take_profit_targets={},
            stop_targets={},
            portfolio_weights={},
            rebalance_buys={},
            risk_engine=get_risk_engine(),
            allocator=get_allocator(),
            strategies=get_strategies(),
            # AI-AGENT-REF: Initialize drawdown circuit breaker for real-time protection
            drawdown_circuit_breaker=(
                DrawdownCircuitBreaker(
>                   max_drawdown=CFG.max_drawdown_threshold, recovery_threshold=0.8
                )
                if DrawdownCircuitBreaker
                else None
            ),
        )
E       AttributeError: 'types.SimpleNamespace' object has no attribute 'max_drawdown_threshold'

ai_trading/core/bot_engine.py:12147: AttributeError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:19Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "Risk engine: ai_trading.risk.engine.RiskEngine", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:3018 Risk engine: ai_trading.risk.engine.RiskEngine
INFO     ai_trading.risk.engine:engine.py:198 Risk engine initialized
WARNING  ai_trading.risk.engine:engine.py:262 Could not initialize data client: object() takes no arguments
___________________ test_run_all_trades_missing_get_account ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d04ef00>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d714846b0>

    def test_run_all_trades_missing_get_account(monkeypatch, caplog):
        monkeypatch.delenv("AI_TRADING_MODEL_PATH", raising=False)
        monkeypatch.delenv("AI_TRADING_MODEL_MODULE", raising=False)
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(bot_engine, "_MODEL_CACHE", None, raising=False)
        monkeypatch.setattr(bot_engine, "_global_ctx", None, raising=False)
        monkeypatch.setattr(bot_engine, "_ctx", None, raising=False)
        monkeypatch.setattr(bot_engine, "ctx", None, raising=False)
    
        monkeypatch.setattr(
            bot_engine.data_fetcher_module,
            "build_fetcher",
            lambda *_: types.SimpleNamespace(source="stub"),
        )
    
        runtime = bot_engine.get_ctx()
>       runtime.api = types.SimpleNamespace(list_positions=lambda: [])

tests/test_run_overlap.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/core/bot_engine.py:12355: in __setattr__
    self._ensure_initialized()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ai_trading.core.bot_engine.LazyBotContext object at 0x7c1d7d04d940>

    def _ensure_initialized(self):
        """Ensure the context is initialized."""
        _init_metrics()
        global _ctx, ctx, _exec_engine, data_fetcher
    
        if self._initialized and self._context is not None:
            return
    
        # Initialize Alpaca clients first if needed
        if should_import_alpaca_sdk():
            _initialize_alpaca_clients()
    
        # AI-AGENT-REF: add null check for stream to handle Alpaca unavailable gracefully
        if stream and hasattr(stream, "subscribe_trade_updates"):
            try:
                stream.subscribe_trade_updates(on_trade_update)
            except (
                FileNotFoundError,
                PermissionError,
                IsADirectoryError,
                JSONDecodeError,
                ValueError,
                KeyError,
                TypeError,
                OSError,
            ) as e:  # AI-AGENT-REF: narrow exception
                logger.warning("Failed to subscribe to trade updates: %s", e)
    
        fetcher = data_fetcher_module.build_fetcher()
        self._context = BotContext(
            api=trading_client,
            data_client=data_client,
            data_fetcher=fetcher,
            signal_manager=signal_manager,
            trade_logger=get_trade_logger(),
            sem=Semaphore(4),
            volume_threshold=get_volume_threshold(),
            entry_start_offset=ENTRY_START_OFFSET,
            entry_end_offset=ENTRY_END_OFFSET,
            market_open=MARKET_OPEN,
            market_close=MARKET_CLOSE,
            regime_lookback=REGIME_LOOKBACK,
            regime_atr_threshold=REGIME_ATR_THRESHOLD,
            daily_loss_limit=get_daily_loss_limit(),
            kelly_fraction=params.get("KELLY_FRACTION", 0.6),
            capital_scaler=CapitalScalingEngine(params),
            adv_target_pct=0.002,
            max_position_dollars=10_000,
            params=params,
            confirmation_count={},
            trailing_extremes={},
            take_profit_targets={},
            stop_targets={},
            portfolio_weights={},
            rebalance_buys={},
            risk_engine=get_risk_engine(),
            allocator=get_allocator(),
            strategies=get_strategies(),
            # AI-AGENT-REF: Initialize drawdown circuit breaker for real-time protection
            drawdown_circuit_breaker=(
                DrawdownCircuitBreaker(
>                   max_drawdown=CFG.max_drawdown_threshold, recovery_threshold=0.8
                )
                if DrawdownCircuitBreaker
                else None
            ),
        )
E       AttributeError: 'types.SimpleNamespace' object has no attribute 'max_drawdown_threshold'

ai_trading/core/bot_engine.py:12147: AttributeError
______________________________ test_runner_starts ______________________________

    def test_runner_starts():
        ctx = bot_engine.ctx
>       ctx.api = _DummyTradingClient()

tests/test_runner_additional.py:8: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/core/bot_engine.py:12355: in __setattr__
    self._ensure_initialized()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ai_trading.core.bot_engine.LazyBotContext object at 0x7c1d71bf1970>

    def _ensure_initialized(self):
        """Ensure the context is initialized."""
        _init_metrics()
        global _ctx, ctx, _exec_engine, data_fetcher
    
        if self._initialized and self._context is not None:
            return
    
        # Initialize Alpaca clients first if needed
        if should_import_alpaca_sdk():
            _initialize_alpaca_clients()
    
        # AI-AGENT-REF: add null check for stream to handle Alpaca unavailable gracefully
        if stream and hasattr(stream, "subscribe_trade_updates"):
            try:
                stream.subscribe_trade_updates(on_trade_update)
            except (
                FileNotFoundError,
                PermissionError,
                IsADirectoryError,
                JSONDecodeError,
                ValueError,
                KeyError,
                TypeError,
                OSError,
            ) as e:  # AI-AGENT-REF: narrow exception
                logger.warning("Failed to subscribe to trade updates: %s", e)
    
        fetcher = data_fetcher_module.build_fetcher()
        self._context = BotContext(
            api=trading_client,
            data_client=data_client,
            data_fetcher=fetcher,
            signal_manager=signal_manager,
            trade_logger=get_trade_logger(),
            sem=Semaphore(4),
            volume_threshold=get_volume_threshold(),
            entry_start_offset=ENTRY_START_OFFSET,
            entry_end_offset=ENTRY_END_OFFSET,
            market_open=MARKET_OPEN,
            market_close=MARKET_CLOSE,
            regime_lookback=REGIME_LOOKBACK,
            regime_atr_threshold=REGIME_ATR_THRESHOLD,
            daily_loss_limit=get_daily_loss_limit(),
            kelly_fraction=params.get("KELLY_FRACTION", 0.6),
            capital_scaler=CapitalScalingEngine(params),
            adv_target_pct=0.002,
            max_position_dollars=10_000,
            params=params,
            confirmation_count={},
            trailing_extremes={},
            take_profit_targets={},
            stop_targets={},
            portfolio_weights={},
            rebalance_buys={},
            risk_engine=get_risk_engine(),
            allocator=get_allocator(),
            strategies=get_strategies(),
            # AI-AGENT-REF: Initialize drawdown circuit breaker for real-time protection
            drawdown_circuit_breaker=(
                DrawdownCircuitBreaker(
>                   max_drawdown=CFG.max_drawdown_threshold, recovery_threshold=0.8
                )
                if DrawdownCircuitBreaker
                else None
            ),
        )
E       AttributeError: 'types.SimpleNamespace' object has no attribute 'max_drawdown_threshold'

ai_trading/core/bot_engine.py:12147: AttributeError
__________________ test_screen_universe_apierror_skips_symbol __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d735f1d00>

    def test_screen_universe_apierror_skips_symbol(monkeypatch):
        monkeypatch.setattr(bot, "_SCREEN_CACHE", {})
        monkeypatch.setattr(bot.time, "sleep", lambda *a, **k: None)
        monkeypatch.setattr(bot, "is_market_open", lambda: True)
        monkeypatch.setattr(bot, "is_valid_ohlcv", lambda df: df is not None and not df.empty)
        monkeypatch.setattr(bot, "_validate_market_data_quality", lambda df, s: {"valid": True})
    
        rows = bot.ATR_LENGTH + 5
        idx = pd.date_range("2024-01-01", periods=rows, tz="UTC")
        base = pd.DataFrame(
            {
                "open": [1.0] * rows,
                "high": [1.0] * rows,
                "low": [1.0] * rows,
                "close": [1.0] * rows,
                "volume": [200_000] * rows,
                "trade_count": [10] * rows,
                "vwap": [1.0] * rows,
            },
            index=idx,
        )
    
        def fake_client_fetch_stock_bars(client, request):
            sym = request.symbol_or_symbols[0]
            if sym == "BAD":
                raise bars.APIError("boom")
            return types.SimpleNamespace(df=base)
    
        monkeypatch.setattr(bars, "_client_fetch_stock_bars", fake_client_fetch_stock_bars)
    
        class DummyFetcher:
            def get_daily_df(self, runtime, sym):
                req = StockBarsRequest(
                    symbol_or_symbols=[sym],
                    timeframe=TimeFrame.Day,
                    start=idx[0],
                    end=idx[-1],
                    feed="iex",
                )
                return bars.safe_get_stock_bars(object(), req, sym, "DAILY")
    
        runtime = types.SimpleNamespace(data_fetcher=DummyFetcher())
    
        monkeypatch.setattr(
            bot,
            "ta",
            types.SimpleNamespace(
                atr=lambda h, l, c, length=bot.ATR_LENGTH: pd.Series([1.0] * len(h))
            ),
        )
    
        result = bot.screen_universe(["AAA", "BAD", "CCC"], runtime)
    
>       assert set(result) == {"AAA", "CCC"}
E       AssertionError: assert set() == {'AAA', 'CCC'}
E         Extra items in the right set:
E         'CCC'
E         'AAA'
E         Use -v to get more diff

tests/test_screen_universe_apierror.py:64: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:26Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "[SCREEN_UNIVERSE] Starting screening of 3 candidates: ['AAA', 'BAD', 'CCC']", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:26Z", "level": "ERROR", "name": "ai_trading.data.bars", "msg": "ALPACA_BARS_APIERROR", "symbol": "BAD", "context": "DAILY", "error": "boom", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:26Z", "level": "INFO", "name": "ai_trading.data_quality", "msg": "DATA_QUALITY_EVENT", "event": "screen_filter", "provider": "alpaca", "reason": "signal_weak", "symbols": ["AAA"], "context": {"signal_strength": 0.0, "threshold": 0.1}, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:26Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "[SCREEN_UNIVERSE] Selected 0 of 3 candidates. Selected: []. Filtered out: 3 symbols: {'AAA': 'signal_weak', 'BAD': 'no_data', 'CCC': 'signal_weak'}", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:26Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "SCREEN_SUMMARY | symbols=3 passed=0 failed=3", "tried": 3, "valid": 0, "empty": 1, "failed": 2, "failed_total": 3, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.bot_engine:bot_engine.py:20579 [SCREEN_UNIVERSE] Starting screening of 3 candidates: ['AAA', 'BAD', 'CCC']
ERROR    ai_trading.data.bars:bars.py:786 ALPACA_BARS_APIERROR
INFO     ai_trading.data_quality:__init__.py:1284 DATA_QUALITY_EVENT
INFO     ai_trading.data_quality:__init__.py:1284 DATA_QUALITY_EVENT
INFO     ai_trading.core.bot_engine:bot_engine.py:20820 [SCREEN_UNIVERSE] Selected 0 of 3 candidates. Selected: []. Filtered out: 3 symbols: {'AAA': 'signal_weak', 'BAD': 'no_data', 'CCC': 'signal_weak'}
INFO     ai_trading.core.bot_engine:bot_engine.py:20829 SCREEN_SUMMARY | symbols=3 passed=0 failed=3
______________________ test_prepare_indicators_calculates ______________________

sample_df =          open       high        low      close      volume
0    9.000000  10.000000   9.000000   9.500000  100.000000
...4
28  13.827586  14.827586  13.827586  14.327586  128.965517
29  14.000000  15.000000  14.000000  14.500000  130.000000
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d735e1910>

    def test_prepare_indicators_calculates(sample_df, monkeypatch):
        """Indicators are added using pandas_ta helpers."""
        import types
        pta = types.ModuleType('pandas_ta')
        pta.vwap = lambda h,l,c,v: pd.Series((h+l+c)/3, index=sample_df.index)
        pta.macd = lambda c, **k: {
            'MACD_12_26_9': c * 0 + 1.0,
            'MACDs_12_26_9': c * 0 + 0.5,
        }
        pta.kc = lambda h,l,c,length=20: pd.DataFrame({0: c*0+1.0,1:c*0+2.0,2:c*0+3.0})
        pta.mfi = lambda h,l,c,v,length=14: pd.Series(c*0+5.0, index=sample_df.index)
        pta.adx = lambda h,l,c,length=14: {
            'ADX_14': pd.Series(c*0+7.0, index=sample_df.index),
            'DMP_14': pd.Series(c*0+1.0, index=sample_df.index),
            'DMN_14': pd.Series(c*0+1.0, index=sample_df.index),
        }
        pta.rsi = lambda *a, **k: pd.Series([50.0]*len(sample_df))
        pta.atr = lambda *a, **k: pd.Series([1.0]*len(sample_df))
        pta.bbands = lambda *a, **k: {
            'BBU_20_2.0': pd.Series([1.0]*len(sample_df)),
            'BBL_20_2.0': pd.Series([1.0]*len(sample_df)),
            'BBP_20_2.0': pd.Series([1.0]*len(sample_df)),
        }
        monkeypatch.setitem(sys.modules, 'pandas_ta', pta)
    
        prepare_mod = types.ModuleType('ai_trading.features.prepare')
    
        def prepare_indicators(df, freq: str = 'intraday'):
            ta = pta
            df = df.copy()
            df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])
            macd = ta.macd(df['close'])
            df['macd'] = macd['MACD_12_26_9']
            kc = ta.kc(df['high'], df['low'], df['close'], length=20)
            df['kc_upper'] = kc.iloc[:, 2]
            df['mfi_14'] = ta.mfi(df['high'], df['low'], df['close'], df['volume'], length=14)
            adx = ta.adx(df['high'], df['low'], df['close'], length=14)
            df['adx'] = adx['ADX_14']
            return df
    
        prepare_mod.prepare_indicators = prepare_indicators
        monkeypatch.setitem(sys.modules, 'ai_trading.features.prepare', prepare_mod)
    
        from ai_trading.features import prepare as retrain
>       out = retrain.prepare_indicators(sample_df)

tests/test_signals.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df =          open       high        low  ...  willr  psar_long  psar_short
0    9.000000  10.000000   9.000000  ...    NaN...N        NaN         NaN
29  14.000000  15.000000  14.000000  ...    NaN        NaN         NaN

[30 rows x 29 columns]
freq = 'daily'

    def prepare_indicators(df: "pd.DataFrame", freq: str = 'daily') -> "pd.DataFrame":
        pd = load_pandas()
        try:
            ta = importlib.import_module('pandas_ta')
        except ImportError as exc:  # pragma: no cover - optional dependency
            msg = (
                "pandas_ta is required for indicator calculations. "
                "Install it via `pip install pandas-ta` or include it from "
                "requirements-extras-ta.txt."
            )
            raise ImportError(msg) from exc
        df = df.copy()
        rename_map = {}
        variants = {'high': ['High', 'HIGH', 'H', 'h'], 'low': ['Low', 'LOW', 'L', 'l'], 'close': ['Close', 'CLOSE', 'C', 'c'], 'open': ['Open', 'OPEN', 'O', 'o'], 'volume': ['Volume', 'VOLUME', 'V', 'v']}
        for std, cols in variants.items():
            for col in cols:
                if col in df.columns:
                    rename_map[col] = std
        if rename_map:
            df = df.rename(columns=rename_map)
        for col in ['high', 'low', 'close', 'volume']:
            if col not in df.columns:
                if col in ('high', 'low') and 'close' in df.columns:
                    df[col] = df['close']
                elif col == 'volume':
                    df[col] = 1
                else:
                    raise KeyError(f"Column '{col}' not found in DataFrame in prepare_indicators")
        if 'open' not in df.columns:
            df['open'] = df['close']
        for col in ['high', 'low', 'close', 'volume', 'open']:
            if col in df.columns:
                df[col] = df[col].astype(float)
        idx = safe_to_datetime(df.index, context='retrain index')
        if idx.empty:
            raise ValueError('Invalid date values in dataframe')
        df = df.sort_index()
        df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume']).astype(float)
        df.dropna(subset=['vwap'], inplace=True)
        df['rsi'] = ta.rsi(df['close'], length=14).astype('float64')
        df.dropna(subset=['rsi'], inplace=True)
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14).astype('float64')
        df.dropna(subset=['atr'], inplace=True)
        df['kc_lower'] = np.nan
        df['kc_mid'] = np.nan
        df['kc_upper'] = np.nan
        try:
            kc = ta.kc(df['high'], df['low'], df['close'], length=20)
            df['kc_lower'] = kc.iloc[:, 0].astype(float)
            df['kc_mid'] = kc.iloc[:, 1].astype(float)
            df['kc_upper'] = kc.iloc[:, 2].astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('KC indicator failed: %s', e)
        df['atr_band_upper'] = np.nan
        df['atr_band_lower'] = np.nan
        df['avg_vol_20'] = np.nan
        df['dow'] = np.nan
        df['atr_band_upper'] = (df['close'] + 1.5 * df['atr']).astype(float)
        df['atr_band_lower'] = (df['close'] - 1.5 * df['atr']).astype(float)
        df['avg_vol_20'] = df['volume'].rolling(20).mean().astype(float)
        if len(idx) == len(df):
            df['dow'] = idx.dayofweek.astype(float)
        df['macd'] = np.nan
        df['macds'] = np.nan
        try:
            macd = ta.macd(df['close'], fast=12, slow=26, signal=9)
            df['macd'] = macd['MACD_12_26_9'].astype(float)
            df['macds'] = macd['MACDs_12_26_9'].astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('MACD calculation failed: %s', e)
        df['bb_upper'] = np.nan
        df['bb_lower'] = np.nan
        df['bb_percent'] = np.nan
        try:
            bb = ta.bbands(df['close'], length=20)
            df['bb_upper'] = bb['BBU_20_2.0'].astype(float)
            df['bb_lower'] = bb['BBL_20_2.0'].astype(float)
            df['bb_percent'] = bb['BBP_20_2.0'].astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('Bollinger Bands failed: %s', e)
        df['adx'] = np.nan
        df['dmp'] = np.nan
        df['dmn'] = np.nan
        try:
            adx = ta.adx(df['high'], df['low'], df['close'], length=14)
            df['adx'] = adx['ADX_14'].astype(float)
            df['dmp'] = adx['DMP_14'].astype(float)
            df['dmn'] = adx['DMN_14'].astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('ADX calculation failed: %s', e)
        df['cci'] = np.nan
        try:
            if hasattr(ta, 'cci'):
                df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20).astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('CCI calculation failed: %s', e)
        try:
            mfi_vals = ta.mfi(df['high'], df['low'], df['close'], df['volume'], length=MFI_PERIOD).astype(float)
            df['mfi_14'] = mfi_vals
            df.dropna(subset=['mfi_14'], inplace=True)
        except (ValueError, TypeError) as e:
            logger.exception('MFI calculation failed: %s', e)
            df['mfi_14'] = np.nan
        df['tema'] = np.nan
        try:
            if hasattr(ta, 'tema'):
                df['tema'] = ta.tema(df['close'], length=10).astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('TEMA calculation failed: %s', e)
        df['willr'] = np.nan
        try:
            if hasattr(ta, 'willr'):
                df['willr'] = ta.willr(df['high'], df['low'], df['close'], length=14).astype(float)
        except (ValueError, TypeError) as e:
            logger.exception('Williams %%R calculation failed: %s', e)
        df['psar_long'] = np.nan
        df['psar_short'] = np.nan
        try:
>           psar = ta.psar(df['high'], df['low'], df['close'])
E           AttributeError: module 'pandas_ta' has no attribute 'psar'

ai_trading/features/prepare.py:132: AttributeError
__________________ test_no_unauthorized_log_when_sip_disabled __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d73344650>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d733e28a0>

    def test_no_unauthorized_log_when_sip_disabled(monkeypatch, caplog):
        monkeypatch.setenv("ALPACA_ALLOW_SIP", "0")
        monkeypatch.setattr("ai_trading.data.fetch._ALLOW_SIP", False)
        monkeypatch.setattr("ai_trading.data.fetch._SIP_DISALLOWED_WARNED", False)
        caplog.set_level("INFO")
        session = SimpleNamespace(get=lambda *a, **k: SimpleNamespace(status_code=401))
        allowed = _sip_fallback_allowed(session, {}, "1Min")
>       assert allowed is False
E       assert True is False

tests/test_sip_disallowed.py:46: AssertionError
____________ test_sip_failover_does_not_call_fetch_when_unentitled _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d733e0c20>

    def test_sip_failover_does_not_call_fetch_when_unentitled(monkeypatch):
        class _Session:
            def __init__(self):
                self.called = False
    
            def get(self, *args, **kwargs):  # pragma: no cover - defensive
                self.called = True
                return SimpleNamespace(status_code=200)
    
        session = _Session()
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr("ai_trading.data.fetch._sip_allowed", lambda: False)
        allowed = _sip_fallback_allowed(session, {}, "1Min")
>       assert allowed is False
E       assert True is False

tests/test_sip_disallowed.py:73: AssertionError
_________________ test_get_bars_unauthorized_sip_returns_empty _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d733e1340>

    def test_get_bars_unauthorized_sip_returns_empty(monkeypatch):
        """Unauthorized SIP access returns an empty DataFrame."""
        monkeypatch.setattr(data_fetcher, "_SIP_UNAUTHORIZED", False, raising=False)
    
        def fake_get(url, params=None, headers=None, timeout=None):  # noqa: ARG001
            return _RespForbidden()
    
        monkeypatch.setattr(data_fetcher.requests, "get", fake_get)
        start = datetime(2024, 1, 1, tzinfo=UTC)
        end = datetime(2024, 1, 2, tzinfo=UTC)
        df = data_fetcher.get_bars("AAPL", "1Min", start, end, feed="sip")
        assert isinstance(df, pd.DataFrame)
>       assert df.empty
E       assert False
E        +  where False =                   timestamp  open  high  low  close  volume\n0 2024-01-01 00:00:00+00:00   NaN   NaN  NaN    NaN       0.empty

tests/test_sip_unauthorized.py:33: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:31Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "yahoo", "from_provider": "alpaca_sip", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "interval": "1m", "start": "2024-01-01T00:00:00+00:00", "end": "2024-01-02T00:00:00+00:00", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
________________ test_safe_clear_dict_restores_import_machinery ________________

    def test_safe_clear_dict_restores_import_machinery():
        # Ensure the sitecustomize hook has patched unittest.mock._clear_dict.
        import sitecustomize  # noqa: F401  # pylint: disable=unused-import
    
>       assert unittest.mock._clear_dict.__name__ == "_safe_clear_dict"
E       AssertionError: assert '_clear_dict' == '_safe_clear_dict'
E         - _safe_clear_dict
E         ? -----
E         + _clear_dict

tests/test_sitecustomize.py:16: AssertionError
______________ test_start_api_waits_for_transient_port_conflicts _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7fbfd0a0>

    def test_start_api_waits_for_transient_port_conflicts(monkeypatch):
        """start_api retries briefly when port is busy without an owning PID."""
    
        probe = socket.socket()
        probe.bind(("0.0.0.0", 0))
        test_port = probe.getsockname()[1]
        probe.close()
    
        original_socket = socket.socket
    
        class RetrySocket(original_socket):
            failures_remaining = 2
    
            def bind(self, address):  # type: ignore[override]
                if RetrySocket.failures_remaining > 0:
                    RetrySocket.failures_remaining -= 1
                    err = OSError(errno.EADDRINUSE, "Address already in use")
                    err.errno = errno.EADDRINUSE
                    raise err
                return super().bind(address)
    
        monkeypatch.setattr(socket, "socket", RetrySocket)
        monkeypatch.setattr(main.socket, "socket", RetrySocket)
        monkeypatch.setattr(main, "get_settings", lambda: DummySettings(test_port, 2.0))
        monkeypatch.setattr(main, "ensure_dotenv_loaded", lambda: None)
        monkeypatch.setattr(main, "get_pid_on_port", lambda _port: None)
    
        fake_time = [0.0]
    
        def fake_monotonic():
            return fake_time[0]
    
        def fake_sleep(seconds: float):
            fake_time[0] += seconds
    
        monkeypatch.setattr(main.time, "monotonic", fake_monotonic)
        monkeypatch.setattr(main.time, "sleep", fake_sleep)
    
        called = {}
    
        def capture_run(port: int, *_args, **_kwargs):
            called["port"] = port
    
        monkeypatch.setattr(main, "run_flask_app", capture_run)
    
        try:
            main.start_api()
        finally:
            monkeypatch.setattr(socket, "socket", original_socket)
            monkeypatch.setattr(main.socket, "socket", original_socket)
    
>       assert called["port"] == test_port
E       KeyError: 'port'

tests/test_start_api_port_increment.py:94: KeyError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.main:main.py:1204 HEALTH_SERVER_STARTED
INFO     ai_trading.main:main.py:1250 API_STARTUP_ABORTED
_____________ test_trade_logger_triggers_conversion_on_audit_rows ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71dbf680>
tmp_path = PosixPath('/tmp/pytest-of-root/pytest-316/test_trade_logger_triggers_con0')

    def test_trade_logger_triggers_conversion_on_audit_rows(monkeypatch, tmp_path):
        reward_log = tmp_path / "reward_log.csv"
        trade_log = tmp_path / "trades.csv"
    
        monkeypatch.setattr(bot_engine, "REWARD_LOG_FILE", str(reward_log))
    
        trade_logger = bot_engine.TradeLogger(path=trade_log)
        state = bot_engine.BotState()
        state.capital_band = "small"
    
        trade_logger.log_entry(
            "AAPL",
            price=100.0,
            qty=1,
            side="buy",
            strategy="test_strategy",
            confidence=0.6,
        )
    
        monkeypatch.setattr(
            bot_engine,
            "get_settings",
            lambda: SimpleNamespace(enable_sklearn=True),
        )
        monkeypatch.setattr(
            "ai_trading.config.get_settings",
            lambda: SimpleNamespace(enable_sklearn=True),
        )
    
        conversion_calls: list[dict[str, object]] = []
    
        def fake_trigger(trade_data: dict[str, object]) -> bool:
            conversion_calls.append(trade_data)
            return True
    
        def fake_quality_report(_path: str) -> dict[str, int]:
            return {"audit_format_rows": 2, "meta_format_rows": 0}
    
        monkeypatch.setattr(meta_learning, "trigger_meta_learning_conversion", fake_trigger)
        monkeypatch.setattr(meta_learning, "validate_trade_data_quality", fake_quality_report)
    
        trade_logger.log_exit(state, "AAPL", 105.0)
    
>       assert len(conversion_calls) == 1
E       assert 0 == 1
E        +  where 0 = len([])

tests/test_trade_logger_meta_conversion.py:52: AssertionError
____________ test_trigger_meta_learning_conversion_pure_meta_format ____________

    def test_trigger_meta_learning_conversion_pure_meta_format():
        """Test trigger function with pure meta-learning format - should return True immediately."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            # Write meta-learning format data
            f.write("symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n")
            f.write("TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n")
            f.write("AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n")
            f.write("MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n")
            test_file = f.name
    
        try:
            # Set the trade log file path
            MockConfig.TRADE_LOG_FILE = test_file
            # Ensure pandas is available for conversion
            meta_learning._import_pandas(optional=True)
    
            # Test trade data
            test_trade = {
                'symbol': 'TEST',
                'qty': 10,
                'side': 'buy',
                'price': 100.0,
                'timestamp': '2025-08-05T23:17:35Z',
                'order_id': 'test-001',
                'status': 'filled'
            }
    
            # Verify quality report shows pure meta format
            quality_report = meta_learning.validate_trade_data_quality(test_file)
            assert meta_learning.has_mixed_format(test_file) is False
            assert quality_report['audit_format_rows'] == 0
            assert quality_report['meta_format_rows'] > 0
    
            # Test the trigger function - should return True immediately (no conversion needed)
            result = meta_learning.trigger_meta_learning_conversion(test_trade)
>           assert result is True
E           assert False is True

tests/test_trigger_meta_learning_conversion.py:85: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:40Z", "level": "INFO", "name": "ai_trading.meta_learning.core", "msg": "META_LEARNING_TRIGGER | symbol=TEST", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:40Z", "level": "WARNING", "name": "ai_trading.meta_learning.core", "msg": "Config not available for meta-learning conversion", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.meta_learning.core:core.py:1381 META_LEARNING_TRIGGER | symbol=TEST
WARNING  ai_trading.meta_learning.core:core.py:1393 Config not available for meta-learning conversion
___________ test_trigger_meta_learning_conversion_pure_audit_format ____________

    def test_trigger_meta_learning_conversion_pure_audit_format():
        """Test trigger function with pure audit format - should attempt conversion."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            # Write audit format data
            f.write("order_id,timestamp,symbol,side,qty,price,mode,status\n")
            f.write("123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:17:35Z,TEST,buy,10,100.0,live,filled\n")
            f.write("234e5678-e89b-12d3-a456-426614174001,2025-08-05T23:18:35Z,TEST,sell,10,105.0,live,filled\n")
            f.write("345e6789-e89b-12d3-a456-426614174002,2025-08-05T23:19:35Z,AAPL,buy,5,150.0,live,filled\n")
            test_file = f.name
    
        try:
            # Set the trade log file path
            MockConfig.TRADE_LOG_FILE = test_file
            # Ensure pandas is available for conversion
            meta_learning._import_pandas(optional=True)
    
            # Test trade data
            test_trade = {
                'symbol': 'TEST',
                'qty': 10,
                'side': 'buy',
                'price': 100.0,
                'timestamp': '2025-08-05T23:17:35Z',
                'order_id': 'test-001',
                'status': 'filled'
            }
    
            # Verify quality report shows pure audit format
            quality_report = meta_learning.validate_trade_data_quality(test_file)
            assert meta_learning.has_mixed_format(test_file) is False
            assert quality_report['audit_format_rows'] > 0
            assert quality_report['meta_format_rows'] == 0
    
            # Test the trigger function - should attempt conversion and return True if successful
            result = meta_learning.trigger_meta_learning_conversion(test_trade)
>           assert result is True  # Should succeed in conversion
E           assert False is True

tests/test_trigger_meta_learning_conversion.py:127: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.meta_learning.core:core.py:1381 META_LEARNING_TRIGGER | symbol=TEST
WARNING  ai_trading.meta_learning.core:core.py:1393 Config not available for meta-learning conversion
______________ test_trigger_meta_learning_conversion_mixed_format ______________

    def test_trigger_meta_learning_conversion_mixed_format():
        """Test trigger function with mixed format - should attempt conversion."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            # Write mixed format data: one meta-learning row and one audit-style row
            f.write(
                "symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n"
            )
            # Meta-learning formatted row
            f.write(
                "TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1,0.8,5.0\n"
            )
            # Audit formatted row
            f.write(
                "123e4567-e89b-12d3-a456-426614174000,2025-08-05T23:19:35Z,AAPL,buy,5,150.0,live,filled\n"
            )
            test_file = f.name
    
        try:
            # Set the trade log file path
            MockConfig.TRADE_LOG_FILE = test_file
            # Ensure pandas is available for conversion
            meta_learning._import_pandas(optional=True)
    
            # Test trade data
            test_trade = {
                'symbol': 'TEST',
                'qty': 10,
                'side': 'buy',
                'price': 100.0,
                'timestamp': '2025-08-05T23:17:35Z',
                'order_id': 'test-001',
                'status': 'filled'
            }
    
            # Verify quality report shows mixed format
            quality_report = meta_learning.validate_trade_data_quality(test_file)
            assert meta_learning.has_mixed_format(test_file) is True
    
            # Test the trigger function - should attempt conversion and return True if successful
            result = meta_learning.trigger_meta_learning_conversion(test_trade)
>           assert result is True  # Conversion should succeed with pandas available
E           assert False is True

tests/test_trigger_meta_learning_conversion.py:174: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:40Z", "level": "WARNING", "name": "ai_trading.meta_learning.core", "msg": "TRADE_HISTORY_MIXED_FORMAT: /tmp/tmpnt8c8srp.csv", "audit_rows": 1, "meta_rows": 1, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.meta_learning.core:core.py:438 TRADE_HISTORY_MIXED_FORMAT: /tmp/tmpnt8c8srp.csv
WARNING  ai_trading.meta_learning.core:core.py:438 TRADE_HISTORY_MIXED_FORMAT: /tmp/tmpnt8c8srp.csv
INFO     ai_trading.meta_learning.core:core.py:1381 META_LEARNING_TRIGGER | symbol=TEST
WARNING  ai_trading.meta_learning.core:core.py:1393 Config not available for meta-learning conversion
________ test_trigger_meta_learning_conversion_problem_statement_exact _________

    def test_trigger_meta_learning_conversion_problem_statement_exact():
        """Test the exact scenario from the problem statement."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            # Create exactly the scenario: no mixed formats (audit_format_rows=0, meta_format_rows=4)
            f.write("symbol,entry_time,entry_price,exit_time,exit_price,qty,side,strategy,classification,signal_tags,confidence,reward\n")
            f.write("TEST,2025-08-05T23:17:35Z,100.0,2025-08-05T23:18:35Z,105.0,10,buy,test_strategy,test,signal1+signal2,0.8,5.0\n")
            f.write("AAPL,2025-08-05T23:19:35Z,150.0,2025-08-05T23:20:35Z,155.0,5,buy,test_strategy,test,signal3,0.7,25.0\n")
            f.write("MSFT,2025-08-05T23:21:35Z,300.0,2025-08-05T23:22:35Z,295.0,2,sell,test_strategy,test,signal4,0.6,-10.0\n")
            f.write("GOOGL,2025-08-05T23:23:35Z,2500.0,2025-08-05T23:24:35Z,2505.0,1,buy,test_strategy,test,signal5,0.9,5.0\n")
            test_file = f.name
    
        try:
            MockConfig.TRADE_LOG_FILE = test_file
    
            test_trade = {
                'symbol': 'TEST',
                'qty': 10,
                'side': 'buy',
                'price': 100.0,
                'timestamp': '2025-08-05T23:17:35Z',
                'order_id': 'test-001',
                'status': 'filled'
            }
    
            # Verify we have the exact scenario from problem statement
            quality_report = meta_learning.validate_trade_data_quality(test_file)
            assert meta_learning.has_mixed_format(test_file) is False
            assert quality_report['audit_format_rows'] == 0
            assert quality_report['meta_format_rows'] > 0  # Should be 5 (4 data + 1 header)
    
            # This should return True immediately (no conversion needed)
            result = meta_learning.trigger_meta_learning_conversion(test_trade)
>           assert result is True, "Should return True for properly formatted meta-learning files"
E           AssertionError: Should return True for properly formatted meta-learning files
E           assert False is True

tests/test_trigger_meta_learning_conversion.py:233: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.meta_learning.core:core.py:1381 META_LEARNING_TRIGGER | symbol=TEST
WARNING  ai_trading.meta_learning.core:core.py:1393 Config not available for meta-learning conversion
_____________________ test_timing_exports_exist_and_behave _____________________

    def test_timing_exports_exist_and_behave() -> None:
        assert isinstance(HTTP_TIMEOUT, (int, float)) and HTTP_TIMEOUT > 0
>       assert clamp_timeout(None) == pytest.approx(float(HTTP_TIMEOUT))
E       assert 20.0 == 10.0 ± 1.0e-05
E         comparison failed
E         Obtained: 20.0
E         Expected: 10.0 ± 1.0e-05

tests/test_utils_timing.py:12: AssertionError
_____________________ test_stock_bars_request_keyword_args _____________________

>   ???
E   AttributeError: '_DummyReq' object has no attribute 'symbol_or_symbols'

tests/test_vendor_stub_alpaca_requests.py:27: AttributeError
__________________ test_stock_bars_request_additional_fields ___________________

>   ???
E   AttributeError: '_DummyReq' object has no attribute 'symbol_or_symbols'

tests/test_vendor_stub_alpaca_requests.py:44: AttributeError
_________ TestProcessSymbol.test_process_symbol_skips_on_all_nan_close _________

self = <test_bot_engine.TestProcessSymbol object at 0x7c1d7f384890>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71d63e60>

    def test_process_symbol_skips_on_all_nan_close(self, monkeypatch):
        state = bot_engine.BotState()
        state.position_cache = {}
        bot_engine.state = state
    
        dummy_halt = DummyHaltManager()
        dummy_ctx = DummyContext(dummy_halt)
        monkeypatch.setattr(bot_engine, "get_ctx", lambda: dummy_ctx)
        monkeypatch.setattr(bot_engine, "is_market_open", lambda: True)
        monkeypatch.setattr(bot_engine, "ensure_final_bar", lambda *_, **__: True)
        monkeypatch.setattr(bot_engine, "log_skip_cooldown", lambda *_, **__: None)
        monkeypatch.setattr(
            bot_engine,
            "skipped_duplicates",
            types.SimpleNamespace(inc=lambda: None),
            raising=False,
        )
        monkeypatch.setattr(
            bot_engine,
            "skipped_cooldown",
            types.SimpleNamespace(inc=lambda: None),
            raising=False,
        )
        monkeypatch.setattr(bot_engine.executors, "_ensure_executors", lambda: None)
        monkeypatch.setattr(bot_engine, "prediction_executor", DummyExecutor(), raising=False)
        monkeypatch.setattr(bot_engine, "_safe_trade", lambda *_, **__: None)
    
        def _raise_nan(_symbol: str):
            err = bot_engine.DataFetchError("close_column_all_nan")
            setattr(err, "fetch_reason", "close_column_all_nan")
            raise err
    
        monkeypatch.setattr(bot_engine, "fetch_minute_df_safe", _raise_nan)
    
        processed, row_counts = bot_engine._process_symbols(["AAPL"], 1000.0, None, True)
    
        assert processed == []
        assert row_counts == {}
>       assert dummy_halt.calls == ["AAPL:empty_frame"]
E       AssertionError: assert [] == ['AAPL:empty_frame']
E         Right contains one more item: 'AAPL:empty_frame'
E         Use -v to get more diff

tests/bot_engine/test_bot_engine.py:112: AssertionError
______________ test_process_symbol_reuses_prefetched_minute_data _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50bed100>

    def test_process_symbol_reuses_prefetched_minute_data(monkeypatch):
        pd = load_pandas()
        sample = _sample_df()
    
        fetch_calls: list[str] = []
    
        def fake_fetch(symbol: str):
            fetch_calls.append(symbol)
            return sample
    
        monkeypatch.setattr(bot_engine, "fetch_minute_df_safe", fake_fetch)
    
        observed: list = []
        fallback_calls: list[str] = []
    
        def fake_fetch_feature_data(ctx, state, symbol, price_df=None):
            observed.append(price_df)
            if price_df is None:
                fallback_calls.append(symbol)
                local_df = bot_engine.fetch_minute_df_safe(symbol)
            else:
                local_df = price_df
            return local_df, local_df, False
    
        monkeypatch.setattr(bot_engine, "_fetch_feature_data", fake_fetch_feature_data)
    
        def fake_trade_logic(
            ctx,
            state,
            symbol,
            balance,
            model,
            regime_ok,
            *,
            price_df=None,
            now_provider=None,
        ):
            bot_engine._fetch_feature_data(ctx, state, symbol, price_df=price_df)
            return True
    
        monkeypatch.setattr(bot_engine, "trade_logic", fake_trade_logic)
    
        state = bot_engine.BotState()
        state.position_cache = {}
        bot_engine.state = state
    
        monkeypatch.setattr(bot_engine, "is_market_open", lambda: True)
        monkeypatch.setattr(bot_engine, "ensure_final_bar", lambda symbol, timeframe: True)
        monkeypatch.setattr(bot_engine, "log_skip_cooldown", lambda *a, **k: None)
        monkeypatch.setattr(
            bot_engine,
            "skipped_duplicates",
            types.SimpleNamespace(inc=lambda: None),
            raising=False,
        )
        monkeypatch.setattr(
            bot_engine,
            "skipped_cooldown",
            types.SimpleNamespace(inc=lambda: None),
            raising=False,
        )
    
        ctx = types.SimpleNamespace(
            halt_manager=None,
            data_fetcher=types.SimpleNamespace(get_daily_df=lambda *_: sample),
            api=types.SimpleNamespace(list_positions=lambda: []),
        )
        monkeypatch.setattr(bot_engine, "get_ctx", lambda: ctx)
    
        prediction_executor = types.SimpleNamespace(
            submit=lambda fn, sym: types.SimpleNamespace(result=lambda: fn(sym))
        )
        monkeypatch.setattr(bot_engine, "prediction_executor", prediction_executor, raising=False)
        monkeypatch.setattr(bot_engine.executors, "_ensure_executors", lambda: None)
    
        processed, _ = bot_engine._process_symbols(["AAPL"], 1000.0, None, True)
    
>       assert processed == ["AAPL"]
E       AssertionError: assert [] == ['AAPL']
E         Right contains one more item: 'AAPL'
E         Use -v to get more diff

tests/bot_engine/test_fetch_minute_df_safe.py:1844: AssertionError
------------------------------ Captured log setup ------------------------------
INFO     ai_trading.logging:alerts.py:92 AlertManager initialized
_____________________ test_get_bars_df_alpaca_iex_columns ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d758773b0>

>   ???

tests/data/test_alpaca_iex_field_aliases.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/alpaca_api.py:827: in get_bars_df
    response = rest.get_stock_bars(req)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ai_trading.alpaca_api.get_data_client_cls.<locals>._UnavailableDataClient object at 0x7c1d50b48f50>
_a = (<test_alpaca_iex_field_aliases.test_get_bars_df_alpaca_iex_columns.<locals>.DummyStockBarsRequest object at 0x7c1d75876c90>,)
_k = {}

    def get_stock_bars(self, *_a, **_k):
>       raise RuntimeError(self._reason)
E       RuntimeError: alpaca-py StockHistoricalDataClient not available

ai_trading/alpaca_api.py:494: RuntimeError
_______________ test_get_daily_df_normalizes_alpaca_iex_columns ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d75877590>

>   ???

tests/data/test_alpaca_iex_field_aliases.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/data/fetch/__init__.py:10455: in get_daily_df
    df = _get_bars_df(
ai_trading/alpaca_api.py:827: in get_bars_df
    response = rest.get_stock_bars(req)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ai_trading.alpaca_api.get_data_client_cls.<locals>._UnavailableDataClient object at 0x7c1d50b481a0>
_a = (<tests.test_bot._DummyReq object at 0x7c1d50b4a120>,), _k = {}

    def get_stock_bars(self, *_a, **_k):
>       raise RuntimeError(self._reason)
E       RuntimeError: alpaca-py StockHistoricalDataClient not available

ai_trading/alpaca_api.py:494: RuntimeError
__________________ test_close_nan_disable_sets_quote_fallback __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d757cd310>

    def test_close_nan_disable_sets_quote_fallback(monkeypatch):
        pd = load_pandas()
        assert pd is not None
    
        fetch_module._reset_provider_auth_state_for_tests()
        fetch_module._FALLBACK_WINDOWS.clear()
        fetch_module._FALLBACK_METADATA.clear()
        fetch_module._ALPACA_CLOSE_NAN_COUNTS.clear()
    
        monitor = fetch_module.provider_monitor
        original_threshold = monitor.threshold
        original_cooldown = monitor.cooldown
        monitor.threshold = 1
        monitor.cooldown = 0
        monitor.fail_counts.clear()
        monitor.disabled_until.clear()
        monitor.disable_counts.clear()
        monitor.disabled_since.clear()
        getattr(monitor, "outage_start", {}).clear()
        getattr(monitor, "_switchover_disable_counts", {}).clear()
    
        symbol = "AAPL"
        timeframe = "1Min"
        start = datetime(2024, 1, 2, 14, 30, tzinfo=UTC)
        end = start + timedelta(minutes=1)
    
        fallback_df = pd.DataFrame(
            {
                "timestamp": pd.to_datetime(["2024-01-02T14:30:00Z"], utc=True),
                "open": [101.0],
                "high": [102.0],
                "low": [100.5],
                "close": [101.5],
                "volume": [1_000],
            }
        )
        fallback_df.attrs["data_provider"] = "yahoo"
        fallback_df.attrs["data_feed"] = "yahoo"
    
        fetch_module._mark_fallback(
            symbol,
            timeframe,
            start,
            end,
            from_provider="alpaca_iex",
            fallback_df=fallback_df,
            resolved_provider="yahoo",
            resolved_feed="yahoo",
            reason="close_column_all_nan",
        )
    
        assert monitor.is_disabled("alpaca")
        assert monitor.is_disabled("alpaca_iex")
>       assert not fetch_module.is_primary_provider_enabled()
E       assert not True
E        +  where True = <function is_primary_provider_enabled at 0x7c1d8170f920>()
E        +    where <function is_primary_provider_enabled at 0x7c1d8170f920> = fetch_module.is_primary_provider_enabled

tests/data/test_daily_fetch_fallback.py:225: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:44Z", "level": "WARNING", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_DISABLED", "provider": "alpaca_iex", "cooldown": 1.0, "disable_count": 1, "backoff_factor": 2.0, "max_cooldown": 600.0, "reason": "nan_close", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:44Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "ALPACA_TEMP_DISABLED", "provider": "alpaca", "disabled_until": "2025-10-27T23:05:45.216770+00:00", "backoff_seconds": 1.0, "disable_count": 1, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:44Z", "level": "INFO", "name": "ai_trading.data.provider_monitor", "msg": "DATA_PROVIDER_STAY | provider=yahoo reason=from_provider_disabled cooldown=0s", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:2085 BACKUP_PROVIDER_USED
WARNING  ai_trading.data.provider_monitor:provider_monitor.py:1291 DATA_PROVIDER_DISABLED
WARNING  ai_trading.data.provider_monitor:provider_monitor.py:1291 DATA_PROVIDER_DISABLED
WARNING  ai_trading.data.fetch:__init__.py:1841 ALPACA_TEMP_DISABLED
INFO     ai_trading.data.provider_monitor:provider_monitor.py:665 DATA_PROVIDER_STAY | provider=yahoo reason=from_provider_disabled cooldown=0s
__________________ test_alpaca_empty_responses_trigger_backup __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50bc9be0>

    def test_alpaca_empty_responses_trigger_backup(monkeypatch):
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        calls = {"count": 0, "feeds": []}
    
        def fake_get(*args, **kwargs):
            calls["count"] += 1
            params = kwargs.get("params") or {}
            calls["feeds"].append(params.get("feed"))
            if params.get("feed") == "iex":
                return _Resp()
            return _RespNonEmpty()
    
        monkeypatch.setattr(fetch._HTTP_SESSION, "get", fake_get)
        monkeypatch.setattr(fetch, "_ENABLE_HTTP_FALLBACK", True)
        monkeypatch.setattr(fetch, "provider_priority", lambda: ["alpaca_iex", "alpaca_sip", "yahoo"])
        monkeypatch.setattr(fetch, "max_data_fallbacks", lambda: 2)
        monkeypatch.setattr(fetch, "_symbol_exists", lambda symbol: True)
        monkeypatch.setattr(fetch, "_window_has_trading_session", lambda start, end: True)
        monkeypatch.setattr(fetch, "_outside_market_hours", lambda start, end: False)
        monkeypatch.setattr(fetch, "is_market_open", lambda: True)
        monkeypatch.setattr(fetch, "_yahoo_get_bars", lambda *args, **kwargs: _make_df())
    
        start = datetime(2024, 1, 2, 15, 30, tzinfo=UTC)
        end = start + timedelta(minutes=1)
        df = fetch._fetch_bars("AAPL", start, end, "1Min", feed="iex")
    
>       assert calls["count"] == 2
E       assert 0 == 2

tests/data/test_empty_responses.py:73: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:44Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "BACKUP_DATA_REJECTED", "provider": "yahoo", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "reason": "invalid_payload", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:6370 BACKUP_DATA_REJECTED
______________________ test_iex_ignores_sip_unauthorized _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7ece4410>

    def test_iex_ignores_sip_unauthorized(monkeypatch) -> None:
        monitor = pm.ProviderMonitor()
        monkeypatch.setattr(pm, "provider_monitor", monitor)
        monkeypatch.setattr(data_fetcher, "provider_monitor", monitor)
        monkeypatch.setattr(pm, "_SAFE_MODE_ACTIVE", False, raising=False)
        monkeypatch.setattr(pm, "_SAFE_MODE_REASON", None, raising=False)
        monkeypatch.setattr(pm, "_sip_auth_events", deque(), raising=False)
    
        monkeypatch.setenv("DATA_FEED_INTRADAY", "iex")
        monkeypatch.setenv("ALPACA_ALLOW_SIP", "1")
        monkeypatch.setenv("ALPACA_SIP_UNAUTHORIZED", "1")
        monkeypatch.setattr(data_fetcher, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(data_fetcher, "_SIP_PRECHECK_DONE", False, raising=False)
        monkeypatch.setattr(data_fetcher, "_SIP_DISALLOWED_WARNED", False, raising=False)
        monkeypatch.setattr(data_fetcher, "_SIP_UNAUTHORIZED", True, raising=False)
        monkeypatch.setattr(data_fetcher, "_SIP_UNAUTHORIZED_UNTIL", None, raising=False)
        monkeypatch.setattr(data_fetcher, "_alpaca_disabled_until", None, raising=False)
    
        class _DummyResponse:
            status_code = 401
    
        class _DummySession:
            def get(self, *args, **kwargs):
                return _DummyResponse()
    
        allowed = data_fetcher._sip_fallback_allowed(_DummySession(), {}, "1Min")
>       assert allowed is False
E       assert True is False

tests/data/test_fetch_feed_modes.py:33: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.logging:alerts.py:92 AlertManager initialized
________________ test_sip_unauthorized_branch_annotates_backup _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d75940b30>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71be30e0>

    def test_sip_unauthorized_branch_annotates_backup(monkeypatch, caplog):
        start, end = _dt_range()
        timestamps = pd.date_range(start=start, periods=5, freq="1min", tz=UTC)
        fallback_df = pd.DataFrame(
            {
                "timestamp": timestamps,
                "open": [1.0] * 5,
                "high": [1.5] * 5,
                "low": [0.5] * 5,
                "close": [1.2] * 5,
                "volume": [100] * 5,
            }
        )
    
        def _fake_yahoo(*args, **kwargs):
            return fallback_df.copy()
    
        monkeypatch.setattr(data_fetcher, "provider_monitor", _DummyProviderMonitor())
        monkeypatch.setattr(data_fetcher, "_yahoo_get_bars", _fake_yahoo)
        monkeypatch.setattr(data_fetcher, "_SIP_UNAUTHORIZED", True, raising=False)
        monkeypatch.setattr(data_fetcher, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(data_fetcher, "_sip_configured", lambda: True)
        monkeypatch.setattr(data_fetcher, "_window_has_trading_session", lambda *a, **k: True)
        monkeypatch.setattr(
            data_fetcher,
            "get_settings",
            lambda: types.SimpleNamespace(backup_data_provider="yahoo"),
        )
        monkeypatch.setenv("DATA_FEED_INTRADAY", "sip")
    
        with caplog.at_level(logging.INFO):
            result = data_fetcher.get_minute_df("AAPL", start, end, feed="sip")
    
        assert isinstance(result, pd.DataFrame)
        assert result.attrs.get("data_provider") == "yahoo"
        assert result.attrs.get("data_feed") == "yahoo"
>       assert any(
            record.message == "USING_BACKUP_PROVIDER" and getattr(record, "provider", None) == "yahoo"
            for record in caplog.records
        )
E       assert False
E        +  where False = any(<generator object test_sip_unauthorized_branch_annotates_backup.<locals>.<genexpr> at 0x7c1d71ae6b20>)

tests/data/test_get_minute_df_fetch_logging.py:83: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:45Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "ALPACA_EMPTY_BAR_BACKOFF", "symbol": "AAPL", "timeframe": "1Min", "occurrences": 3, "backoff": 1, "finnhub_enabled": false, "feed": "sip", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.data.fetch:__init__.py:4097 UNAUTHORIZED_SIP
WARNING  ai_trading.data.fetch:__init__.py:9283 ALPACA_FETCH_FAILED
WARNING  ai_trading.data.fetch:__init__.py:2085 BACKUP_PROVIDER_USED
WARNING  ai_trading.data.fetch:__init__.py:3519 ALPACA_EMPTY_BAR_BACKOFF
_______________________ test_skip_when_pdt_limit_reached _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b49580>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d50b4ad50>

    def test_skip_when_pdt_limit_reached(monkeypatch, caplog):
        guards.STATE.pdt = guards.PDTState()
        guards.STATE.shadow_cycle = False
        guards.STATE.shadow_cycle_forced = False
        engine = lt.ExecutionEngine.__new__(lt.ExecutionEngine)
        engine._refresh_settings = lambda: None
        engine._ensure_initialized = lambda: True
        engine._pre_execution_checks = lambda: True
        engine.is_initialized = True
        engine.shadow_mode = False
        engine.stats = {}
        monkeypatch.setenv("EXECUTION_DAYTRADE_LIMIT", "3")
    
        account_snapshot = {"pattern_day_trader": True, "daytrade_count": 3}
        engine._get_account_snapshot = lambda: account_snapshot
    
        called = {"preflight": False}
    
        def forbidden_preflight(*args, **kwargs):
            called["preflight"] = True
            raise AssertionError("preflight should not be called when PDT blocks")
    
        monkeypatch.setattr(lt, "preflight_capacity", forbidden_preflight)
    
        caplog.set_level(logging.INFO)
    
>       result = engine.submit_market_order("AAPL", "buy", 1)

tests/execution/test_broker_capacity_preflight.py:145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/execution/live_trading.py:1660: in submit_market_order
    capacity = _call_preflight_capacity(
ai_trading/execution/live_trading.py:610: in _call_preflight_capacity
    return fn(symbol, side, price_hint, quantity, broker, account=account_snapshot)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('AAPL', 'buy', None, 1, None)
kwargs = {'account': {'daytrade_count': 3, 'pattern_day_trader': True}}

    def forbidden_preflight(*args, **kwargs):
        called["preflight"] = True
>       raise AssertionError("preflight should not be called when PDT blocks")
E       AssertionError: preflight should not be called when PDT blocks

tests/execution/test_broker_capacity_preflight.py:139: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:45Z", "level": "INFO", "name": "ai_trading.execution.pdt", "msg": "PDT_CHECK_OK", "blocked": false, "reason": "swing_mode_entry", "context": {"closing_position": false, "symbol": "AAPL", "side": "buy", "pattern_day_trader": true, "daytrade_limit": 3, "daytrade_count": 3, "active": false, "limit": 0, "count": 0, "current_position": 0, "swing_mode_enabled": false, "block_enforced": false}, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.execution.pdt:__init__.py:1257 PDT_CHECK_OK
____________________ test_slippage_converts_market_to_limit ____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b49670>

    def test_slippage_converts_market_to_limit(monkeypatch):
        os.environ["TESTING"] = "true"
        monkeypatch.setenv("MAX_SLIPPAGE_BPS", "10")
        monkeypatch.setenv("SLIPPAGE_LIMIT_TOLERANCE_BPS", "5")
        monkeypatch.setattr("ai_trading.execution.engine.hash", lambda x: 99, raising=False)
        engine = ExecutionEngine()
        monkeypatch.setattr(engine, "_guess_price", lambda symbol: 100.0)
        oid = engine.execute_order("AAPL", OrderSide.BUY, 10)
        order = engine.order_manager.orders[oid]
>       assert order.order_type == OrderType.LIMIT
E       AssertionError: assert <OrderType.MARKET: 'market'> == <OrderType.LIMIT: 'limit'>
E        +  where <OrderType.MARKET: 'market'> = <ai_trading.execution.engine.Order object at 0x7c1d7576acc0>.order_type
E        +  and   <OrderType.LIMIT: 'limit'> = OrderType.LIMIT

tests/execution/test_execute_entry_trade_log.py:67: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:45Z", "level": "INFO", "name": "ai_trading.execution.engine", "msg": "Order submitted: d7d0d98a-db14-4edf-b3e7-0e5795b3f91a buy 10 AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:46Z", "level": "INFO", "name": "ai_trading.execution.engine", "msg": "SLIPPAGE_DIAGNOSTIC", "symbol": "AAPL", "expected_price": 100.0, "actual_price": 99.73, "slippage_bps": -27.0, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.execution.engine:engine.py:522 Order submitted: d7d0d98a-db14-4edf-b3e7-0e5795b3f91a buy 10 AAPL
INFO     ai_trading.execution.engine:engine.py:1911 SLIPPAGE_DIAGNOSTIC
_______________________ test_slippage_reduces_order_size _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b488f0>

    def test_slippage_reduces_order_size(monkeypatch):
        os.environ["TESTING"] = "true"
        monkeypatch.setenv("MAX_SLIPPAGE_BPS", "10")
        monkeypatch.setattr("ai_trading.execution.engine.hash", lambda x: 99, raising=False)
        engine = ExecutionEngine()
        oid = engine.execute_order("AAPL", OrderSide.BUY, 10)
        order = engine.order_manager.orders[oid]
>       assert order.quantity < 10
E       assert 10 < 10
E        +  where 10 = <ai_trading.execution.engine.Order object at 0x7c1d50b4b5f0>.quantity

tests/execution/test_execute_entry_trade_log.py:78: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:46Z", "level": "INFO", "name": "ai_trading.execution.engine", "msg": "Order submitted: 2ec4629d-0c86-455e-934a-0ef3f6c5543b buy 10 AAPL", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.execution.engine:engine.py:522 Order submitted: 2ec4629d-0c86-455e-934a-0ef3f6c5543b buy 10 AAPL
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.execution.engine:engine.py:1911 SLIPPAGE_DIAGNOSTIC
_________________ test_execute_order_returns_execution_result __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b4ca70>

    def test_execute_order_returns_execution_result(monkeypatch):
        engine = ExecutionEngine()
    
        def submit_and_fill(self, order):
            self.orders[order.id] = order
            self.active_orders[order.id] = order
            order.add_fill(order.quantity, 101.0)
            return SimpleNamespace(id=order.id)
    
        monkeypatch.setattr(OrderManager, "submit_order", submit_and_fill, raising=False)
        monkeypatch.setattr(ExecutionEngine, "_simulate_market_execution", lambda self, order: None, raising=False)
    
        result = engine.execute_order("AAPL", OrderSide.BUY, 5)
>       assert isinstance(result, ExecutionResult)
E       AssertionError: assert False
E        +  where False = isinstance('12babf55-cb18-4921-896e-e23c5fda976f', ExecutionResult)

tests/execution/test_execution_engine.py:36: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:46Z", "level": "INFO", "name": "ai_trading.execution.engine", "msg": "Order submitted: 12babf55-cb18-4921-896e-e23c5fda976f buy 5 AAPL", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.execution.engine:engine.py:522 Order submitted: 12babf55-cb18-4921-896e-e23c5fda976f buy 5 AAPL
_____________________ test_async_fill_triggers_risk_engine _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b4cc50>

    def test_async_fill_triggers_risk_engine(monkeypatch):
        risk = DummyRiskEngine()
        ctx = SimpleNamespace(risk_engine=risk)
        engine = ExecutionEngine(ctx=ctx)
    
        monkeypatch.setattr(OrderManager, "submit_order", _accepting_submit, raising=False)
        monkeypatch.setattr(ExecutionEngine, "_simulate_market_execution", lambda self, order: None, raising=False)
    
        signal = TradeSignal(
            symbol="AAPL",
            side="buy",
            confidence=1.0,
            strategy="s",
            weight=0.5,
            asset_class="equity",
        )
    
        result = engine.execute_order(
            "AAPL",
            OrderSide.BUY,
            10,
            signal=signal,
            signal_weight=signal.weight,
        )
>       assert isinstance(result, ExecutionResult)
E       AssertionError: assert False
E        +  where False = isinstance('8049c18a-35ae-4ccd-9ab6-31c0d7f5e326', ExecutionResult)

tests/execution/test_execution_engine.py:67: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.execution.engine", "msg": "Order submitted: 8049c18a-35ae-4ccd-9ab6-31c0d7f5e326 buy 10 AAPL", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
INFO     ai_trading.execution.engine:engine.py:522 Order submitted: 8049c18a-35ae-4ccd-9ab6-31c0d7f5e326 buy 10 AAPL
___________________________ test_empty_minute_raises ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50beb8c0>

    def test_empty_minute_raises(monkeypatch):
        """fetch_minute_df_safe should raise when providers return empty data."""
        monkeypatch.setattr(
            "ai_trading.core.bot_engine.get_minute_df",
            lambda *a, **k: pd.DataFrame(),
        )
        with pytest.raises(DataFetchError):
>           fetch_minute_df_safe("AAPL")

tests/execution/test_fetch_minute_df_safe_empty.py:13: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ai_trading/core/bot_engine.py:5344: in fetch_minute_df_safe
    df = _sanitize_minute_df(df, symbol=symbol, current_now=now_utc)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

raw_df = Empty DataFrame
Columns: []
Index: []

    def _sanitize_minute_df(
        raw_df: pd.DataFrame,
        *,
        symbol: str,
        current_now: datetime,
    ) -> pd.DataFrame:
        try:
            raw_attrs = dict(getattr(raw_df, "attrs", {}) or {})
        except COMMON_EXC:
            raw_attrs = {}
        df = raw_df.copy()
        if raw_attrs:
            try:
                df.attrs.update(raw_attrs)
            except COMMON_EXC:  # pragma: no cover - metadata best-effort only
                pass
        # Drop bars with zero volume or from the current (incomplete) minute
        try:
            current_minute = current_now.replace(second=0, microsecond=0)
            ts_col = "timestamp" if "timestamp" in df.columns else None
            if ts_col is not None:
                df = df[df[ts_col] < current_minute]
            else:
                df = df[df.index < current_minute]
            if "volume" in df.columns:
                volume_series = pd.to_numeric(df["volume"], errors="coerce")
                df = df.loc[volume_series.notna()]
                volume_series = volume_series.loc[df.index]
                provider_hint = str(
                    raw_attrs.get("fallback_provider")
                    or raw_attrs.get("data_provider")
                    or ""
                ).strip().lower()
                allow_zero_volume = provider_hint in {
                    "yahoo",
                    "finnhub",
                    "finnhub_low_latency",
                }
                if allow_zero_volume:
                    non_negative = volume_series >= 0
                else:
                    non_negative = volume_series > 0
                df = df.loc[non_negative]
                volume_series = volume_series.loc[df.index]
                try:
                    df.loc[:, "volume"] = volume_series
                except COMMON_EXC:
                    df["volume"] = volume_series.to_numpy()
        except (*COMMON_EXC, AttributeError) as exc:  # pragma: no cover - defensive
            logger.debug("minute bar filtering failed: %s", exc)
    
        if df.empty:
            msg = (
                "Minute bars DataFrame is empty after fallbacks; market likely closed"
            )  # AI-AGENT-REF
            logger.warning(
                "FETCH_MINUTE_EMPTY",
                extra={"reason": "empty", "context": "market_closed"},
            )
>           raise DataFetchError(msg)
E           ai_trading.data.fetch.DataFetchError: Minute bars DataFrame is empty after fallbacks; market likely closed

ai_trading/core/bot_engine.py:5151: DataFetchError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=10 key=\"EMPTY_BARS_DETECTED\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "EMPTY_BARS_DETECTED", "symbol": "AAPL", "timeframe": "1Min", "feed": "iex", "attempt": 1, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=4 key=\"ALPACA_FEED_SWITCH\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "ALPACA_FEED_SWITCH", "provider": "alpaca", "from": "iex", "to": "sip", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "alpaca_sip", "from_provider": "alpaca_iex", "from_feed": "iex", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "fallback": ["1Min", "sip", "2025-10-24T13:30:00+00:00", "2025-10-24T20:00:00+00:00"], "start": "2025-10-24T13:30:00+00:00", "end": "2025-10-24T20:00:00+00:00", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "WARNING", "name": "ai_trading.utils.base", "msg": "No market schedule for 2025-10-24 in is_market_open (likely holiday); returning False.", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "WARNING", "name": "ai_trading.logging", "msg": "FETCH_ATTEMPT", "provider": "alpaca", "remaining_retries": 4, "url": "https://data.alpaca.markets/v2/stocks/AAPL/bars", "symbol": "AAPL", "feed": "sip", "timeframe": "1Min", "params": {"start": "2025-10-24T13:30:00+00:00", "end": "2025-10-24T20:00:00+00:00", "timeframe": "1Min", "feed": "sip", "adjustment": "raw", "limit": 10000}, "correlation_id": null, "status": 599, "error": "empty", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=3 key=\"PERSISTENT_EMPTY_ABORT\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "PERSISTENT_EMPTY_ABORT", "symbol": "AAPL", "timeframe": "1Min", "attempts": 2, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.utils.base", "msg": "LOG_THROTTLE_SUMMARY | suppressed=6 key=\"NO_MARKET_SCHEDULE_FOR_2025_10_27_IN_IS_MARKET_OPEN_LIKELY_HOLIDAY_RETURNING_FALSE\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "WARNING", "name": "ai_trading.utils.base", "msg": "No market schedule for 2025-10-27 in is_market_open (likely holiday); returning False.", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=3 key=\"ALPACA_FETCH_ABORTED\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "ALPACA_FETCH_ABORTED", "provider": "alpaca", "status": "empty", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "start": "2025-10-24T13:30:00+00:00", "end": "2025-10-24T20:00:00+00:00", "correlation_id": null, "retries": 0, "remaining_retries": 4, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.utils.base", "msg": "LOG_THROTTLE_SUMMARY | suppressed=3 key=\"NO_MARKET_SCHEDULE_FOR_2025_10_24_IN_IS_MARKET_OPEN_LIKELY_HOLIDAY_RETURNING_FALSE\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "yahoo", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "ALPACA_FETCH_MARKET_CLOSED", "provider": "alpaca", "status": "market_closed", "feed": "sip", "timeframe": "1Min", "symbol": "AAPL", "start": "2025-10-24T13:30:00+00:00", "end": "2025-10-24T20:00:00+00:00", "correlation_id": null, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "ALPACA_EMPTY_BAR_MARKET_CLOSED", "symbol": "AAPL", "timeframe": "1Min", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=10 key="EMPTY_BARS_DETECTED"
INFO     ai_trading.data.fetch:__init__.py:7747 EMPTY_BARS_DETECTED
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=4 key="ALPACA_FEED_SWITCH"
INFO     ai_trading.data.fetch:__init__.py:7769 ALPACA_FEED_SWITCH
INFO     ai_trading.data.fetch:__init__.py:7070 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:7747 EMPTY_BARS_DETECTED
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-24 in is_market_open (likely holiday); returning False.
WARNING  ai_trading.logging:__init__.py:1313 FETCH_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=3 key="PERSISTENT_EMPTY_ABORT"
WARNING  ai_trading.data.fetch:__init__.py:8022 PERSISTENT_EMPTY_ABORT
INFO     ai_trading.utils.base:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=6 key="NO_MARKET_SCHEDULE_FOR_2025_10_27_IN_IS_MARKET_OPEN_LIKELY_HOLIDAY_RETURNING_FALSE"
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-27 in is_market_open (likely holiday); returning False.
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=3 key="ALPACA_FETCH_ABORTED"
WARNING  ai_trading.data.fetch:__init__.py:8072 ALPACA_FETCH_ABORTED
INFO     ai_trading.utils.base:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=3 key="NO_MARKET_SCHEDULE_FOR_2025_10_24_IN_IS_MARKET_OPEN_LIKELY_HOLIDAY_RETURNING_FALSE"
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-24 in is_market_open (likely holiday); returning False.
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
INFO     ai_trading.data.fetch:__init__.py:8631 ALPACA_FETCH_MARKET_CLOSED
WARNING  ai_trading.data.fetch:__init__.py:3519 FETCH_MINUTE_EMPTY
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-27 in is_market_open (likely holiday); returning False.
INFO     ai_trading.data.fetch:__init__.py:3519 ALPACA_EMPTY_BAR_MARKET_CLOSED
WARNING  ai_trading.core.bot_engine:bot_engine.py:5147 FETCH_MINUTE_EMPTY
______ test_submit_limit_order_handles_timeout_without_unboundlocalerror _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b6bf80>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d52f21af0>

    def test_submit_limit_order_handles_timeout_without_unboundlocalerror(monkeypatch, caplog):
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(live_trading, "get_alpaca_creds", lambda: ("key", "secret"))
        monkeypatch.setattr(live_trading, "get_tick_size", lambda symbol: Decimal("0.01"))
        monkeypatch.setattr(
            live_trading,
            "_call_preflight_capacity",
            lambda *args, **kwargs: SimpleNamespace(can_submit=True, suggested_qty=args[3]),
        )
        monkeypatch.setattr(live_trading, "_safe_mode_guard", lambda *args, **kwargs: False)
        monkeypatch.setattr(live_trading.ExecutionEngine, "_refresh_settings", lambda self: None)
        monkeypatch.setattr(live_trading.ExecutionEngine, "_get_account_snapshot", lambda self: {})
        monkeypatch.setattr(
            live_trading.ExecutionEngine,
            "_should_skip_for_pdt",
            lambda self, *a, **k: (False, "", {}),
        )
    
        engine = live_trading.ExecutionEngine()
        engine.is_initialized = True
        engine.trading_client = object()
        engine._ensure_initialized = lambda: True
        engine._pre_execution_checks = lambda: True
    
        def _raise_timeout(*args, **kwargs):
            raise TimeoutError("provider timeout")
    
        engine._execute_with_retry = _raise_timeout  # type: ignore[assignment]
    
        with caplog.at_level("ERROR"):
            result = engine.submit_limit_order("AAPL", "buy", 1, limit_price=123.45)
    
        assert result is None
>       assert any(record.message == "ORDER_SUBMIT_RETRIES_EXHAUSTED" for record in caplog.records)
E       assert False
E        +  where False = any(<generator object test_submit_limit_order_handles_timeout_without_unboundlocalerror.<locals>.<genexpr> at 0x7c1d7d33f100>)

tests/execution/test_live_trading_errors.py:54: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:47Z", "level": "INFO", "name": "ai_trading.execution.live_trading", "msg": "LOG_THROTTLE_SUMMARY | suppressed=4 key=\"EXECUTIONENGINE_INITIALIZED\"", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.execution.live_trading:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=4 key="EXECUTIONENGINE_INITIALIZED"
INFO     ai_trading.execution.live_trading:live_trading.py:1108 ExecutionEngine initialized
____________________ test_manual_price_slippage_adjustment _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d714660c0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71d2ab70>

    def test_manual_price_slippage_adjustment(monkeypatch, caplog):
        """Manual price orders should adjust without raising assertion errors."""
    
        os.environ["TESTING"] = "true"
        monkeypatch.setenv("MAX_SLIPPAGE_BPS", "10")
        monkeypatch.setenv("SLIPPAGE_LIMIT_TOLERANCE_BPS", "5")
        monkeypatch.setattr("ai_trading.execution.engine.hash", lambda _: 99, raising=False)
    
        engine = ExecutionEngine()
        monkeypatch.setattr(engine, "_guess_price", lambda symbol: 100.0)
    
        caplog.set_level("WARNING")
    
        order_id = engine.execute_order(
            "AAPL",
            OrderSide.BUY,
            10,
            order_type=OrderType.MARKET,
            price=100.0,
        )
    
        order = engine.order_manager.orders[order_id]
    
>       assert order.order_type == OrderType.LIMIT
E       AssertionError: assert <OrderType.MARKET: 'market'> == <OrderType.LIMIT: 'limit'>
E        +  where <OrderType.MARKET: 'market'> = <ai_trading.execution.engine.Order object at 0x7c1d71d57ec0>.order_type
E        +  and   <OrderType.LIMIT: 'limit'> = OrderType.LIMIT

tests/execution/test_manual_price_slippage.py:69: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:48Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "AAPL", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
________________ test_buy_slippage_improvement_allows_execution ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71d28230>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d52f210a0>

    def test_buy_slippage_improvement_allows_execution(monkeypatch, caplog):
        """A buy order with better-than-expected price should proceed."""
    
        engine = _make_engine(monkeypatch, hash_value=0)
    
        with caplog.at_level("WARNING"):
            result = engine.execute_order(
                "AAPL",
                OrderSide.BUY,
                10,
                order_type=OrderType.MARKET,
                price=100.0,
            )
    
        order = engine.order_manager.orders[str(result)]
>       assert order.order_type == OrderType.MARKET
E       AssertionError: assert <OrderType.LIMIT: 'limit'> == <OrderType.MARKET: 'market'>
E        +  where <OrderType.LIMIT: 'limit'> = <ai_trading.execution.engine.Order object at 0x7c1d52f21790>.order_type
E        +  and   <OrderType.MARKET: 'market'> = OrderType.MARKET

tests/execution/test_manual_price_slippage.py:92: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.execution.engine", "msg": "SLIPPAGE_LIMIT_CONVERSION", "order_id": "c3b88293-8507-4e8e-8bb0-95d2061313bd", "symbol": "AAPL", "side": "buy", "predicted_slippage_bps": 47.0, "threshold_bps": 10.0, "tolerance_bps": 5.0, "limit_price": 100.05, "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.execution.engine", "msg": "SLIPPAGE_QTY_REDUCED", "order_id": "c3b88293-8507-4e8e-8bb0-95d2061313bd", "symbol": "AAPL", "original_qty": 10, "adjusted_qty": 2, "reduction_scale": 0.212766, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
WARNING  ai_trading.execution.engine:engine.py:1804 SLIPPAGE_LIMIT_CONVERSION
WARNING  ai_trading.execution.engine:engine.py:1825 SLIPPAGE_QTY_REDUCED
____________ test_adverse_slippage_still_triggers_controls[99-buy] _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d52f20cb0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71d2bf20>
hash_value = 99, side = <OrderSide.BUY: 'buy'>

    @pytest.mark.parametrize("hash_value, side", [(99, OrderSide.BUY), (0, OrderSide.SELL)])
    def test_adverse_slippage_still_triggers_controls(monkeypatch, caplog, hash_value, side):
        """Directional slippage against the order should still be mitigated."""
    
        engine = _make_engine(monkeypatch, hash_value=hash_value)
    
        with caplog.at_level("WARNING"):
            result = engine.execute_order(
                "AAPL",
                side,
                10,
                order_type=OrderType.MARKET,
                price=100.0,
            )
    
        order = engine.order_manager.orders[str(result)]
>       assert order.order_type == OrderType.LIMIT
E       AssertionError: assert <OrderType.MARKET: 'market'> == <OrderType.LIMIT: 'limit'>
E        +  where <OrderType.MARKET: 'market'> = <ai_trading.execution.engine.Order object at 0x7c1d71d282c0>.order_type
E        +  and   <OrderType.LIMIT: 'limit'> = OrderType.LIMIT

tests/execution/test_manual_price_slippage.py:139: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
____________ test_adverse_slippage_still_triggers_controls[0-sell] _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71d28050>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d50b346b0>
hash_value = 0, side = <OrderSide.SELL: 'sell'>

    @pytest.mark.parametrize("hash_value, side", [(99, OrderSide.BUY), (0, OrderSide.SELL)])
    def test_adverse_slippage_still_triggers_controls(monkeypatch, caplog, hash_value, side):
        """Directional slippage against the order should still be mitigated."""
    
        engine = _make_engine(monkeypatch, hash_value=hash_value)
    
        with caplog.at_level("WARNING"):
            result = engine.execute_order(
                "AAPL",
                side,
                10,
                order_type=OrderType.MARKET,
                price=100.0,
            )
    
        order = engine.order_manager.orders[str(result)]
>       assert order.order_type == OrderType.LIMIT
E       AssertionError: assert <OrderType.MARKET: 'market'> == <OrderType.LIMIT: 'limit'>
E        +  where <OrderType.MARKET: 'market'> = <ai_trading.execution.engine.Order object at 0x7c1d50b376e0>.order_type
E        +  and   <OrderType.LIMIT: 'limit'> = OrderType.LIMIT

tests/execution/test_manual_price_slippage.py:139: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
__________________ test_execute_entry_uses_config_max_factor ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b36990>

    def test_execute_entry_uses_config_max_factor(monkeypatch):
        monkeypatch.setattr(bot_engine, "submit_order", lambda *a, **k: None)
        monkeypatch.setattr(bot_engine, "vwap_pegged_submit", lambda *a, **k: None)
        monkeypatch.setattr(bot_engine, "pov_submit", lambda *a, **k: None)
        monkeypatch.setattr(bot_engine, "POV_SLICE_PCT", 0)
        monkeypatch.setattr(bot_engine, "SLICE_THRESHOLD", 10)
        df = pd.DataFrame({"close": [100.0], "atr": [1.0]})
        monkeypatch.setattr(bot_engine, "fetch_minute_df_safe", lambda symbol: df)
        monkeypatch.setattr(bot_engine, "prepare_indicators", lambda raw: raw)
        monkeypatch.setattr(bot_engine, "get_latest_close", lambda df: float(df["close"].iloc[-1]))
        monkeypatch.setattr(bot_engine, "is_high_vol_regime", lambda: False)
        captured = {}
        def fake_scaled_atr_stop(entry, atr, now, mo, mc, max_factor, min_factor):
            captured["max_factor"] = max_factor
            return (entry - 1, entry + 1)
        monkeypatch.setattr(bot_engine, "scaled_atr_stop", fake_scaled_atr_stop)
        monkeypatch.setattr(bot_engine, "targets_lock", threading.Lock())
        monkeypatch.setenv("TAKE_PROFIT_FACTOR", "3.0")
    
        ctx = SimpleNamespace(
            api=SimpleNamespace(get_account=lambda: SimpleNamespace(buying_power="10000")),
            trade_logger=None,
            market_open=time(9, 30),
            market_close=time(16, 0),
            stop_targets={},
            take_profit_targets={},
        )
    
        execution_flow.execute_entry(ctx, "AAPL", 1, "buy")
>       assert captured["max_factor"] == 3.0
E       KeyError: 'max_factor'

tests/execution/test_max_factor_config.py:38: KeyError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.core.bot_engine", "msg": "MARKET_CLOSED_ORDER_SKIP", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.data.fetch", "msg": "FETCH_MINUTE_EMPTY", "provider": "alpaca_iex", "timeframe": "1Min", "reason": "empty_bars", "detail": "alpaca_empty: symbol=AAPL, timeframe=1Min, feed=sip, reason=market_closed", "symbol": "AAPL", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.core.execution_flow", "msg": "NO_MINUTE_BARS_POST_ENTRY", "symbol": "AAPL", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.core.execution_flow:execution_flow.py:619 MARKET_ENTRY
WARNING  ai_trading.core.bot_engine:bot_engine.py:14917 MARKET_CLOSED_ORDER_SKIP
INFO     ai_trading.data.fetch:__init__.py:7747 EMPTY_BARS_DETECTED
INFO     ai_trading.data.fetch:__init__.py:7769 ALPACA_FEED_SWITCH
INFO     ai_trading.data.fetch:__init__.py:7070 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:7747 EMPTY_BARS_DETECTED
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-24 in is_market_open (likely holiday); returning False.
WARNING  ai_trading.logging:__init__.py:1313 FETCH_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:8022 PERSISTENT_EMPTY_ABORT
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-27 in is_market_open (likely holiday); returning False.
WARNING  ai_trading.data.fetch:__init__.py:8072 ALPACA_FETCH_ABORTED
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-24 in is_market_open (likely holiday); returning False.
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
INFO     ai_trading.data.fetch:__init__.py:8631 ALPACA_FETCH_MARKET_CLOSED
WARNING  ai_trading.data.fetch:__init__.py:3519 FETCH_MINUTE_EMPTY
WARNING  ai_trading.utils.base:base.py:387 No market schedule for 2025-10-27 in is_market_open (likely holiday); returning False.
INFO     ai_trading.data.fetch:__init__.py:3519 ALPACA_EMPTY_BAR_MARKET_CLOSED
WARNING  ai_trading.core.bot_engine:bot_engine.py:5147 FETCH_MINUTE_EMPTY
WARNING  ai_trading.core.execution_flow:execution_flow.py:625 NO_MINUTE_BARS_POST_ENTRY
____________ test_scheduler_logs_and_continues_after_runtime_error _____________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71ddf9e0>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d71ddd940>

    def test_scheduler_logs_and_continues_after_runtime_error(monkeypatch, caplog):
        monkeypatch.setenv("SCHEDULER_ITERATIONS", "2")
        monkeypatch.setenv("ALLOW_AFTER_HOURS", "1")
    
        settings = types.SimpleNamespace(
            alpaca_data_feed="iex",
            alpaca_adjustment="raw",
            iterations=None,
            scheduler_iterations=0,
            interval=1,
            interval_when_closed=1,
            api_port=9001,
            api_port_wait_seconds=0,
            health_tick_seconds=60,
            max_position_mode="STATIC",
            max_position_size=1.0,
            http_connect_timeout=1.0,
            http_read_timeout=1.0,
            http_pool_maxsize=1,
            http_total_retries=1,
            http_backoff_factor=0.1,
            alpaca_base_url="https://paper-api.example.com",
            capital_cap=0.1,
            dollar_risk_limit=0.1,
            paper=True,
            trading_mode="balanced",
        )
    
        monkeypatch.setattr(main, "_check_alpaca_sdk", lambda: None)
        monkeypatch.setattr(main, "_fail_fast_env", lambda: None)
        monkeypatch.setattr(main, "get_settings", lambda: settings)
        monkeypatch.setattr(main, "_assert_singleton_api", lambda *_a, **_k: None)
        monkeypatch.setattr(main, "_init_http_session", lambda *_a, **_k: True)
        monkeypatch.setattr(main, "ensure_trade_log_path", lambda: None)
        monkeypatch.setattr(main, "preflight_import_health", lambda: None)
        monkeypatch.setattr(main, "ensure_dotenv_loaded", lambda: None)
        monkeypatch.setattr(main, "_is_market_open_base", lambda: True)
        monkeypatch.setattr(main, "optimize_memory", lambda: {})
        monkeypatch.setattr(main, "resolve_max_position_size", lambda *a, **k: (1.0, {}))
        monkeypatch.setattr(main.time, "sleep", lambda *_a, **_k: None)
        monkeypatch.setattr(main, "_interruptible_sleep", lambda *_a, **_k: None)
        monkeypatch.setattr(main, "get_histogram", lambda *a, **k: _DummyMetric())
        monkeypatch.setattr(main, "get_counter", lambda *a, **k: _DummyMetric())
    
        def _start_api_with_signal(api_ready, _api_error):
            api_ready.set()
    
        monkeypatch.setattr(main, "start_api_with_signal", _start_api_with_signal)
    
        class _InlineThread:
            def __init__(self, target, args=(), kwargs=None, daemon=None):
                self._target = target
                self._args = args
                self._kwargs = kwargs or {}
                self.daemon = daemon
    
            def start(self):
                self._target(*self._args, **self._kwargs)
    
            def is_alive(self):
                return False
    
        monkeypatch.setattr(main, "Thread", _InlineThread)
    
        calls = {"count": 0}
    
        def _run_cycle_stub():
            calls["count"] += 1
            if calls["count"] == 2:
                raise RuntimeError("boom")
    
        monkeypatch.setattr(main, "run_cycle", _run_cycle_stub)
    
        with caplog.at_level(logging.ERROR, logger=main.logger.name):
            main.main(["--iterations", "2", "--interval", "1"])
    
>       assert calls["count"] == 3  # warm-up + two scheduler iterations
E       assert 0 == 3

tests/integration/test_main_scheduler_resilience.py:94: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:05:48Z", "level": "INFO", "name": "ai_trading.position_sizing", "msg": "LOG_THROTTLE_SUMMARY | suppressed=9 key=\"ALPACA_EQUITY_UNAVAILABLE\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:05:48Z", "level": "WARNING", "name": "ai_trading.position_sizing", "msg": "ALPACA_EQUITY_UNAVAILABLE", "reason": "missing_credentials", "has_key": false, "has_secret": "***", "base_url": "https://paper-api.example.com", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.position_sizing:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=9 key="ALPACA_EQUITY_UNAVAILABLE"
WARNING  ai_trading.position_sizing:position_sizing.py:250 ALPACA_EQUITY_UNAVAILABLE
______________________ test_max_position_size_consistency ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d52ffbf20>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d50b37770>

    def test_max_position_size_consistency(monkeypatch, caplog):
        """Bot uses resolved max position size consistently at runtime."""
        # Minimal env so TradingConfig.from_env works
        monkeypatch.setenv("ALPACA_API_KEY", "k")
        monkeypatch.setenv("ALPACA_SECRET_KEY", "s")
        monkeypatch.setenv("ALPACA_API_URL", "https://paper-api.alpaca.markets")
        monkeypatch.setenv("ALPACA_DATA_FEED", "iex")
        monkeypatch.setenv("WEBHOOK_SECRET", "wh")
        monkeypatch.setenv("CAPITAL_CAP", "0.04")
        monkeypatch.setenv("DOLLAR_RISK_LIMIT", "0.05")
        monkeypatch.setenv("SCHEDULER_ITERATIONS", "1")
        monkeypatch.delenv("MAX_POSITION_SIZE", raising=False)
        monkeypatch.delenv("AI_TRADING_MAX_POSITION_SIZE", raising=False)
    
        # Avoid external side effects
        monkeypatch.setattr(main, "_fail_fast_env", lambda: None)
        monkeypatch.setattr(main, "_init_http_session", lambda cfg: True)
        monkeypatch.setattr(main, "start_api_with_signal", lambda ready, err: ready.set())
        monkeypatch.setattr(main, "optimize_memory", lambda: {})
        monkeypatch.setattr(main, "StageTimer", lambda *a, **k: contextlib.nullcontext())
        monkeypatch.setattr(main, "_interruptible_sleep", lambda s: None)
        monkeypatch.setattr(main, "_get_equity_from_alpaca", lambda cfg: 100000, raising=False)
        monkeypatch.setattr(
            "ai_trading.core.runtime._get_equity_from_alpaca", lambda cfg, force_refresh=True: 100000
        )
        monkeypatch.setattr("ai_trading.utils.device.get_device", lambda: None)
    
        from ai_trading.config.management import TradingConfig
        from ai_trading.core.runtime import build_runtime
        from ai_trading.config import get_settings
    
        captured = {}
    
        def fake_run_cycle():
            cfg = TradingConfig.from_env()
            S = get_settings()
            mps = getattr(S, "max_position_size", None)
            if mps is not None:
                try:
                    object.__setattr__(cfg, "max_position_size", float(mps))
                except Exception:
                    try:
                        setattr(cfg, "max_position_size", float(mps))
                    except Exception:
                        pass
            runtime = build_runtime(cfg)
            captured["runtime"] = runtime
    
        monkeypatch.setattr(main, "run_cycle", fake_run_cycle)
    
        with caplog.at_level(logging.INFO):
            main.main([])
    
>       runtime = captured["runtime"]
E       KeyError: 'runtime'

tests/runtime/test_max_position_size_consistency.py:63: KeyError
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.main:main.py:281 ALPACA_PY_SKIPPED_UNDER_TEST
WARNING  ai_trading.main:main.py:1428 MARKET_CLOSED_SLEEP
INFO     ai_trading.main:main.py:1472 DATA_CONFIG feed=iex adjustment=all timeframe=1Day/1Min provider=alpaca
INFO     ai_trading.main:main.py:1486 SERVICE_SHUTDOWN
_______________________ test_no_broad_except_in_hotpaths _______________________

    def test_no_broad_except_in_hotpaths():
        p = subprocess.run([sys.executable, "tools/audit_exceptions.py", "--paths", *HOTPATHS], capture_output=True, text=True)
        assert p.returncode == 0
        data = json.loads(p.stdout.splitlines()[0])
        by_file = data.get("by_file", {})
        offenders = {f: hits for f, hits in by_file.items() if hits}
>       assert offenders == {}, f"broad except present in: {list(offenders)}"
E       AssertionError: broad except present in: ['ai_trading/core/bot_engine.py']
E       assert {'ai_trading/...    pass\n'}]} == {}
E         Left contains 1 more item:
E         {'ai_trading/core/bot_engine.py': [{'file': 'ai_trading/core/bot_engine.py',
E                                             'line': 7771,
E                                             'snippet': '    except Exception:\n'
E                                                        '        pass\n'},
E                                            {'file': 'ai_trading/core/bot_engine.py',
E                                             'line': 13354,...
E         
E         ...Full output truncated (11 lines hidden), use '-vv' to show

tests/runtime/test_no_broad_except_in_hotpaths.py:22: AssertionError
_______________ test_start_api_with_signal_sets_error_on_failure _______________

    def test_start_api_with_signal_sets_error_on_failure():
        """If start_api raises, the error flag should be set."""
        api_ready = Event()
        api_error = Event()
        with patch("ai_trading.main.start_api", side_effect=RuntimeError):
            start_api_with_signal(api_ready, api_error)
>       assert api_error.is_set()
E       assert False
E        +  where False = <bound method Event.is_set of <threading.Event at 0x7c1d52ffbf50: unset>>()
E        +    where <bound method Event.is_set of <threading.Event at 0x7c1d52ffbf50: unset>> = <threading.Event at 0x7c1d52ffbf50: unset>.is_set

tests/runtime/test_runtime_behavior.py:22: AssertionError
_________________ test_log_trade_permission_error_triggers_fix _________________

    def test_log_trade_permission_error_triggers_fix():
        """log_trade should attempt to fix permissions when a PermissionError occurs."""
        with patch("ai_trading.audit._ensure_file_header", lambda p, h: None), \
             patch("ai_trading.audit.open", side_effect=PermissionError), \
             patch("ai_trading.audit.fix_file_permissions", return_value=True) as mock_fix:
            audit.log_trade("AAPL", 1, "buy", 100.0, timestamp="2024-01-01")
>           mock_fix.assert_called_once()

tests/unit/test_audit_log_trade.py:12: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='fix_file_permissions' id='136465387754976'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'fix_file_permissions' to have been called once. Called 0 times.

/usr/lib/python3.12/unittest/mock.py:923: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:06:10Z", "level": "INFO", "name": "ai_trading.logging", "msg": "Logging configured successfully - no duplicates possible", "bot_phase": "GENERAL"}
______________ test_sip_fallback_skipped_when_marked_unauthorized ______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d50b48770>

    def test_sip_fallback_skipped_when_marked_unauthorized(monkeypatch: pytest.MonkeyPatch):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", True, raising=False)
        calls = {"count": 0}
    
        def fake_get(url, params=None, headers=None, timeout=None):
            calls["count"] += 1
            return _Resp(429, payload={"message": "rate limit"})
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
    
        start, end = _dt_range(2)
>       with pytest.raises(ValueError, match="rate_limited"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/unit/test_data_fetcher_http.py:95: Failed
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:06:10Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "DATA_SOURCE_FALLBACK_ATTEMPT", "provider": "yahoo", "from_provider": "alpaca_iex", "feed": "sip", "timeframe": "1Min", "symbol": "TEST", "interval": "1m", "start": "2025-10-27T23:04:00+00:00", "end": "2025-10-27T23:05:00+00:00", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:06:10Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "LOG_THROTTLE_SUMMARY | suppressed=7 key=\"USING_BACKUP_PROVIDER\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:06:10Z", "level": "INFO", "name": "ai_trading.data.fetch", "msg": "USING_BACKUP_PROVIDER", "provider": "yahoo", "symbol": "TEST", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
INFO     ai_trading.data.fetch:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=7 key="USING_BACKUP_PROVIDER"
INFO     ai_trading.data.fetch:__init__.py:5137 USING_BACKUP_PROVIDER
________________________ test_timeout_triggers_fallback ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71c49c10>

    def test_timeout_triggers_fallback(monkeypatch: pytest.MonkeyPatch):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        calls = {"count": 0}
    
        class _Timeout(df.Timeout):
            pass
    
        def fake_get(url, params=None, headers=None, timeout=None):
            calls["count"] += 1
            feed = (params or {}).get("feed")
            if calls["count"] == 1 and feed == "sip":
                raise _Timeout("timeout")
            ts_iso = datetime.now(UTC).isoformat()
            return _Resp(200, payload=_bars_payload(ts_iso))
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get)
    
        start, end = _dt_range(2)
        out = df.get_bars("TEST", timeframe="1Min", start=start, end=end, feed="sip", adjustment="raw")
        assert isinstance(out, pd.DataFrame) and not out.empty
>       assert calls["count"] >= 2
E       assert 0 >= 2

tests/unit/test_data_fetcher_http.py:196: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
____________________ test_429_rate_limit_triggers_fallback _____________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71c4a7e0>

    def test_429_rate_limit_triggers_fallback(monkeypatch: pytest.MonkeyPatch):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        calls = {"count": 0}
    
        def fake_get(url, params=None, headers=None, timeout=None):
            calls["count"] += 1
            feed = (params or {}).get("feed")
            ts_iso = datetime.now(UTC).isoformat()
            if calls["count"] == 1 and feed == "sip":
                return _Resp(429, payload={"message": "rate limit"})
            return _Resp(200, payload=_bars_payload(ts_iso))
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get)
    
        start, end = _dt_range(2)
        out = df.get_bars("TEST", timeframe="1Min", start=start, end=end, feed="sip", adjustment="raw")
        assert isinstance(out, pd.DataFrame) and not out.empty
>       assert calls["count"] >= 2
E       assert 0 >= 2

tests/unit/test_data_fetcher_http.py:343: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
___________________________ test_empty_bars_fallback ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d7d335fa0>

    def test_empty_bars_fallback(monkeypatch: pytest.MonkeyPatch):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        calls = {"count": 0}
    
        def fake_get(url, params=None, headers=None, timeout=None):
            calls["count"] += 1
            ts_iso = datetime.now(UTC).isoformat()
            if calls["count"] == 1:
                return _Resp(200, payload={"bars": []})
            return _Resp(200, payload=_bars_payload(ts_iso))
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get)
    
        start, end = _dt_range(2)
        out = df.get_bars("TEST", timeframe="1Min", start=start, end=end, feed="iex", adjustment="raw")
>       assert isinstance(out, pd.DataFrame) and not out.empty
E       AssertionError: assert (False)
E        +  where False = isinstance(None, <class 'pandas.core.frame.DataFrame'>)
E        +    where <class 'pandas.core.frame.DataFrame'> = pd.DataFrame

tests/unit/test_data_fetcher_http.py:361: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
___________________ test_sip_fallback_precheck_skips_request ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d71c48c20>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d52f60ce0>

    def test_sip_fallback_precheck_skips_request(monkeypatch: pytest.MonkeyPatch, caplog: pytest.LogCaptureFixture):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(df, "_SIP_PRECHECK_DONE", False, raising=False)
        monkeypatch.setattr(df, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(df, "_SIP_UNAVAILABLE_LOGGED", set(), raising=False)
        feeds: list[str | None] = []
    
        def fake_get(url, params=None, headers=None, timeout=None):  # noqa: ARG001
            feed = (params or {}).get("feed")
            feeds.append(feed)
            if feed == "iex":
                return _Resp(429, payload={"message": "rate limit"})
            return _Resp(403, payload={"message": "auth"})
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get)
        start, end = _dt_range(2)
        with caplog.at_level("WARNING"):
>           with pytest.raises(ValueError, match="rate_limited"):
E           Failed: DID NOT RAISE <class 'ValueError'>

tests/unit/test_data_fetcher_http.py:382: Failed
_____________________ test_rate_limit_disable_and_recover ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d52f61b20>

    def test_rate_limit_disable_and_recover(monkeypatch: pytest.MonkeyPatch):
        from ai_trading.data.fetch.metrics import (
            inc_provider_disable_total,
            provider_disabled,
        )
        from ai_trading.config import settings as config_settings
    
        monkeypatch.setenv("BACKUP_DATA_PROVIDER", "yahoo")
        config_settings.get_settings.cache_clear()
        monkeypatch.setattr(df, "_FALLBACK_WINDOWS", set(), raising=False)
        monkeypatch.setattr(df, "_FALLBACK_UNTIL", {}, raising=False)
        monkeypatch.setattr(df, "_alpaca_disabled_until", None, raising=False)
        monkeypatch.setattr(df.provider_monitor, "threshold", 1, raising=False)
        monkeypatch.setattr(df.provider_monitor, "cooldown", 1, raising=False)
        df.provider_monitor.fail_counts.clear()
        df.provider_monitor.disabled_until.clear()
        calls = {"alpaca": 0, "backup": 0}
    
        def rate_limit_resp(url, params=None, headers=None, timeout=None):
            calls["alpaca"] += 1
            return _Resp(429, payload={"message": "rate limit"}, headers={"Retry-After": "1"})
    
        def backup_resp(symbol, start, end, interval):
            calls["backup"] += 1
            ts = datetime.now(UTC)
            return pd.DataFrame(
                {
                    "timestamp": [ts],
                    "open": [1.0],
                    "high": [1.0],
                    "low": [1.0],
                    "close": [1.0],
                    "volume": [0],
                }
            ).set_index("timestamp")
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", rate_limit_resp)
        monkeypatch.setattr(df, "_backup_get_bars", backup_resp)
        start, end = _dt_range(2)
        before = inc_provider_disable_total("alpaca")
        out = df._fetch_bars("TEST", start, end, "1Min", feed="iex")
        assert calls["alpaca"] == 1
        assert calls["backup"] == 1
        assert provider_disabled("alpaca") == 1
        assert inc_provider_disable_total("alpaca") == before + 1
        assert isinstance(out, pd.DataFrame) and not out.empty
>       assert df._alpaca_disabled_until is not None
E       assert None is not None
E        +  where None = df._alpaca_disabled_until

tests/unit/test_data_fetcher_http.py:464: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.logging:__init__.py:1313 FETCH_ATTEMPT
WARNING  ai_trading.data.fetch:__init__.py:7611 DATA_SOURCE_RATE_LIMITED
WARNING  ai_trading.data.provider_monitor:provider_monitor.py:906 DATA_PROVIDER_FAILURE_DIAGNOSTIC
ERROR    ai_trading.data.provider_monitor:provider_monitor.py:928 DATA_PROVIDER_FAILURE
ERROR    ai_trading.logging:alerts.py:116 Alert created: CRITICAL - Data provider alpaca failure
WARNING  ai_trading.data.provider_monitor:provider_monitor.py:1291 DATA_PROVIDER_DISABLED
WARNING  ai_trading.data.fetch:__init__.py:1841 ALPACA_TEMP_DISABLED
_______________________ test_rate_limit_fallback_success _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d714a1ca0>
capmetrics = [Rec(name='data.fetch.fallback_attempt', tags={'provider': 'yahoo', 'symbol': 'AAPL', 'feed': 'yahoo', 'timeframe': '1Min'}, value=1.0)]

    def test_rate_limit_fallback_success(monkeypatch: pytest.MonkeyPatch, capmetrics: list[Rec]):
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(df, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(df, "_SIP_PRECHECK_DONE", False, raising=False)
        monkeypatch.setattr(df, "max_data_fallbacks", lambda: 2, raising=False)
        monkeypatch.setattr(df, "_sip_configured", lambda: True, raising=False)
        start, end = _ts_window()
        responses: dict[str, list[Resp]] = {
            "iex": [Resp(429, {}), Resp(429, {})],
            "sip": [Resp(200, _bars_payload(start))],
        }
    
        def fake_get(*args, **kwargs):
            params = kwargs.get("params") or {}
            feed = params.get("feed", "iex")
            queue = responses.get(feed)
            if not queue:
                raise AssertionError(f"unexpected feed request: {feed}")
            return queue.pop(0)
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get, raising=False)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
        out = df._fetch_bars("AAPL", start, end, "1Min", feed="iex")
>       assert not out.empty
E       AttributeError: 'NoneType' object has no attribute 'empty'

tests/unit/test_data_fetcher_metrics.py:86: AttributeError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
________________ test_rate_limit_no_retry_when_sip_unauthorized ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d714a3500>
capmetrics = [Rec(name='data.fetch.fallback_attempt', tags={'provider': 'yahoo', 'symbol': 'AAPL', 'feed': 'yahoo', 'timeframe': '1Min'}, value=1.0)]

    def test_rate_limit_no_retry_when_sip_unauthorized(
        monkeypatch: pytest.MonkeyPatch, capmetrics: list[Rec]
    ):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", True, raising=False)
        start, end = _ts_window()
        calls = {"count": 0}
    
        def fake_get(*args, **kwargs):
            calls["count"] += 1
            return Resp(429, {})
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get, raising=False)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
    
>       with pytest.raises(ValueError, match="rate_limited"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/unit/test_data_fetcher_metrics.py:118: Failed
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
________________________ test_timeout_fallback_success _________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d714a2ba0>
capmetrics = [Rec(name='data.fetch.fallback_attempt', tags={'provider': 'yahoo', 'symbol': 'AAPL', 'feed': 'yahoo', 'timeframe': '1Min'}, value=1.0)]

    def test_timeout_fallback_success(monkeypatch: pytest.MonkeyPatch, capmetrics: list[Rec]):
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(df, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(df, "_SIP_PRECHECK_DONE", False, raising=False)
        monkeypatch.setattr(df, "max_data_fallbacks", lambda: 2, raising=False)
        monkeypatch.setattr(df, "_sip_configured", lambda: True, raising=False)
        start, end = _ts_window()
        events: dict[str, list[object]] = {
            "iex": [df.Timeout("boom"), df.Timeout("boom")],
            "sip": [Resp(200, _bars_payload(start))],
        }
    
        def fake_get(*args, **kwargs):
            params = kwargs.get("params") or {}
            feed = params.get("feed", "iex")
            queue = events.get(feed)
            if not queue:
                raise AssertionError(f"unexpected feed request: {feed}")
            ev = queue.pop(0)
            if isinstance(ev, Exception):
                raise ev
            return ev
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get, raising=False)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
        out = df._fetch_bars("AAPL", start, end, "1Min", feed="iex")
>       assert not out.empty
E       AttributeError: 'NoneType' object has no attribute 'empty'

tests/unit/test_data_fetcher_metrics.py:154: AttributeError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
_____________________ test_unauthorized_sip_returns_empty ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d52f82240>
capmetrics = [Rec(name='data.fetch.fallback_attempt', tags={'provider': 'yahoo', 'symbol': 'AAPL', 'feed': 'yahoo', 'timeframe': '1Min'}, value=1.0)]

    def test_unauthorized_sip_returns_empty(
        monkeypatch: pytest.MonkeyPatch, capmetrics: list[Rec]
    ):
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        start, end = _ts_window()
        responses = [Resp(403, {})]
    
        def fake_get(*args, **kwargs):
            return responses.pop(0)
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get, raising=False)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
        monkeypatch.setattr(df, "_backup_get_bars", lambda *a, **k: pd.DataFrame())
        out = df._fetch_bars("AAPL", start, end, "1Min", feed="sip")
        assert out is None or out.empty
        names = [r.name for r in capmetrics]
>       assert names == ["data.fetch.unauthorized"]
E       AssertionError: assert ['data.fetch....back_attempt'] == ['data.fetch.unauthorized']
E         At index 0 diff: 'data.fetch.fallback_attempt' != 'data.fetch.unauthorized'
E         Use -v to get more diff

tests/unit/test_data_fetcher_metrics.py:188: AssertionError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
_____________________ test_empty_payload_fallback_success ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d52f83230>
capmetrics = [Rec(name='data.fetch.fallback_attempt', tags={'provider': 'yahoo', 'symbol': 'AAPL', 'feed': 'yahoo', 'timeframe': '1Min'}, value=1.0)]

    def test_empty_payload_fallback_success(
        monkeypatch: pytest.MonkeyPatch, capmetrics: list[Rec]
    ):
        monkeypatch.setenv("PYTEST_RUNNING", "1")
        monkeypatch.setattr(df, "_SIP_UNAUTHORIZED", False, raising=False)
        monkeypatch.setattr(df, "_ALLOW_SIP", True, raising=False)
        monkeypatch.setattr(df, "_SIP_PRECHECK_DONE", False, raising=False)
        monkeypatch.setattr(df, "max_data_fallbacks", lambda: 2, raising=False)
        monkeypatch.setattr(df, "_sip_configured", lambda: True, raising=False)
        start, end = _ts_window()
        responses: dict[str, list[Resp]] = {
            "iex": [Resp(200, {"bars": []}), Resp(200, {"bars": []})],
            "sip": [Resp(200, _bars_payload(start))],
        }
    
        def fake_get(*args, **kwargs):
            params = kwargs.get("params") or {}
            feed = params.get("feed", "iex")
            queue = responses.get(feed)
            if not queue:
                raise AssertionError(f"unexpected feed request: {feed}")
            return queue.pop(0)
    
        monkeypatch.setattr(df._HTTP_SESSION, "get", fake_get, raising=False)
        monkeypatch.setattr(df.requests, "get", fake_get, raising=False)
        out = df._fetch_bars("AAPL", start, end, "1Min", feed="iex")
>       assert not out.empty
E       AttributeError: 'NoneType' object has no attribute 'empty'

tests/unit/test_data_fetcher_metrics.py:219: AttributeError
------------------------------ Captured log call -------------------------------
INFO     ai_trading.data.fetch:__init__.py:6356 DATA_SOURCE_FALLBACK_ATTEMPT
_________________ test_get_latest_price_http_error_falls_back __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d72fd7230>

    def test_get_latest_price_http_error_falls_back(monkeypatch):
        symbol = "MSFT"
        monkeypatch.setenv("ALPACA_DATA_FEED", "iex")
        monkeypatch.setattr(
            "ai_trading.core.bot_engine.is_alpaca_service_available",
            lambda: True,
        )
    
        def fake_alpaca_get(*_, **__):
            raise AlpacaOrderHTTPError(404, "missing", payload={"msg": "not found"})
    
        def fake_symbols():
            return fake_alpaca_get, None
    
        monkeypatch.setattr(bot_engine, "_alpaca_symbols", fake_symbols)
        monkeypatch.setattr("ai_trading.data.fetch._backup_get_bars", lambda *a, **k: object())
        monkeypatch.setattr("ai_trading.core.bot_engine.get_latest_close", lambda df: 77.0)
    
        price = bot_engine.get_latest_price(symbol)
    
>       assert price == pytest.approx(77.0)
E       assert None == 77.0 ± 7.7e-05
E         comparison failed
E         Obtained: None
E         Expected: 77.0 ± 7.7e-05

tests/unit/test_price_quote_feed.py:135: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:06:11Z", "level": "WARNING", "name": "ai_trading.core.bot_engine", "msg": "ALPACA_TRADE_FETCH_FAILED", "provider": "alpaca_trade", "symbol": "MSFT", "error": "HTTP 404: missing", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:06:11Z", "level": "INFO", "name": "ai_trading.core.bot_engine", "msg": "LOG_THROTTLE_SUMMARY | suppressed=22 key=\"LATEST_PRICE_FALLBACK_FAILED\"", "bot_phase": "GENERAL"}
{"ts": "2025-10-27T23:06:11Z", "level": "ERROR", "name": "ai_trading.core.bot_engine", "msg": "LATEST_PRICE_FALLBACK_FAILED", "symbol": "MSFT", "error": "alpaca-py StockHistoricalDataClient not available", "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.core.bot_engine:bot_engine.py:1518 ALPACA_TRADE_FETCH_FAILED
INFO     ai_trading.core.bot_engine:__init__.py:39 LOG_THROTTLE_SUMMARY | suppressed=22 key="LATEST_PRICE_FALLBACK_FAILED"
ERROR    ai_trading.core.bot_engine:bot_engine.py:2074 LATEST_PRICE_FALLBACK_FAILED
__________________ test_cached_alpaca_fallback_feed_sanitized __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d72fbff20>
caplog = <_pytest.logging.LogCaptureFixture object at 0x7c1d52f82e70>

    def test_cached_alpaca_fallback_feed_sanitized(monkeypatch, caplog):
        symbol = "AAPL"
        bot_engine._reset_cycle_cache()
        caplog.set_level("WARNING")
        monkeypatch.setattr(bot_engine, "_INTRADAY_FEED_CACHE", None, raising=False)
        monkeypatch.setenv("ALPACA_ALLOW_SIP", "1")
        monkeypatch.setenv("ALPACA_API_KEY", "key")
        monkeypatch.setenv("ALPACA_SECRET_KEY", "secret")
        monkeypatch.setattr(
            bot_engine,
            "_GLOBAL_INTRADAY_FALLBACK_FEED",
            "yahoo",
            raising=False,
        )
        monkeypatch.setattr(
            bot_engine.data_fetcher_module,
            "_sip_configured",
            lambda: True,
        )
        monkeypatch.setattr(
            "ai_trading.core.bot_engine.is_alpaca_service_available",
            lambda: True,
        )
        monkeypatch.setattr(
            bot_engine,
            "_get_price_provider_order",
            lambda: ("alpaca_trade",),
        )
    
        captured: dict[str, object] = {}
    
        def fake_alpaca_get(url, *, params=None, **_):
            captured["url"] = url
            captured["params"] = params
            return {"price": 145.0}
    
        monkeypatch.setattr(
            bot_engine,
            "_alpaca_symbols",
            lambda: (fake_alpaca_get, None),
        )
    
        bot_engine._cache_cycle_fallback_feed("alpaca_sip")
        assert bot_engine._prefer_feed_this_cycle() == "sip"
    
        price = bot_engine.get_latest_price(symbol)
    
        assert price == pytest.approx(145.0)
>       assert captured["params"] == {"feed": "sip"}
E       AssertionError: assert {'feed': 'iex'} == {'feed': 'sip'}
E         Differing items:
E         {'feed': 'iex'} != {'feed': 'sip'}
E         Use -v to get more diff

tests/unit/test_price_quote_feed.py:379: AssertionError
_____________ test_retry_mode_returns_fallback_without_retryerror ______________

    def test_retry_mode_returns_fallback_without_retryerror():
        calls = {"fail": 0}
    
        @retry_mode(retries=2, delay=0.001, fallback="ok", exceptions=(RuntimeError,))
        def always_fail():
            calls["fail"] += 1
            raise RuntimeError
    
        try:
>           result = always_fail()

tests/unit/test_retry_mode.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @retry_mode(retries=2, delay=0.001, fallback="ok", exceptions=(RuntimeError,))
    def always_fail():
        calls["fail"] += 1
>       raise RuntimeError
E       RuntimeError

tests/unit/test_retry_mode.py:34: RuntimeError
______________________ test_retry_reraise_explicit_policy ______________________

    def test_retry_reraise_explicit_policy():
        attempts = {"count": 0}
    
        @retry(
            stop=stop_after_attempt(2),
            retry=retry_if_exception_type(ValueError),
            reraise=True,
        )
        def boom():
            attempts["count"] += 1
            raise ValueError("boom")
    
        with pytest.raises(ValueError):
            boom()
    
>       assert attempts["count"] == 2
E       assert 3 == 2

tests/unit/test_retry_reraise.py:59: AssertionError
________________ test_run_all_trades_no_warning_with_valid_api _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7c1d70f2ba70>

    def test_run_all_trades_no_warning_with_valid_api(monkeypatch):
        """A valid client with get_orders should not trigger a warning."""
    
        # Stub Alpaca modules so the shim translates status -> GetOrdersRequest
        enums_mod = types.ModuleType("alpaca.trading.enums")
        requests_mod = types.ModuleType("alpaca.trading.requests")
    
        class OrderStatus:
            OPEN = "open"
    
        class GetOrdersRequest:
            def __init__(self, *, statuses=None):
                self.statuses = statuses
    
        enums_mod.OrderStatus = OrderStatus
        requests_mod.GetOrdersRequest = GetOrdersRequest
        monkeypatch.setitem(sys.modules, "alpaca", types.ModuleType("alpaca"))
        monkeypatch.setitem(sys.modules, "alpaca.trading", types.ModuleType("alpaca.trading"))
        monkeypatch.setitem(sys.modules, "alpaca.trading.enums", enums_mod)
        monkeypatch.setitem(sys.modules, "alpaca.trading.requests", requests_mod)
    
        class DummyAPI:
            def __init__(self):
                self.called_with: dict | None = None
    
            def get_orders(self, *args, **kwargs):  # noqa: D401
                """Capture forwarded kwargs."""
                self.called_with = kwargs
                return []
    
            def cancel_order(self, *args, **kwargs):  # noqa: D401 - stub
                """Provide minimal cancel capability for validation."""
                return None
    
        class DummyRiskEngine:
            def wait_for_exposure_update(self, timeout: float) -> None:  # noqa: D401
                """No-op risk update."""
    
        state = eng.BotState()
        api = DummyAPI()
        runtime = types.SimpleNamespace(api=api, risk_engine=DummyRiskEngine())
    
        # Minimal patches to isolate the order-check logic
        monkeypatch.setattr(eng, "_ensure_alpaca_classes", lambda: None)
        monkeypatch.setattr(eng, "_init_metrics", lambda: None)
        monkeypatch.setattr(eng, "is_market_open", lambda: True)
        monkeypatch.setattr(eng, "ensure_alpaca_attached", lambda runtime: None)
        monkeypatch.setattr(eng, "check_pdt_rule", lambda runtime: False)
        monkeypatch.setattr(eng, "get_strategies", lambda: [])
        monkeypatch.setattr(eng, "get_verbose_logging", lambda: False)
    
        def _raise_df(*_a, **_k):
            raise eng.DataFetchError("boom")
    
        monkeypatch.setattr(eng, "_prepare_run", _raise_df)
        monkeypatch.setattr(eng.CFG, "log_market_fetch", False, raising=False)
        stub_fetcher = types.SimpleNamespace()
        monkeypatch.setattr(
            eng.data_fetcher_module, "build_fetcher", lambda *_a, **_k: stub_fetcher
        )
        monkeypatch.setattr(eng, "data_fetcher", stub_fetcher, raising=False)
    
        class DummyLock:
            def acquire(self, blocking: bool = False) -> bool:  # noqa: D401
                """Always acquire."""
    
                return True
    
            def release(self) -> None:
                """No-op release."""
    
        monkeypatch.setattr(eng, "run_lock", DummyLock())
    
        warn_mock = Mock()
        info_mock = Mock()
        monkeypatch.setattr(eng.logger_once, "warning", warn_mock)
        monkeypatch.setattr(eng.logger_once, "info", info_mock)
    
        eng.run_all_trades_worker(state, runtime)
    
>       info_mock.assert_called_once_with("API_GET_ORDERS_MAPPED", key="alpaca_get_orders_mapped")

tests/unit/test_run_all_trades_warning.py:207: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Mock id='136465890849664'>, args = ('API_GET_ORDERS_MAPPED',)
kwargs = {'key': 'alpaca_get_orders_mapped'}
msg = "Expected 'mock' to be called once. Called 0 times."

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'mock' to be called once. Called 0 times.

/usr/lib/python3.12/unittest/mock.py:955: AssertionError
----------------------------- Captured stdout call -----------------------------
{"ts": "2025-10-27T23:06:12Z", "level": "WARNING", "name": "ai_trading.core.bot_engine", "msg": "DATA_FETCHER_UNAVAILABLE", "detail": "boom", "attempt": 1, "bot_phase": "GENERAL"}
------------------------------ Captured log call -------------------------------
WARNING  ai_trading.core.bot_engine:bot_engine.py:12400 Runtime context unavailable for risk exposure update: 'types.SimpleNamespace' object has no attribute 'max_drawdown_threshold'
WARNING  ai_trading.core.bot_engine:bot_engine.py:22802 DATA_FETCHER_ATTACHED_LATE
WARNING  ai_trading.core.bot_engine:bot_engine.py:24179 DATA_FETCHER_UNAVAILABLE
WARNING  ai_trading.core.bot_engine:bot_engine.py:24179 DATA_FETCHER_UNAVAILABLE
WARNING  ai_trading.core.bot_engine:bot_engine.py:24179 DATA_FETCHER_UNAVAILABLE
INFO     ai_trading.core.bot_engine:bot_engine.py:24622 CYCLE_GATES
INFO     ai_trading.core.bot_engine:bot_engine.py:23317 HEARTBEAT
=========================== short test summary info ============================
SKIPPED [1] tests/test_health.py:10: torch not installed
SKIPPED [1] tests/test_integration_robust.py:12: torch not installed
SKIPPED [1] tests/test_meta_learning.py:7: torch not installed
SKIPPED [1] tests/test_model_io.py:5: could not import 'dill': No module named 'dill'
SKIPPED [1] tests/test_portfolio_rl.py:6: torch not installed
SKIPPED [1] tests/test_property_based.py:6: could not import 'hypothesis': No module named 'hypothesis'
SKIPPED [1] tests/test_retrain_smoke.py:9: could not import 'lightgbm': No module named 'lightgbm'
SKIPPED [1] tests/test_rl_import_performance.py:7: could not import 'gymnasium': No module named 'gymnasium'
SKIPPED [1] tests/test_safe_utcnow_freezegun.py:5: could not import 'freezegun': No module named 'freezegun'
SKIPPED [1] tests/test_sentiment_interface.py:6: could not import 'tenacity': No module named 'tenacity'
SKIPPED [1] tests/test_sentiment_pkg_interface.py:7: could not import 'tenacity': No module named 'tenacity'
SKIPPED [1] tests/optdeps.py:25: Install with: pip install "tenacity"
SKIPPED [1] tests/optdeps.py:25: Install with: pip install pytest_asyncio
SKIPPED [1] tests/slow/test_meta_learning_heavy.py:11: torch not installed
SKIPPED [1] tests/test_advanced_features.py:43: slippage monitoring not available
SKIPPED [1] tests/test_alpaca_init_contract.py:67: could not import 'alpaca': No module named 'alpaca'
SKIPPED [1] tests/test_alpaca_init_contract.py:102: could not import 'alpaca': No module named 'alpaca'
SKIPPED [1] tests/test_daily_bars_datetime_sanitization.py:155: could not import 'freezegun': No module named 'freezegun'
SKIPPED [1] tests/test_data_fetch.py:16: requires valid Alpaca credentials
SKIPPED [1] tests/test_http_timeouts.py:89: requires requests missing
SKIPPED [1] tests/test_import_wiring.py:30: could not import 'psutil': No module named 'psutil'
SKIPPED [2] ../../usr/lib/python3/dist-packages/_pytest/python.py:184: async def function and no async plugin installed (see warnings)
SKIPPED [1] tests/test_model_registry.py:136: could not import 'cloudpickle': No module named 'cloudpickle'
SKIPPED [1] tests/test_model_registry.py:154: could not import 'cloudpickle': No module named 'cloudpickle'
SKIPPED [1] tests/test_model_registry_roundtrip.py:130: could not import 'cloudpickle': No module named 'cloudpickle'
SKIPPED [1] tests/test_peak_performance.py:206: lightgbm not installed
SKIPPED [1] tests/test_production_fixes.py:126: Required modules not available for data validation tests
SKIPPED [1] tests/test_production_fixes.py:95: Required modules not available for data validation tests
SKIPPED [1] tests/test_production_fixes.py:109: Required modules not available for data validation tests
SKIPPED [1] tests/test_signals.py:16: hmmlearn not installed
SKIPPED [1] tests/config/test_import_sanity.py:18: import sanity guard disabled
SKIPPED [1] tests/data/test_yahoo_intraday_reliability.py:54: Backup provider path not triggered under current configuration
XFAIL tests/test_backtest_smoke.py::test_backtester_engine_basic - minimal data may produce no trades
FAILED tests/test_feed_failover.py::test_feed_override_used_on_subsequent_requests
FAILED tests/test_fetch_empty_early_exit.py::test_persistent_empty_aborts_early
FAILED tests/test_fetch_empty_handling.py::test_warn_on_empty_when_market_open
FAILED tests/test_fetch_empty_handling.py::test_skip_retry_outside_market_hours
FAILED tests/test_fetch_empty_handling.py::test_fetch_bars_raises_on_retry_limit
FAILED tests/test_fetch_empty_priority.py::test_fetch_bars_handles_empty_priority
FAILED tests/test_get_latest_price_fallback.py::test_get_latest_price_uses_yahoo_when_alpaca_none
FAILED tests/test_get_latest_price_fallback.py::test_get_latest_price_skips_non_positive_from_yahoo
FAILED tests/test_get_latest_price_fallback.py::test_get_latest_price_handles_auth_failure
FAILED tests/test_get_latest_price_fallback.py::test_get_latest_price_prefer_backup_skips_alpaca
FAILED tests/test_get_latest_price_fallback.py::test_resolve_trade_quote_prefers_backup_when_primary_zero
FAILED tests/test_get_minute_df_fetch_logging.py::test_fetch_success_no_error_logged
FAILED tests/test_idempotency_cache_fallback.py::test_order_idempotency_cache_fallback
FAILED tests/test_iex_sip_fallback.py::test_iex_empty_switches_to_sip[preferred_list]
FAILED tests/test_iex_sip_fallback.py::test_iex_empty_switches_to_sip[no_preference]
FAILED tests/test_import_side_effects.py::test_module_imports_without_heavy_stacks
FAILED tests/test_import_side_effects.py::test_run_module_emits_no_runtime_warning
FAILED tests/test_imports_smoke.py::test_submodules_import - ImportError: can...
FAILED tests/test_kelly_confidence_fix.py::test_calculate_entry_size_zero_price
FAILED tests/test_lazy_pandas_import.py::test_module_import_without_pandas[ai_trading.metrics]
FAILED tests/test_log_deduper_provider_spam.py::test_throttle_summaries_flush_each_cycle
FAILED tests/test_log_deduper_provider_spam.py::test_provider_dedupe_emits_summary
FAILED tests/test_logging_behavior.py::test_health_rows_throttle - AttributeE...
FAILED tests/test_logging_scrubbed.py::test_no_secrets_in_logs - AssertionErr...
FAILED tests/test_main_alpaca_py_required.py::test_main_exits_when_alpaca_sdk_missing
FAILED tests/test_main_extended2.py::test_run_flask_app - AttributeError: 'Ap...
FAILED tests/test_main_extended2.py::test_run_flask_app_port_in_use - Attribu...
FAILED tests/test_main_extended2.py::test_run_flask_app_skips_ipv6_port - Att...
FAILED tests/test_main_extended2.py::test_validate_environment_missing - Fail...
FAILED tests/test_main_extended2.py::test_main_runs_once - AssertionError: as...
FAILED tests/test_main_trade_log_path.py::test_ensure_trade_log_path_unwritable
FAILED tests/test_market_closed_logging.py::test_market_closed_sleep_is_capped
FAILED tests/test_meta_learn_persistence.py::test_trade_persistence_updates_canonical_history
FAILED tests/test_metrics_registry.py::test_register_checks_existing_keys - a...
FAILED tests/test_minute_cache_helpers.py::test_age_cached_minute_timestamps
FAILED tests/test_model_loading.py::test_heuristic_fallback_marked_placeholder
FAILED tests/test_momentum_extra.py::test_generate_ret_nan - AttributeError: ...
FAILED tests/test_nameerror_integration.py::test_bot_engine_import_no_nameerror
FAILED tests/test_order_price_sourcing.py::test_price_source_prefers_nbbo - a...
FAILED tests/test_order_price_sourcing.py::test_price_source_primary_mid_when_nbbo_missing
FAILED tests/test_order_price_sourcing.py::test_price_source_backup_mid - ass...
FAILED tests/test_order_price_sourcing.py::test_price_source_last_close_fallback
FAILED tests/test_performance_fixes.py::test_meta_learning_mixed_format - Ass...
FAILED tests/test_performance_fixes.py::test_comprehensive_fixes - AssertionE...
FAILED tests/test_portfolio.py::test_short_close_queued - AttributeError: Non...
FAILED tests/test_predict_smoke.py::test_predict_function - AttributeError: m...
FAILED tests/test_preflight_trade_logger.py::test_preflight_initializes_trade_log
FAILED tests/test_prof_budget.py::test_soft_budget_direct_usage_elapsed_and_over
FAILED tests/test_prof_budget.py::test_soft_budget_context_manager_resets_start
FAILED tests/test_prof_budget.py::test_elapsed_ms_accumulates_fractional_nanoseconds
FAILED tests/test_prof_budget.py::test_over_budget_and_remaining_use_existing_start
FAILED tests/test_prof_soft_budget_regression.py::test_soft_budget_direct_regression
FAILED tests/test_prof_soft_budget_regression.py::test_soft_budget_context_manager_regression
FAILED tests/test_prof_soft_budget_regression.py::test_soft_budget_short_sleep_respects_budget
FAILED tests/test_prof_soft_budget_regression.py::test_soft_budget_reset_clears_fractional_state
FAILED tests/test_prof_soft_budget_regression.py::test_soft_budget_fractional_over_budget_threshold
FAILED tests/test_provider_monitor.py::test_primary_dwell_prevents_immediate_switchback
FAILED tests/test_provider_monitor_backoff.py::test_record_switchover_backoff
FAILED tests/test_provider_monitor_backoff.py::test_record_switchover_thrashing_disables
FAILED tests/test_provider_quiet_period_config.py::test_provider_quiet_period_respects_config
FAILED tests/test_run_cycle_after_hours.py::test_run_cycle_aborts_on_alpaca_auth_failure
FAILED tests/test_run_overlap.py::test_run_all_trades_overlap - AttributeErro...
FAILED tests/test_run_overlap.py::test_run_all_trades_missing_get_account - A...
FAILED tests/test_runner_additional.py::test_runner_starts - AttributeError: ...
FAILED tests/test_screen_universe_apierror.py::test_screen_universe_apierror_skips_symbol
FAILED tests/test_signals.py::test_prepare_indicators_calculates - AttributeE...
FAILED tests/test_sip_disallowed.py::test_no_unauthorized_log_when_sip_disabled
FAILED tests/test_sip_disallowed.py::test_sip_failover_does_not_call_fetch_when_unentitled
FAILED tests/test_sip_unauthorized.py::test_get_bars_unauthorized_sip_returns_empty
FAILED tests/test_sitecustomize.py::test_safe_clear_dict_restores_import_machinery
FAILED tests/test_start_api_port_increment.py::test_start_api_waits_for_transient_port_conflicts
FAILED tests/test_trade_logger_meta_conversion.py::test_trade_logger_triggers_conversion_on_audit_rows
FAILED tests/test_trigger_meta_learning_conversion.py::test_trigger_meta_learning_conversion_pure_meta_format
FAILED tests/test_trigger_meta_learning_conversion.py::test_trigger_meta_learning_conversion_pure_audit_format
FAILED tests/test_trigger_meta_learning_conversion.py::test_trigger_meta_learning_conversion_mixed_format
FAILED tests/test_trigger_meta_learning_conversion.py::test_trigger_meta_learning_conversion_problem_statement_exact
FAILED tests/test_utils_timing.py::test_timing_exports_exist_and_behave - ass...
FAILED tests/test_vendor_stub_alpaca_requests.py::test_stock_bars_request_keyword_args
FAILED tests/test_vendor_stub_alpaca_requests.py::test_stock_bars_request_additional_fields
FAILED tests/bot_engine/test_bot_engine.py::TestProcessSymbol::test_process_symbol_skips_on_all_nan_close
FAILED tests/bot_engine/test_fetch_minute_df_safe.py::test_process_symbol_reuses_prefetched_minute_data
FAILED tests/data/test_alpaca_iex_field_aliases.py::test_get_bars_df_alpaca_iex_columns
FAILED tests/data/test_alpaca_iex_field_aliases.py::test_get_daily_df_normalizes_alpaca_iex_columns
FAILED tests/data/test_daily_fetch_fallback.py::test_close_nan_disable_sets_quote_fallback
FAILED tests/data/test_empty_responses.py::test_alpaca_empty_responses_trigger_backup
FAILED tests/data/test_fetch_feed_modes.py::test_iex_ignores_sip_unauthorized
FAILED tests/data/test_get_minute_df_fetch_logging.py::test_sip_unauthorized_branch_annotates_backup
FAILED tests/execution/test_broker_capacity_preflight.py::test_skip_when_pdt_limit_reached
FAILED tests/execution/test_execute_entry_trade_log.py::test_slippage_converts_market_to_limit
FAILED tests/execution/test_execute_entry_trade_log.py::test_slippage_reduces_order_size
FAILED tests/execution/test_execution_engine.py::test_execute_order_returns_execution_result
FAILED tests/execution/test_execution_engine.py::test_async_fill_triggers_risk_engine
FAILED tests/execution/test_fetch_minute_df_safe_empty.py::test_empty_minute_raises
FAILED tests/execution/test_live_trading_errors.py::test_submit_limit_order_handles_timeout_without_unboundlocalerror
FAILED tests/execution/test_manual_price_slippage.py::test_manual_price_slippage_adjustment
FAILED tests/execution/test_manual_price_slippage.py::test_buy_slippage_improvement_allows_execution
FAILED tests/execution/test_manual_price_slippage.py::test_adverse_slippage_still_triggers_controls[99-buy]
FAILED tests/execution/test_manual_price_slippage.py::test_adverse_slippage_still_triggers_controls[0-sell]
FAILED tests/execution/test_max_factor_config.py::test_execute_entry_uses_config_max_factor
FAILED tests/integration/test_main_scheduler_resilience.py::test_scheduler_logs_and_continues_after_runtime_error
FAILED tests/runtime/test_max_position_size_consistency.py::test_max_position_size_consistency
FAILED tests/runtime/test_no_broad_except_in_hotpaths.py::test_no_broad_except_in_hotpaths
FAILED tests/runtime/test_runtime_behavior.py::test_start_api_with_signal_sets_error_on_failure
FAILED tests/unit/test_audit_log_trade.py::test_log_trade_permission_error_triggers_fix
FAILED tests/unit/test_data_fetcher_http.py::test_sip_fallback_skipped_when_marked_unauthorized
FAILED tests/unit/test_data_fetcher_http.py::test_timeout_triggers_fallback
FAILED tests/unit/test_data_fetcher_http.py::test_429_rate_limit_triggers_fallback
FAILED tests/unit/test_data_fetcher_http.py::test_empty_bars_fallback - Asser...
FAILED tests/unit/test_data_fetcher_http.py::test_sip_fallback_precheck_skips_request
FAILED tests/unit/test_data_fetcher_http.py::test_rate_limit_disable_and_recover
FAILED tests/unit/test_data_fetcher_metrics.py::test_rate_limit_fallback_success
FAILED tests/unit/test_data_fetcher_metrics.py::test_rate_limit_no_retry_when_sip_unauthorized
FAILED tests/unit/test_data_fetcher_metrics.py::test_timeout_fallback_success
FAILED tests/unit/test_data_fetcher_metrics.py::test_unauthorized_sip_returns_empty
FAILED tests/unit/test_data_fetcher_metrics.py::test_empty_payload_fallback_success
FAILED tests/unit/test_price_quote_feed.py::test_get_latest_price_http_error_falls_back
FAILED tests/unit/test_price_quote_feed.py::test_cached_alpaca_fallback_feed_sanitized
FAILED tests/unit/test_retry_mode.py::test_retry_mode_returns_fallback_without_retryerror
FAILED tests/unit/test_retry_reraise.py::test_retry_reraise_explicit_policy
FAILED tests/unit/test_run_all_trades_warning.py::test_run_all_trades_no_warning_with_valid_api
120 failed, 1895 passed, 33 skipped, 1 xfailed in 455.89s (0:07:35)
